filepath = ./deployments/terraform/modules/postgres-instance/variables.tf
variable "instance_name" {
  description = "The unique name for the PostgreSQL StatefulSet and related resources."
  type        = string
}

variable "namespace" {
  description = "The Kubernetes namespace to deploy the resources into."
  type        = string
}

variable "database_name" {
  description = "The name of the database to create."
  type        = string
}

variable "database_user" {
  description = "The username for the database."
  type        = string
}

variable "database_pass" {
  description = "The password for the database user."
  type        = string
  sensitive   = true
}

variable "storage_class_name" {
  description = "The name of the StorageClass to use for the PersistentVolumeClaim."
  type        = string
}

variable "storage_size" {
  description = "The size of the persistent volume (e.g., '10Gi')."
  type        = string
}-------------------------------------------------
filepath = ./deployments/terraform/modules/postgres-instance/main.tf
terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.20"
    }
  }
}

resource "kubernetes_secret" "postgres_secret" {
  metadata {
    name      = "${var.instance_name}-secret"
    namespace = var.namespace
    labels = {
      app = var.instance_name
    }
  }
  data = {
    "POSTGRES_USER"     = var.database_user
    "POSTGRES_PASSWORD" = var.database_pass
    "POSTGRES_DB"       = var.database_name
  }
  type = "Opaque"
}

resource "kubernetes_stateful_set" "postgres_sts" {
  metadata {
    name      = var.instance_name
    namespace = var.namespace
  }
  spec {
    service_name = "${var.instance_name}-headless"
    replicas     = 1

    selector {
      match_labels = {
        app = var.instance_name
      }
    }

    template {
      metadata {
        labels = {
          app = var.instance_name
        }
      }
      spec {
        termination_grace_period_seconds = 10
        containers {
          name  = "postgres"
          image = "postgres:15-alpine"
          ports {
            container_port = 5432
            name           = "postgres"
          }
          env_from {
            secret_ref {
              name = kubernetes_secret.postgres_secret.metadata[0].name
            }
          }
          volume_mounts {
            name       = "postgres-storage"
            mount_path = "/var/lib/postgresql/data"
          }
          liveness_probe {
            exec {
              command = ["pg_isready", "-U", var.database_user, "-d", var.database_name]
            }
            initial_delay_seconds = 30
            period_seconds        = 10
          }
          readiness_probe {
            exec {
              command = ["pg_isready", "-U", var.database_user, "-d", var.database_name]
            }
            initial_delay_seconds = 5
            period_seconds        = 5
          }
        }
      }
    }
    volume_claim_template {
      metadata {
        name = "postgres-storage"
      }
      spec {
        access_modes       = ["ReadWriteOnce"]
        storage_class_name = var.storage_class_name
        resources {
          requests = {
            storage = var.storage_size
          }
        }
      }
    }
  }
  depends_on = [kubernetes_secret.postgres_secret]
}

resource "kubernetes_service" "postgres_service" {
  metadata {
    name      = var.instance_name
    namespace = var.namespace
  }
  spec {
    selector = {
      app = var.instance_name
    }
    ports {
      port        = 5432
      target_port = 5432
    }
    type = "ClusterIP"
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/postgres-instance/outputs.tf
output "service_name" {
  description = "The name of the PostgreSQL Kubernetes service."
  value       = kubernetes_service.postgres_service.metadata[0].name
}

output "service_endpoint" {
  description = "The internal DNS endpoint for the service."
  value       = "${kubernetes_service.postgres_service.metadata[0].name}.${var.namespace}.svc.cluster.local"
}

output "secret_name" {
  description = "The name of the secret containing the database credentials."
  value       = kubernetes_secret.postgres_secret.metadata[0].name
}-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/variables.tf
variable "instance_name" {
  description = "A logical name for this database instance (used for naming the secret)."
  type        = string
}

variable "namespace" {
  description = "The Kubernetes namespace to create the secret in."
  type        = string
}

variable "db_host" {
  description = "The hostname or IP address of the external MySQL database."
  type        = string
}

variable "db_port" {
  description = "The port number of the external MySQL database."
  type        = string
  default     = "3306"
}

variable "database_name" {
  description = "The name of the database to connect to."
  type        = string
}

variable "database_user" {
  description = "The username for the external database."
  type        = string
}

variable "database_pass" {
  description = "The password for the external database user."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/main.tf
terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.20"
    }
  }
}

resource "kubernetes_secret" "external_mysql_secret" {
  metadata {
    name      = "${var.instance_name}-secret"
    namespace = var.namespace
    labels = {
      app  = var.instance_name
      type = "external-db"
    }
  }

  # Note: The keys here (e.g., DB_HOST) must match what your application expects
  # to read from the environment.
  data = {
    "DB_HOST"     = var.db_host
    "DB_PORT"     = var.db_port
    "DB_USER"     = var.database_user
    "DB_PASSWORD" = var.database_pass
    "DB_NAME"     = var.database_name
  }

  type = "Opaque"
}-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/outputs.tf
output "secret_name" {
  description = "The name of the secret containing the external database credentials."
  value       = kubernetes_secret.external_mysql_secret.metadata[0].name
}-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/variables.tf
variable "bucket_names" {
  description = "A list of bucket names to create in Backblaze B2."
  type        = list(string)
  default     = []
}

variable "tags" {
  description = "A map of tags to assign to the resources."
  type        = map(string)
  default     = {}
}-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/main.tf
# main.tf for the s3-buckets module

terraform {
  required_providers {
    b2 = {
      source  = "Backblaze/b2"
      version = "~> 0.6" # Use a recent version of the Backblaze provider
    }
  }
}

# Create a Backblaze B2 bucket for each name in the list
resource "b2_bucket" "storage_buckets" {
  for_each = toset(var.bucket_names)

  bucket_name = each.key
  bucket_type = "allPrivate" # Default to private, can be overridden

  lifecycle {
    prevent_destroy = false # Set to true in production for safety
  }

  cors_rules {
    cors_rule_name = "allowAll"
    allowed_origins = ["*"]
    allowed_operations = [
      "s3_delete",
      "s3_get",
      "s3_head",
      "s3_post",
      "s3_put",
    ]
    allowed_headers = ["*"]
    expose_headers = ["x-bz-content-sha1"]
    max_age_seconds = 3600
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/outputs.tf
output "bucket_ids" {
  description = "A map of bucket names to their Backblaze B2 IDs."
  value = {
    for bucket in b2_bucket.storage_buckets : bucket.bucket_name => bucket.bucket_id
  }
}

output "bucket_names" {
  description = "A list of the names of the created buckets."
  value = [for bucket in b2_bucket.storage_buckets : bucket.bucket_name]
}

variable "b2_application_key_id" {
  description = "The application key ID for Backblaze B2."
  type        = string
  sensitive   = true
}

variable "b2_application_key" {
  description = "The application key for Backblaze B2."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/variables.tf
variable "kustomize_path" {
  description = "The path to the Kustomize overlay to apply."
  type        = string
}

variable "service_name" {
  description = "The name of the Kubernetes deployment resource."
  type        = string
}

variable "namespace" {
  description = "The Kubernetes namespace to deploy into."
  type        = string
}

variable "image_tag" {
  description = "The Docker image tag to apply to the deployment."
  type        = string
  default     = "latest"
}

variable "image_repository" {
  description = "The Docker image repository (e.g., 'aqls/personae-auth-service')."
  type        = string
}

variable "config_sha" {
  description = "A hash of the service's config file to trigger updates."
  type        = string
  default     = ""
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/main.tf
resource "null_resource" "apply_kustomization" {
  triggers = {
    image_tag_trigger = var.image_tag
    config_sha_trigger = var.config_sha
  }

  provisioner "local-exec" {
    command = <<-EOT
      set -e
      echo "Applying Kustomize overlay at ${var.kustomize_path}"
      kubectl apply -k ${var.kustomize_path}

      echo "Setting image for deployment/${var.service_name} to ${var.image_repository}:${var.image_tag}"
      kubectl set image deployment/${var.service_name} ${var.service_name}=${var.image_repository}:${var.image_tag} -n ${var.namespace}

      echo "Waiting for rollout of deployment/${var.service_name}..."
      kubectl rollout status deployment/${var.service_name} -n ${var.namespace} --timeout=5m
    EOT
  }
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/terraform.tfvars
chart_version = "x.y.z"-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/variables.tf
# terraform/modules/nginx_ingress/variables.tf

variable "ingress_namespace" {
  description = "Namespace to deploy the NGINX Ingress controller into."
  type        = string
  default     = "ingress-nginx"
}

variable "helm_chart_version" {
  description = "Version of the ingress-nginx Helm chart to deploy."
  type        = string
  default     = "4.10.1" # Example, use a known good/recent version
}

variable "helm_values_content" {
  description = "YAML content string for Helm values. Pass using file() function from root module."
  type        = string
  default     = "" # Empty by default, meaning chart defaults unless provided
}

variable "create_namespace" {
  description = "Whether the module should create the namespace for the ingress controller."
  type        = bool
  default     = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/providers.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/main.tf
# terraform/modules/nginx_ingress/main.tf

resource "kubernetes_namespace" "ns" {
  count = var.create_namespace ? 1 : 0 # Create namespace only if variable is true
  metadata {
    name = var.ingress_namespace
    labels = {
      name = var.ingress_namespace
    }
  }
}

resource "helm_release" "ingress_nginx" {
  name       = "ingress-nginx"
  repository = "https://kubernetes.github.io/ingress-nginx"
  chart      = "ingress-nginx"
  namespace  = var.create_namespace ? kubernetes_namespace.ns[0].metadata[0].name : var.ingress_namespace
  version    = var.helm_chart_version

  values = var.helm_values_content != "" ? [var.helm_values_content] : []

  # Common overrides if not in values file, especially LoadBalancer type
  set {
    name  = "controller.service.type"
    value = "NodePort"
  }
/*  set {
    name  = "controller.replicaCount"
    value = "2" # Default to 2 replicas
  }*/

  depends_on = [kubernetes_namespace.ns] # Depends on namespace if created by module
}

data "kubernetes_service" "ingress_controller_service" {
  # This data source might fail if the service is not immediately available.
  # Consider making it optional or using a more robust way to get the IP if needed for immediate output.
  # For now, it assumes the service name convention from the chart.
  # The actual service name might vary based on the release name and chart.
  # Usually it's <release-name>-controller, so "ingress-nginx-controller".
  metadata {
    name      = "${helm_release.ingress_nginx.name}-controller"
    namespace = var.create_namespace ? kubernetes_namespace.ns[0].metadata[0].name : var.ingress_namespace
  }
  depends_on = [helm_release.ingress_nginx]
}-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/config/ingress-nginx-values.yaml
# terraform/modules/nginx_ingress/config/ingress-nginx-values.yaml

controller:
  # Configure the kind of Deployment/DaemonSet to have it across all nodes
  kind: DaemonSet
  # replicaCount: 2

  # Set up node affinity to use spot instances if available
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
              - key: "role"
                operator: In
                values:
                  - "spot-instance"

  # Set up resource limits
  resources:
    requests:
      cpu: 150m
      memory: 192Mi
    limits:
      cpu: 500m
      memory: 512Mi

  # Allow for graceful termination
  terminationGracePeriodSeconds: 300

  # Configure metrics for Prometheus
  metrics:
    enabled: true
    service:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "10254"

  # Configure the service
  service:
    type: NodePort
    externalTrafficPolicy: Local
    nodePorts:
      http: 30080
      https: 30443

  # Configure admission webhooks
  admissionWebhooks:
    enabled: true
    failurePolicy: Fail
    timeoutSeconds: 10

  # Configure configmaps
  config:
    # Enable gzip compression
    use-gzip: "true"
    gzip-types: "application/json application/javascript text/css text/javascript text/plain text/xml application/xml+rss"
    # Configure log format
    log-format-upstream: '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" $request_length $request_time [$proxy_upstream_name] [$proxy_alternative_upstream_name] $upstream_addr $upstream_response_length $upstream_response_time $upstream_status $req_id'
    # Configure rate limiting
    limit-req-status-code: "429"
    # Configure proxy timeout
    proxy-read-timeout: "180"
    proxy-send-timeout: "180"
    proxy-body-size: "50m"
    proxy-next-upstream-timeout: "20"
    client-header-timeout: "60"
    client-body-timeout: "60"
    http-snippet: |
      map $http_upgrade $connection_upgrade {
        default upgrade;
        '' close;
      }
    # Configure SSL
    ssl-protocols: "TLSv1.2 TLSv1.3"
    ssl-ciphers: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384"
    # Configure HSTS
    hsts: "true"
    hsts-include-subdomains: "true"
    hsts-max-age: "31536000"

# Default backend for 404 pages
defaultBackend:
  enabled: true
  image:
    registry: k8s.gcr.io
    image: defaultbackend-amd64
    tag: "1.5"
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 10m
      memory: 20Mi
  service:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/outputs.tf
# terraform/modules/nginx_ingress/outputs.tf
# ~/projects/terraform/rackspace_generic/terraform/modules/nginx_ingress/outputs.tf

output "namespace" {
  description = "Namespace where the ingress controller is deployed."
  value       = var.create_namespace ? kubernetes_namespace.ns[0].metadata[0].name : var.ingress_namespace
}

output "release_name" {
  description = "Helm release name for the ingress controller."
  value       = helm_release.ingress_nginx.name
}

output "loadbalancer_ip" {
  description = "External IP or Hostname of the NGINX Ingress controller LoadBalancer."
  value = try(
    # Attempt to get IP from the first ingress of the first load_balancer
    data.kubernetes_service.ingress_controller_service.status[0].load_balancer[0].ingress[0].ip,
    # Fallback to hostname if IP isn't present
    data.kubernetes_service.ingress_controller_service.status[0].load_balancer[0].ingress[0].hostname,
    "IP/Hostname pending or not a LoadBalancer" # Generic fallback
  )
}

# For debugging the structure, you can add:
output "debug_ingress_load_balancer_status_block" {
  description = "The raw load_balancer status block for debugging."
  value       = try(data.kubernetes_service.ingress_controller_service.status[0].load_balancer, null)
}-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/versions.tf
# terraform/modules/nginx_ingress/versions.tf

terraform {
  required_version = ">= 1.0"
  required_providers {
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.17.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.36.0"
    }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/variables.tf
variable "operator_namespace" {
  description = "Namespace to deploy the Strimzi Kafka operator into."
  type        = string
}

variable "watched_namespaces_list" {
  description = "List of namespaces for the Strimzi operator to watch."
  type        = list(string)
}

variable "strimzi_yaml_source_path" {
  description = "Path to the directory containing the Strimzi YAML files to apply (e.g., ./strimzi-yaml-0.45.0/install/cluster-operator/)."
  type        = string
}

variable "operator_deployment_yaml_filename" {
  description = "Filename of the main operator deployment YAML within the strimzi_yaml_source_path (used for trigger)."
  type        = string
  default     = "060-Deployment-strimzi-cluster-operator.yaml"
}

variable "cluster_kubeconfig_path" {
  description = "Path to the kubeconfig file for the target Kubernetes cluster."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/048-Crd-kafkamirrormaker2.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/023-amended-ClusterRole-strimzi-cluster-operator-role.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/045-Crd-kafkamirrormaker.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/042-Crd-strimzipodset.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/047-Crd-kafkaconnector.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/020-ClusterRole-strimzi-cluster-operator-role.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/049-Crd-kafkarebalance.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/021-ClusterRole-strimzi-cluster-operator-role.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/023-RoleBinding-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/031-ClusterRole-strimzi-entity-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/030-ClusterRole-strimzi-kafka-broker.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/022-ClusterRole-strimzi-cluster-operator-role.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/022-RoleBinding-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/041-Crd-kafkaconnect.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/046-Crd-kafkabridge.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/010-ServiceAccount-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/043-Crd-kafkatopic.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/020-RoleBinding-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/04A-Crd-kafkanodepool.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/021-ClusterRoleBinding-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/050-ConfigMap-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/044-Crd-kafkauser.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/033-ClusterRole-strimzi-kafka-client.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/040-Crd-kafka.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/060-Deployment-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/added-clusterrolebinding-operator-watched.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/providers.tf
/*
provider "kubernetes" {
  config_path = "~/.kube/config_production_sydney"
}

provider "helm" {
  kubernetes {
    config_path = "~/.kube/config_production_sydney"
  }
}*/-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/main.tf
# This module assumes the namespaces (operator_namespace and watched_namespaces_list)
# are created by a separate configuration or exist.
# The main.tf in the instance directory (e.g., 030-strimzi-operator) will create them.

resource "null_resource" "apply_strimzi_operator_yaml" {
  triggers = {
    operator_deployment_sha1 = fileexists("${var.strimzi_yaml_source_path}/${var.operator_deployment_yaml_filename}") ? filesha1("${var.strimzi_yaml_source_path}/${var.operator_deployment_yaml_filename}") : ""
    watched_namespaces_trigger = join(",", var.watched_namespaces_list)
  }

  provisioner "local-exec" {
    command = "kubectl apply --namespace ${var.operator_namespace} --filename ${var.strimzi_yaml_source_path}/"
    environment = {
      KUBECONFIG = var.cluster_kubeconfig_path
    }
  }
}

-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/README.md
The file [060-Deployment-strimzi-cluster-operator.yaml](strimzi-yaml-0.45.0/060-Deployment-strimzi-cluster-operator.yaml)
was altered to add the namespaces that we want strimzi kafka to watch
s/b value: "kafka,personae,strimzi"
(not valueFrom: fieldRef: fieldPath: metadata.namespace)

all myproject namespaces in yamls have to be sed replaced or find/replaced to strimzi

added the clusterrolebinding added-clusterrolebinding-operator-watched.yaml in config dir

github of strimzi files is:
https://github.com/strimzi/strimzi-kafka-operator-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/outputs.tf
output "operator_namespace_used" {
  description = "Namespace where the Strimzi operator was deployed."
  value       = var.operator_namespace
}

output "watched_namespaces_configured" {
  description = "Namespaces the Strimzi operator is configured to watch."
  value       = var.watched_namespaces_list
}-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/versions.tf

terraform {
  required_version = ">=1.0"
  required_providers {
    helm = { source = "hashicorp/helm", version = "~> 2.17.0" }
    kubernetes = { source = "hashicorp/kubernetes", version = "~> 2.36.0"}
    null = { source = "hashicorp/null", version = "~> 3.2.4" }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/variables.tf
variable "kafka_cr_namespace" {
  description = "Namespace where the Kafka CR will be applied (must be watched by Strimzi operator)."
  type        = string
}

variable "kafka_cr_yaml_file_path" {
  description = "Path to the Kafka Custom Resource YAML file."
  type        = string
}

variable "kubeconfig_path" {
  description = "Path to the kubeconfig file for the target Kubernetes cluster."
  type        = string
  sensitive   = true
}

variable "kube_context_name" {
  description = "The kubectl context to use for applying resources. Must be valid for the provided kubeconfig_path."
  type        = string
  # This will be provided by the calling component
}

# Variables to construct output values, assuming fixed naming conventions from Strimzi
variable "kafka_cr_cluster_name" {
  description = "The metadata.name of the Kafka cluster defined in the CR YAML."
  type        = string
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/providers.tf
# terraform/environments/production/services-sydney/030-kafka-cluster/providers.tf
provider "kubernetes" {
  config_path = var.kubeconfig_path
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/main.tf
resource "null_resource" "apply_kafka_cluster_cr" {
  triggers = {
    yaml_file_sha1 = fileexists(var.kafka_cr_yaml_file_path) ? filesha1(var.kafka_cr_yaml_file_path) : ""
    # Adding context and namespace to triggers to ensure re-apply if they change for some reason
    context_trigger   = var.kube_context_name
    namespace_trigger = var.kafka_cr_namespace
  }

  provisioner "local-exec" {
    command = "kubectl --kubeconfig=${var.kubeconfig_path} --context=${var.kube_context_name} apply --namespace ${var.kafka_cr_namespace} --filename ${var.kafka_cr_yaml_file_path}"
    # The KUBECONFIG env var is redundant if --kubeconfig is used in the command, but harmless.
    environment = {
      KUBECONFIG = var.kubeconfig_path
    }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafkauser-permissive-test.yaml
# test-anonymous-broader-corrected.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: test-anonymous-broader
  namespace: kafka
  labels:
    strimzi.io/cluster: personae-kafka-cluster
spec:
  authorization:
    type: simple
    acls:
      # Allow Describe on ALL topics
      - resource:
          type: topic
          name: "*"
          patternType: literal # For all topics, name should be "*" and patternType literal
        operations:
          - Describe
        host: "*"
      # Allow Describe on the cluster
      - resource:
          type: cluster # For 'cluster' type, no 'name' or 'patternType' needed in the resource block.
          # Strimzi implicitly uses 'kafka-cluster' as the resource name.
        operations:
          - Describe
        host: "*"-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafka-temp-fix.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: personae-kafka-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.9.0
    replicas: 1
    listeners:
      # We are defining ONLY the plaintext listener.
      - name: plain
        port: 9092
        type: internal
        tls: false
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
      default.replication.factor: 1
      min.insync.replicas: 1
      inter.broker.protocol.version: "3.9"
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  zookeeper:
    replicas: 1
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafkauser-test.yaml
# modules/kafka_cluster/config/kafkauser-test.yaml
# test-anonymous-describe-user.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: test-anonymous-describe
  namespace: kafka
  labels:
    strimzi.io/cluster: personae-kafka-cluster
spec:
  authorization:
    type: simple
    acls:
      - resource:
          type: topic
          name: personae-
          patternType: prefix
        host: "*"
        operations:
          - Describe
-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafka-cluster-cr-dev.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: personae-kafka-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.9.0
    replicas: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false # Match working config
    config:
      default.replication.factor: 1
      min.insync.replicas: 1
      inter.broker.protocol.version: "3.9"
      log.message.format.version: "3.9"
      log.retention.hours: "160"
      log4j.logger.kafka.authorizer.logger: INFO
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
    resources:
      requests:
        memory: "512Mi"
        cpu: "300m"
      limits:
        memory: "1000Mi"
        cpu: "500"
  zookeeper:
    replicas: 1
    resources:
      requests:
        memory: "250Mi"
        cpu: "250m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false # Match working config
  entityOperator:
    topicOperator:
      reconciliationIntervalMs: 90000
    userOperator: {}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/personae-app-anonymous-v2.yaml
# personae-app-anonymous-V2.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: personae-app-anonymous
  namespace: kafka
  labels:
    strimzi.io/cluster: personae-kafka-cluster
spec:
  authorization:
    type: simple
    acls:
      # For init containers & tools to describe specific topics
      - resource:
          type: topic
          name: personae-core-requests # Be very specific for the test topic
          patternType: literal
        operations: [ Describe ]
        host: "*"
      # For init containers & tools to list/describe all topics (if needed)
      - resource:
          type: topic
          name: "*" # All topics
          patternType: literal
        operations: [ Describe ]
        host: "*"
      # For applications to Read/Write their topics
      - resource:
          type: topic
          name: personae- # Topics starting with "personae-"
          patternType: prefix
        operations: [ Read, Write, Create, Describe ] # Describe is good for apps too
        host: "*"

      # For applications and tools to describe the cluster (often needed for metadata)
      - resource:
          type: cluster
        operations: [ Describe ]
        host: "*"

      # For personae-core-manager consumer group (explicit literal match)
      - resource:
          type: group
          name: personae-core-manager # Explicit name
          patternType: literal
        operations: [ Read, Describe ] # Read is for consuming offsets, Describe for FindCoordinator etc.
        host: "*"
      # General describe for any other group (e.g., if other agents use different group names)
      - resource:
          type: group
          name: "*"
          patternType: literal
        operations: [ Describe ]
        host: "*"-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/personae-app-anonymous.yaml
# personae-app-anonymous-access.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: personae-app-anonymous
  namespace: kafka # Users must be in the same namespace as the Kafka cluster
  labels:
    strimzi.io/cluster: personae-kafka-cluster
spec:
  # No authentication block: applies to User:ANONYMOUS
  authorization:
    type: simple
    acls:
      # For init containers and apps to find/describe topics
      - resource:
          type: topic
          name: personae- # Topics starting with "personae-"
          patternType: prefix
        operations:
          - Describe
          - Read    # For consumers
          - Write   # For producers (if any agents produce to these)
          - Create  # If apps/producers might create them (less likely if Strimzi manages topics)
        host: "*"

      # For consumers to operate on their groups
      # Assuming consumer group names might also start with "personae-" or are related
      - resource:
          type: group
          name: personae- # Consumer groups starting with "personae-"
          patternType: prefix # Adjust if group names are different
        operations:
          - Read
          - Describe
          # - Delete # If consumers manage their own offset commits and group membership actively
        host: "*"

      # For consumers to find their coordinator (often needs Describe on a group)
      # The previous rule for group 'personae-' prefix should cover FindCoordinator for those groups.
      # If using arbitrary group names, you'd need a broader group rule like:
      - resource:
          type: group
          name: "*" # Or a more specific group prefix/name if known
          patternType: literal
        operations:
          - Describe # For FindCoordinator for any group
          # - Read # Be cautious with Read on group "*"
        host: "*"

      # General cluster describe, often needed by tools and sometimes clients
      - resource:
          type: cluster
        operations:
          - Describe
        host: "*"-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafka-cluster-cr.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: personae-kafka-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.9.0 # Match operator compatibility
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    storage:
      type: persistent-claim
      size: 100Gi
      class: ssd-large
      deleteClaim: false # Match working config
    config:
      default.replication.factor: 3
      min.insync.replicas: 2
      inter.broker.protocol.version: "3.9"
      log.message.format.version: "3.9"
      log.retention.hours: "160"
      log4j.logger.kafka.authorizer.logger: INFO
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "1500Mi"
        cpu: "1"
  zookeeper:
    replicas: 3
    resources:
      requests:
        memory: "512Mi" # Adjust
        cpu: "250m"
      limits:
        memory: "768Mi" # Adjust
        cpu: "500m"
    storage:
      type: persistent-claim
      size: 10Gi
      class: ssd
      deleteClaim: false # Match working config
  entityOperator:
    topicOperator:
      reconciliationIntervalMs: 90000
    userOperator: {}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafka-kraft-cluster.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: personae-kafka-cluster
  namespace: kafka
  annotations:
    strimzi.io/kraft: "enabled"
spec:
  kafka:
    version: 3.9.0
    replicas: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    config:
      # Single-node compatible settings
      default.replication.factor: 1
      min.insync.replicas: 1
      inter.broker.protocol.version: "3.9"
      log.retention.hours: 160
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
    resources:
      requests:
        memory: 512Mi
        cpu: 300m
      limits:
        memory: 1000Mi
        cpu: 500m
    # KRaft mode - no separate ZooKeeper needed
    metadataVersion: 3.9-IV4
  # No zookeeper section needed for KRaft
  entityOperator:
    topicOperator:
      reconciliationIntervalMs: 90000
    userOperator: {}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/README.md
when trying to deploy using a kubernetes_manifest for this kafka CRD it continually bugged out when setting config variables, so we are doing it using null_resource kubectl apply ...

this didn't work:
resource "kubernetes_manifest" "kafka_cluster" {
manifest = {
"apiVersion" = "kafka.strimzi.io/v1beta2"
"kind"       = "Kafka"
"metadata" = {
"name"      = var.kafka_cluster_name
"namespace" = var.kafka_cluster_namespace
}
"spec" = {
"kafka" = {
"version"  = var.kafka_version
"replicas" = var.kafka_replicas
"listeners" = [ # Minimal listeners
{ "name": "plain", "port": 9092, "type": "internal", "tls": false },
{ "name" = "tls", "port" = 9093, "type" = "internal", "tls"  = true }
]
"storage" = merge( # Assuming you kept the merge logic for class
{
"type" = "persistent-claim"
"size" = var.kafka_persistent_claim_size
},
var.kafka_persistent_claim_storage_class == null ? {} : { "class" = var.kafka_persistent_claim_storage_class
}
),
"config" = var.kafka_config
}
"entityOperator" = {
"topicOperator" = var.enable_topic_operator ? {} : null
"userOperator"  = var.enable_user_operator ? {} : null
}
}
}
computed_fields = [
"spec.kafka.config"
]
}

because of the "config" = va.kafka_config didn't read the map properly:
variable "kafka_config" {
description = "List of Kafka broker configuration overrides."
type        = map(string)
default = {
"log.message.format.version" = "4.0",
"log.retention.hours"        = "168"
}
}

I got:
terraform apply -auto-approve
kubernetes_manifest.kafka_cluster: Refreshing state...
╷
│ Error: Failed to update proposed state from prior state
│
│   with kubernetes_manifest.kafka_cluster,
│   on main.tf line 3, in resource "kubernetes_manifest" "kafka_cluster":
│    3: resource "kubernetes_manifest" "kafka_cluster" {
│
│ AttributeName("config"): can't use tftypes.Object["log.message.format.version":tftypes.String, "log.retention.hours":tftypes.String] as
│ tftypes.Map[tftypes.String]
╵
-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/outputs.tf
# terraform/modules/kafka_cluster/outputs.tf
output "cluster_name_applied" {
  description = "The name of the Kafka cluster that was applied."
  value       = var.kafka_cr_cluster_name
}

output "cluster_namespace_applied" {
  description = "The namespace where the Kafka cluster CR was applied."
  value       = var.kafka_cr_namespace
}

output "bootstrap_servers_plain" {
  description = "Assumed Internal Plaintext Bootstrap Servers."
  value       = "${var.kafka_cr_cluster_name}-kafka-bootstrap.${var.kafka_cr_namespace}.svc:9092"
}

output "bootstrap_servers_tls" {
  description = "Assumed Internal TLS Bootstrap Servers."
  value       = "${var.kafka_cr_cluster_name}-kafka-bootstrap.${var.kafka_cr_namespace}.svc:9093"
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/versions.tf
terraform {
  required_providers {
    kubernetes = { # Needed if you want to add data sources for services later, but not strictly for null_resource
      source  = "hashicorp/kubernetes"
      version = "~> 2.36.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "~> 3.2.4"
    }
  }
  required_version = ">= 1.0"
}-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/variables.tf
# ~/projects/terraform/rackspace_generic/terraform/modules/kubernetes_cluster_rackspace/variables.tf

variable "cluster_name" {
  description = "Name for the Kubernetes cluster (Spot Cloudspace)."
  type        = string
}

variable "rackspace_region" {
  description = "Rackspace region for the cluster (e.g., aus-syd-1)."
  type        = string
}

variable "kubernetes_version" {
  description = "Kubernetes version for the cluster."
  type        = string
  default     = "1.31.1"
}

variable "cni" {
  description = "CNI plugin for the cluster."
  type        = string
  default     = "calico"
}

variable "hacontrol_plane" {
  description = "Enable HA control plane."
  type        = bool
  default     = false
}

variable "preemption_webhook_url" {
  description = "Preemption webhook URL (e.g., Slack webhook)."
  type        = string
  sensitive   = true
  default     = null
}

# --- REMOVED OLD ON-DEMAND VARIABLES ---
# variable "ondemand_node_count" { ... }
# variable "ondemand_node_flavor" { ... }
# variable "ondemand_node_labels" { ... }
# variable "ondemand_node_taints" { ... }

# --- REMOVED OLD SPOT VARIABLES ---
# variable "spot_min_nodes" { ... }
# variable "spot_max_nodes" { ... }
# variable "spot_node_flavor" { ... }
# variable "spot_max_price" { ... }
# variable "spot_node_labels" { ... }


# +++ ADDED NEW FLEXIBLE ON-DEMAND POOL VARIABLE +++
variable "ondemand_node_pools" {
  description = "A map of on-demand node pools to create."
  type = map(object({
    node_count = number
    flavor     = string
    labels     = map(string)
    taints = list(object({
      key    = string
      value  = string
      effect = string
    }))
  }))
  default = {
    "default_pool" = {
      node_count = 0
      flavor     = "gp.small" # Example, replace with your actual flavor
      labels = {
        "role"       = "general",
        "app.type"   = "stateful",
        "managed-by" = "terraform"
      }
      taints = []
    }
  }
}

# +++ ADDED NEW FLEXIBLE SPOT POOL VARIABLE +++
variable "spot_node_pools" {
  description = "A map of spot node pools to create."
  type = map(object({
    min_nodes = number
    max_nodes = number
    flavor    = string
    max_price = number
    labels    = map(string)
  }))
  default = {
    "spot_worker_pool" = {
      min_nodes = 3
      max_nodes = 5
      flavor    = "c.large" # Example, replace with your actual flavor
      max_price = 0.01
      labels = {
        "role"       = "spot-instance",
        "app.type"   = "stateless",
        "managed-by" = "terraform"
      }
    }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/providers.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/main.tf
data "spot_serverclasses" "available_flavors" {
  # This data source does not take a region argument.
  # It's here mostly for reference if you output it.
}

resource "spot_cloudspace" "cluster" {
  cloudspace_name      = var.cluster_name
  region               = var.rackspace_region
  hacontrol_plane      = var.hacontrol_plane
  preemption_webhook   = var.preemption_webhook_url # Using the module's input variable
  wait_until_ready     = true
  kubernetes_version   = var.kubernetes_version
  cni                  = var.cni
}

# +++ CREATES MULTIPLE, DYNAMIC SPOT POOLS +++
resource "spot_spotnodepool" "spot_pools" {
  for_each = var.spot_node_pools

  cloudspace_name = spot_cloudspace.cluster.cloudspace_name
  server_class    = each.value.flavor
  bid_price       = each.value.max_price
  autoscaling = {
    min_nodes = each.value.min_nodes
    max_nodes = each.value.max_nodes
  }
  labels     = each.value.labels
  depends_on = [spot_cloudspace.cluster]
}

# +++ CREATES MULTIPLE, DYNAMIC ON-DEMAND POOLS +++
resource "spot_ondemandnodepool" "ondemand_pools" {
  for_each = var.ondemand_node_pools

  cloudspace_name      = spot_cloudspace.cluster.cloudspace_name
  server_class         = each.value.flavor
  desired_server_count = each.value.node_count
  labels               = each.value.labels
  taints               = each.value.taints
  depends_on           = [spot_cloudspace.cluster]
}


data "spot_kubeconfig" "cluster_kubeconfig" {
  cloudspace_name = spot_cloudspace.cluster.cloudspace_name
  depends_on = [
    spot_cloudspace.cluster,
    spot_ondemandnodepool.ondemand_pools, # Depends on the collection of pools
    spot_spotnodepool.spot_pools          # Depends on the collection of pools
  ]
}-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/README.md
-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/outputs.tf
# terraform/modules/kubernetes_cluster_rackspace/outputs.tf

output "kubeconfig_raw" {
  description = "Raw kubeconfig content for the cluster."
  value       = data.spot_kubeconfig.cluster_kubeconfig.raw
  sensitive   = true
}
output "cluster_name_actual" {
  description = "Name of the created Kubernetes cluster."
  value       = spot_cloudspace.cluster.cloudspace_name
}
output "cluster_endpoint_actual" {
  description = "API endpoint for the Kubernetes cluster."
  value       = data.spot_kubeconfig.cluster_kubeconfig.kubeconfigs[0].host
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/versions.tf
terraform {
  required_providers {
    spot = {
      source  = "rackerlabs/spot"
      version = "~> 0.1.4"
    }
  }
  required_version = ">= 1.0"
}
-------------------------------------------------
