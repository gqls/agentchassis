filepath = ./cmd/reasoning-agent/Dockerfile
# FILE: Dockerfile.reasoning
# A dedicated Dockerfile for building the reasoning agent service.

# Stage 1: Build the application
FROM golang:1.21-alpine AS builder

WORKDIR /app

# Copy modules and download dependencies
COPY go.mod go.sum ./
RUN go mod download

# Copy the entire project context
COPY . .

# Build the specific service binary
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /app/reasoning-agent ./cmd/reasoning-agent

# Stage 2: Create the final small image
FROM alpine:latest
RUN apk --no-cache add ca-certificates

# Create a non-root user for security
RUN addgroup -S appgroup && adduser -S -G appgroup appuser

WORKDIR /app

# Copy only the compiled binary from the builder stage
COPY --from=builder /app/reasoning-agent /app/reasoning-agent

# Copy its specific configuration file
COPY configs/reasoning-agent.yaml /app/configs/reasoning-agent.yaml

RUN chown appuser:appgroup /app/reasoning-agent

USER appuser

# The command to run the service, pointing to its own config
CMD ["./reasoning-agent", "-config", "configs/reasoning-agent.yaml"]
-------------------------------------------------
filepath = ./cmd/reasoning-agent/main.go
// FILE: cmd/reasoning-agent/main.go
package main

import (
	"context"
	"flag"
	"log"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/gqls/agentchassis/internal/agents/reasoning"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/logger"
	"go.uber.org/zap"
)

func main() {
	configPath := flag.String("config", "configs/reasoning-agent.yaml", "Path to config file")
	flag.Parse()

	cfg, err := config.Load(*configPath)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	appLogger, err := logger.New(cfg.Logging.Level)
	if err != nil {
		log.Fatalf("Failed to initialize logger: %v", err)
	}
	defer appLogger.Sync()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	agent, err := reasoning.NewAgent(ctx, cfg, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to initialize reasoning agent", zap.Error(err))
	}

	// Start health endpoint HERE
	agent.StartHealthServer("9090")

	// Start the agent's main run loop in a goroutine
	go func() {
		if err := agent.Run(); err != nil {
			appLogger.Error("Reasoning agent failed to run", zap.Error(err))
			cancel()
		}
	}()

	// Wait for shutdown signal
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit
	appLogger.Info("Shutdown signal received, shutting down reasoning agent...")

	cancel()
	time.Sleep(2 * time.Second)
	appLogger.Info("Reasoning agent service stopped.")
}
-------------------------------------------------
filepath = ./internal/agents/reasoning/API.md
# Reasoning Agent Internal API Documentation

## Overview

The Reasoning Agent is a code-driven agent that performs logical analysis, decision making, and complex reasoning tasks. It processes requests via Kafka messages and returns structured reasoning results.

## Kafka Topics

### Consumed Topics

#### agents.reasoning.process
Main processing topic for reasoning requests.

**Message Format:**
```json
{
  "action": "analyze|decide|evaluate|reason",
  "data": {
    "context": "Background information for reasoning",
    "question": "What needs to be analyzed or decided",
    "constraints": [
      "List of constraints to consider"
    ],
    "options": [
      {
        "id": "option1",
        "description": "First option to consider"
      }
    ],
    "criteria": {
      "factors": ["cost", "time", "quality"],
      "weights": {
        "cost": 0.3,
        "time": 0.2,
        "quality": 0.5
      }
    }
  }
}
```

**Required Headers:**
- `correlation_id`: Unique request identifier
- `request_id`: Request tracking ID
- `client_id`: Client identifier
- `agent_instance_id`: Specific agent instance
- `fuel_budget`: Available fuel for this operation

### Produced Topics

#### agents.reasoning.results
Results of reasoning operations.

**Message Format:**
```json
{
  "success": true,
  "data": {
    "reasoning_type": "analysis|decision|evaluation",
    "conclusion": "Main conclusion or recommendation",
    "reasoning_steps": [
      {
        "step": 1,
        "description": "Identified key factors",
        "findings": ["Factor 1", "Factor 2"]
      },
      {
        "step": 2,
        "description": "Evaluated options",
        "analysis": {
          "option1": {
            "score": 0.85,
            "pros": ["Pro 1", "Pro 2"],
            "cons": ["Con 1"]
          }
        }
      }
    ],
    "confidence": 0.85,
    "assumptions": [
      "List of assumptions made"
    ],
    "recommendations": [
      {
        "priority": "high",
        "action": "Recommended action",
        "rationale": "Why this is recommended"
      }
    ]
  }
}
```

#### system.agent.metrics
Performance metrics and usage data.

**Message Format:**
```json
{
  "agent_type": "reasoning",
  "agent_instance_id": "instance-uuid",
  "metrics": {
    "processing_time_ms": 1500,
    "fuel_consumed": 25,
    "reasoning_depth": 3,
    "options_evaluated": 5
  },
  "timestamp": "2024-01-01T00:00:00Z"
}
```

## Agent Configuration

The reasoning agent accepts the following configuration parameters:

```json
{
  "model": "claude-3-opus",
  "temperature": 0.2,
  "max_reasoning_steps": 10,
  "reasoning_style": "analytical|creative|balanced",
  "output_format": "structured|narrative",
  "enable_assumptions": true,
  "confidence_threshold": 0.7,
  "parallel_evaluation": true,
  "memory_enabled": true,
  "memory_context_limit": 5
}
```

## Supported Actions

### analyze
Performs deep analysis of a situation or problem.

**Input:**
```json
{
  "action": "analyze",
  "data": {
    "subject": "Market entry strategy for Product X",
    "context": "Current market conditions and company capabilities",
    "aspects": ["market_size", "competition", "regulatory", "timing"]
  }
}
```

### decide
Makes decisions based on given criteria and options.

**Input:**
```json
{
  "action": "decide",
  "data": {
    "decision": "Which cloud provider to choose",
    "options": ["AWS", "GCP", "Azure"],
    "criteria": {
      "factors": ["cost", "features", "support"],
      "constraints": ["Must support Kubernetes", "Budget < $10k/month"]
    }
  }
}
```

### evaluate
Evaluates proposals, plans, or strategies.

**Input:**
```json
{
  "action": "evaluate",
  "data": {
    "proposal": "Details of the proposal",
    "evaluation_criteria": ["feasibility", "roi", "risk"],
    "benchmark": "Optional benchmark for comparison"
  }
}
```

### reason
General reasoning about complex scenarios.

**Input:**
```json
{
  "action": "reason",
  "data": {
    "scenario": "Description of the scenario",
    "question": "What needs to be determined",
    "approach": "deductive|inductive|abductive"
  }
}
```

## Memory Integration

When memory is enabled, the agent:
- Retrieves relevant past reasoning for similar problems
- Stores successful reasoning patterns
- Learns from previous decisions and their outcomes

**Memory Entry Format:**
```json
{
  "type": "reasoning_pattern",
  "content": {
    "problem_type": "decision|analysis|evaluation",
    "pattern": "Description of successful reasoning approach",
    "effectiveness": 0.9,
    "context_tags": ["finance", "strategy", "risk"]
  }
}
```

## Error Handling

The agent returns errors in the following format:

```json
{
  "success": false,
  "error": {
    "code": "REASONING_001",
    "message": "Unable to complete reasoning",
    "details": {
      "reason": "Insufficient context provided",
      "missing_elements": ["criteria", "constraints"]
    }
  }
}
```

### Error Codes
- `REASONING_001`: Invalid input format
- `REASONING_002`: Insufficient context
- `REASONING_003`: Reasoning timeout
- `REASONING_004`: Conflicting constraints
- `REASONING_005`: Low confidence result

## Performance Characteristics

- Average processing time: 1-3 seconds
- Fuel consumption: 10-50 units depending on complexity
- Memory usage: ~100MB baseline + variable based on context
- Concurrent request handling: Yes
- Max context size: 100KB

## Integration Notes

### For Orchestrators
- Provide comprehensive context for better reasoning
- Include all relevant constraints upfront
- Specify desired output format
- Handle low-confidence results appropriately

### For Other Agents
- Can chain reasoning results with other agents
- Use structured output for easier parsing
- Consider confidence scores in workflows

## Environment Variables

```bash
# Agent Configuration
AGENT_TYPE=reasoning
KAFKA_CONSUMER_GROUP=reasoning-agent-group
KAFKA_TOPICS=agents.reasoning.process

# AI Service
AI_PROVIDER=anthropic
ANTHROPIC_API_KEY=<api-key>

# Performance
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT=30s
MEMORY_LIMIT=512MB

# Monitoring
METRICS_ENABLED=true
METRICS_INTERVAL=60s
```-------------------------------------------------
filepath = ./internal/agents/reasoning/agent.go
// FILE: internal/agents/reasoning/agent.go
package reasoning

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/platform/aiservice"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/kafka"
	"go.uber.org/zap"
)

const (
	requestTopic  = "system.agent.reasoning.process"
	responseTopic = "system.responses.reasoning"
	consumerGroup = "reasoning-agent-group"
)

// RequestPayload defines the data this agent expects
type RequestPayload struct {
	Action string `json:"action"`
	Data   struct {
		ContentToReview string                 `json:"content_to_review"`
		ReviewCriteria  []string               `json:"review_criteria"`
		BriefContext    map[string]interface{} `json:"brief_context"`
	} `json:"data"`
}

// ResponsePayload defines the response format
type ResponsePayload struct {
	ReviewPassed bool     `json:"review_passed"`
	Score        float64  `json:"score"`
	Suggestions  []string `json:"suggestions"`
	Reasoning    string   `json:"reasoning"`
}

// Agent is the reasoning specialist
type Agent struct {
	ctx      context.Context
	logger   *zap.Logger
	consumer *kafka.Consumer
	producer kafka.Producer
	aiClient aiservice.AIService
}

// NewAgent creates a new reasoning agent
func NewAgent(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger) (*Agent, error) {
	consumer, err := kafka.NewConsumer(cfg.Infrastructure.KafkaBrokers, requestTopic, consumerGroup, logger)
	if err != nil {
		return nil, fmt.Errorf("failed to create consumer: %w", err)
	}

	producer, err := kafka.NewProducer(cfg.Infrastructure.KafkaBrokers, logger)
	if err != nil {
		consumer.Close()
		return nil, fmt.Errorf("failed to create producer: %w", err)
	}

	// Initialize AI client from custom config
	aiConfig := cfg.Custom["ai_service"].(map[string]interface{})
	aiClient, err := aiservice.NewAnthropicClient(ctx, aiConfig)
	if err != nil {
		consumer.Close()
		producer.Close()
		return nil, fmt.Errorf("failed to create AI client: %w", err)
	}

	return &Agent{
		ctx:      ctx,
		logger:   logger,
		consumer: consumer,
		producer: producer,
		aiClient: aiClient,
	}, nil
}

// Run starts the agent's main loop
func (a *Agent) Run() error {
	a.logger.Info("Reasoning Agent is running and waiting for tasks...")

	for {
		select {
		case <-a.ctx.Done():
			a.consumer.Close()
			a.producer.Close()
			return nil
		default:
			msg, err := a.consumer.FetchMessage(a.ctx)
			if err != nil {
				if err == context.Canceled {
					continue
				}
				a.logger.Error("Failed to fetch message", zap.Error(err))
				continue
			}
			go a.handleMessage(msg)
		}
	}
}

// handleMessage processes a single reasoning request
func (a *Agent) handleMessage(msg kafka.Message) {
	headers := kafka.HeadersToMap(msg.Headers)
	l := a.logger.With(zap.String("correlation_id", headers["correlation_id"]))

	var req RequestPayload
	if err := json.Unmarshal(msg.Value, &req); err != nil {
		l.Error("Failed to unmarshal request", zap.Error(err))
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}

	// Build the reasoning prompt
	prompt := a.buildReasoningPrompt(req)

	// Call the AI service
	result, err := a.aiClient.GenerateText(a.ctx, prompt, nil)
	if err != nil {
		l.Error("AI reasoning call failed", zap.Error(err))
		a.sendErrorResponse(headers, "Failed to perform reasoning")
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}

	// Parse the AI response
	var responsePayload ResponsePayload
	if err := json.Unmarshal([]byte(result), &responsePayload); err != nil {
		l.Error("Failed to parse AI response", zap.Error(err))
		// Fallback response
		responsePayload = ResponsePayload{
			ReviewPassed: false,
			Score:        0,
			Suggestions:  []string{"Could not parse AI response"},
			Reasoning:    result,
		}
	}

	// Send response
	a.sendResponse(headers, responsePayload)

	// Commit message
	a.consumer.CommitMessages(context.Background(), msg)
}

// buildReasoningPrompt creates the prompt for the LLM
func (a *Agent) buildReasoningPrompt(req RequestPayload) string {
	return fmt.Sprintf(`You are a logical reasoning engine. Review the following content based on these criteria: %v.

Context: %v

Content to review: "%s"

Provide your analysis as a JSON object with the following structure:
{
    "review_passed": boolean,
    "score": number (0-10),
    "suggestions": ["suggestion1", "suggestion2", ...],
    "reasoning": "detailed explanation of your analysis"
}

Be thorough but concise in your reasoning.`,
		req.Data.ReviewCriteria,
		req.Data.BriefContext,
		req.Data.ContentToReview,
	)
}

// sendResponse sends a successful response
func (a *Agent) sendResponse(headers map[string]string, payload ResponsePayload) {
	responseBytes, _ := json.Marshal(payload)
	responseHeaders := map[string]string{
		"correlation_id": headers["correlation_id"],
		"causation_id":   headers["request_id"],
		"request_id":     uuid.NewString(),
	}

	if err := a.producer.Produce(a.ctx, responseTopic, responseHeaders,
		[]byte(headers["correlation_id"]), responseBytes); err != nil {
		a.logger.Error("Failed to produce response", zap.Error(err))
	}
}

// sendErrorResponse sends an error response
func (a *Agent) sendErrorResponse(headers map[string]string, errorMsg string) {
	payload := map[string]interface{}{
		"success": false,
		"error":   errorMsg,
	}
	responseBytes, _ := json.Marshal(payload)
	responseHeaders := map[string]string{
		"correlation_id": headers["correlation_id"],
		"causation_id":   headers["request_id"],
		"request_id":     uuid.NewString(),
	}

	a.producer.Produce(a.ctx, responseTopic, responseHeaders,
		[]byte(headers["correlation_id"]), responseBytes)
}

// StartHealthServer starts a simple HTTP server for health checks
func (a *Agent) StartHealthServer(port string) {
	http.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		w.WriteHeader(http.StatusOK)
		json.NewEncoder(w).Encode(map[string]string{
			"status": "healthy",
			"agent":  "reasoning-agent",
		})
	})

	go func() {
		a.logger.Info("Starting health server", zap.String("port", port))
		if err := http.ListenAndServe(":"+port, nil); err != nil {
			a.logger.Error("Health server failed", zap.Error(err))
		}
	}()
}
-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reasoning-agent
  labels:
    app: reasoning-agent
spec:
  selector:
    matchLabels:
      app: reasoning-agent
  template:
    metadata:
      labels:
        app: reasoning-agent
    spec:
      containers:
        - name: reasoning-agent
          image: your-container-registry/reasoning-agent:latest # Patched by overlays
          envFrom:
            - configMapRef:
                name: placeholder-config # Patched by overlays
            - secretRef:
                name: placeholder-secrets # Patched by overlays
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "1"
              memory: "2Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
bases:
  - ../../base

commonLabels:
  environment: development

resources:
  - ../../../../infrastructure/configs/development/configmap-dev.yaml
  - ../../../../infrastructure/configs/development/secrets-dev.yaml

patches:
  - path: patch-deployment-dev.yaml
    target:
      kind: Deployment
      name: reasoning-agent-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/development/patch-deployment-dev.yaml
- op: replace
  path: /spec/replicas
  value: 1 # Single replica for dev
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/reasoning-agent:latest
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-dev-config
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-dev-secrets
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "125m"
    memory: "256Mi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "500m"
    memory: "1Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/production/uk_991/patch-deployment.yaml
- op: replace
  path: /spec/replicas
  value: 3
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/reasoning-agent:v1.2.0 # Specific version tag
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-prod-config-uk001
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-prod-secrets-uk001
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "500m"
    memory: "1Gi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "1.5"
    memory: "4Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/production/uk_991/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-prod
bases:
  - ../../../base

commonLabels:
  environment: production
  region: uk

resources:
  - ../../../../../infrastructure/configs/production/uk_001/configmap-prod-uk001.yaml
  - ../../../../../infrastructure/configs/production/uk_001/secrets-prod-uk001.yaml

patches:
  - path: patch-deployment.yaml
    target:
      kind: Deployment
      name: reasoning-agent-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/README.md
Just like the agent-chassis, this is a specialized worker that communicates only via Kafka, so it only needs a Deployment manifest.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2220-reasoning-agent/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2220-reasoning-agent/variables.tf
# No variables nee# No variables needed as the path is static for this service definition.ded as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2220-reasoning-agent/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-reasoning-agent-dev"
    config_path   = "~/.kube/config"
  }
}

module "reasoning_agent_deployment_dev" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the DEVELOPMENT overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/reasoning-agent/overlays/development"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2220-reasoning-agent/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the development reasoning-agent."
  value       = module.reasoning_agent_deployment_dev.status
}-------------------------------------------------
filepath = ./platform/kafka/types.go
// FILE: platform/kafka/types.go
package kafka

import (
	"github.com/segmentio/kafka-go"
)

// Message wraps kafka-go Message to implement our interface
type Message = kafka.Message

// Header wraps kafka-go Header
type Header = kafka.Header
-------------------------------------------------
filepath = ./platform/kafka/consumer.go
// FILE: platform/kafka/consumer.go (updated version)
package kafka

import (
	"context"
	"fmt"

	"github.com/segmentio/kafka-go"
	"go.uber.org/zap"
)

// Consumer wraps the kafka-go reader for standardized consumption
type Consumer struct {
	reader *kafka.Reader
	logger *zap.Logger
}

// NewConsumer creates a new standardized Kafka consumer
func NewConsumer(brokers []string, topic, groupID string, logger *zap.Logger) (*Consumer, error) {
	if len(brokers) == 0 {
		return nil, fmt.Errorf("kafka brokers list cannot be empty")
	}
	if topic == "" {
		return nil, fmt.Errorf("kafka topic cannot be empty")
	}
	if groupID == "" {
		return nil, fmt.Errorf("kafka groupID cannot be empty")
	}

	reader := kafka.NewReader(kafka.ReaderConfig{
		Brokers:        brokers,
		GroupID:        groupID,
		Topic:          topic,
		MinBytes:       10e3, // 10KB
		MaxBytes:       10e6, // 10MB
		CommitInterval: 0,    // Manual commit
	})

	logger.Info("Kafka consumer created",
		zap.Strings("brokers", brokers),
		zap.String("topic", topic),
		zap.String("groupID", groupID),
	)

	return &Consumer{
		reader: reader,
		logger: logger,
	}, nil
}

// FetchMessage fetches the next message from the topic
// Returns the native kafka.Message type
func (c *Consumer) FetchMessage(ctx context.Context) (Message, error) {
	msg, err := c.reader.FetchMessage(ctx)
	if err != nil {
		if err == context.Canceled {
			return Message{}, err
		}
		c.logger.Error("Failed to fetch message from Kafka", zap.Error(err))
		return Message{}, err
	}
	return msg, nil
}

// CommitMessages commits the offset for the given messages
func (c *Consumer) CommitMessages(ctx context.Context, msgs ...Message) error {
	err := c.reader.CommitMessages(ctx, msgs...)
	if err != nil {
		c.logger.Error("Failed to commit Kafka messages", zap.Error(err))
	}
	return err
}

// Close gracefully closes the consumer's reader
func (c *Consumer) Close() error {
	c.logger.Info("Closing Kafka consumer...")
	return c.reader.Close()
}
-------------------------------------------------
filepath = ./platform/kafka/utils.go
// FILE: platform/kafka/utils.go
package kafka

import "github.com/segmentio/kafka-go"

// HeadersToMap converts Kafka headers to a map for easier access
func HeadersToMap(headers []kafka.Header) map[string]string {
	result := make(map[string]string)
	for _, h := range headers {
		result[h.Key] = string(h.Value)
	}
	return result
}
-------------------------------------------------
filepath = ./platform/kafka/mock_producer.go
// FILE: platform/kafka/mock_producer.go
// Mock producer for testing
package kafka

import (
	"context"
	"github.com/stretchr/testify/mock"
)

// MockProducer is a mock implementation of the Producer interface
type MockProducer struct {
	mock.Mock
}

// Produce mocks the Produce method
func (m *MockProducer) Produce(ctx context.Context, topic string, headers map[string]string, key, value []byte) error {
	args := m.Called(ctx, topic, headers, key, value)
	return args.Error(0)
}

// Close mocks the Close method
func (m *MockProducer) Close() error {
	args := m.Called()
	return args.Error(0)
}
-------------------------------------------------
filepath = ./platform/kafka/producer.go
// FILE: platform/kafka/producer.go
package kafka

import (
	"context"
	"fmt"
	"time"

	"github.com/segmentio/kafka-go"
	"go.uber.org/zap"
)

// Producer defines the interface for Kafka message production
type Producer interface {
	Produce(ctx context.Context, topic string, headers map[string]string, key, value []byte) error
	Close() error
}

// KafkaProducer wraps the kafka-go writer for standardized message production
type KafkaProducer struct {
	writer *kafka.Writer
	logger *zap.Logger
}

// NewProducer creates a new standardized Kafka producer
func NewProducer(brokers []string, logger *zap.Logger) (Producer, error) {
	if len(brokers) == 0 {
		return nil, fmt.Errorf("kafka brokers list cannot be empty")
	}

	writer := &kafka.Writer{
		Addr:         kafka.TCP(brokers...),
		Balancer:     &kafka.LeastBytes{},
		RequiredAcks: kafka.RequireAll,
		Async:        false,
		WriteTimeout: 10 * time.Second,
	}

	logger.Info("Kafka producer created", zap.Strings("brokers", brokers))

	return &KafkaProducer{
		writer: writer,
		logger: logger,
	}, nil
}

// Produce sends a message to a specific topic with standard headers
func (p *KafkaProducer) Produce(ctx context.Context, topic string, headers map[string]string, key, value []byte) error {
	kafkaHeaders := make([]kafka.Header, 0, len(headers))
	for k, v := range headers {
		kafkaHeaders = append(kafkaHeaders, kafka.Header{Key: k, Value: []byte(v)})
	}

	msg := kafka.Message{
		Topic:   topic,
		Key:     key,
		Value:   value,
		Headers: kafkaHeaders,
		Time:    time.Now().UTC(),
	}

	err := p.writer.WriteMessages(ctx, msg)
	if err != nil {
		p.logger.Error("Failed to produce Kafka message",
			zap.String("topic", topic),
			zap.Error(err),
		)
		return fmt.Errorf("failed to write message to kafka: %w", err)
	}

	p.logger.Debug("Successfully produced message", zap.String("topic", topic), zap.String("key", string(key)))
	return nil
}

// Close gracefully closes the producer's writer
func (p *KafkaProducer) Close() error {
	p.logger.Info("Closing Kafka producer...")
	return p.writer.Close()
}
-------------------------------------------------
filepath = ./platform/kafka/producer_test.go
// FILE: platform/kafka/producer_test.go
package kafka

import (
	"testing"

	"github.com/stretchr/testify/assert"
	"go.uber.org/zap"
)

func TestNewProducer(t *testing.T) {
	// Test with empty brokers
	_, err := NewProducer([]string{}, zap.NewNop())
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "kafka brokers list cannot be empty")

	// Test with valid brokers (won't actually connect in unit test)
	producer, err := NewProducer([]string{"localhost:9092"}, zap.NewNop())
	assert.NoError(t, err)
	assert.NotNil(t, producer)

	// Clean up
	producer.Close()
}

func TestProducerInterface(t *testing.T) {
	// Ensure KafkaProducer implements Producer interface
	var _ Producer = (*KafkaProducer)(nil)
}
-------------------------------------------------
filepath = ./platform/logger/logger.go
// FILE: platform/logger/logger.go
package logger

import (
	"fmt"
	"log"

	"go.uber.org/zap"
	"go.uber.org/zap/zapcore"
)

// New creates a new Zap logger with a specified log level
func New(logLevel string) (*zap.Logger, error) {
	var level zapcore.Level
	if err := level.UnmarshalText([]byte(logLevel)); err != nil {
		log.Printf("logger: invalid log level '%s', defaulting to 'info'", logLevel)
		level = zapcore.InfoLevel
	}

	config := zap.Config{
		Level:       zap.NewAtomicLevelAt(level),
		Development: false,
		Encoding:    "json",
		EncoderConfig: zapcore.EncoderConfig{
			TimeKey:        "ts",
			LevelKey:       "level",
			NameKey:        "logger",
			CallerKey:      "caller",
			MessageKey:     "msg",
			StacktraceKey:  "stacktrace",
			LineEnding:     zapcore.DefaultLineEnding,
			EncodeLevel:    zapcore.LowercaseLevelEncoder,
			EncodeTime:     zapcore.ISO8601TimeEncoder,
			EncodeDuration: zapcore.SecondsDurationEncoder,
			EncodeCaller:   zapcore.ShortCallerEncoder,
		},
		OutputPaths:      []string{"stdout"},
		ErrorOutputPaths: []string{"stderr"},
	}

	logger, err := config.Build()
	if err != nil {
		return nil, fmt.Errorf("logger: failed to initialize zap logger: %w", err)
	}

	logger.Info("Logger initialized successfully", zap.String("level", level.String()))
	return logger, nil
}
-------------------------------------------------
filepath = ./platform/resilience/circuit_breaker.go
// FILE: platform/resilience/circuit_breaker.go
package resilience

import (
	"context"
	"net/http"
	"time"

	"github.com/sony/gobreaker"
	"go.uber.org/zap"
)

// CircuitBreakerConfig holds configuration for circuit breakers
type CircuitBreakerConfig struct {
	Name                string
	MaxRequests         uint32
	Interval            time.Duration
	Timeout             time.Duration
	ConsecutiveFailures uint32
	FailureRatio        float64
}

// DefaultCircuitBreakerConfig returns sensible defaults
func DefaultCircuitBreakerConfig(name string) CircuitBreakerConfig {
	return CircuitBreakerConfig{
		Name:                name,
		MaxRequests:         3,
		Interval:            60 * time.Second,
		Timeout:             60 * time.Second,
		ConsecutiveFailures: 5,
		FailureRatio:        0.6,
	}
}

// CircuitBreaker wraps the gobreaker implementation
type CircuitBreaker struct {
	breaker *gobreaker.CircuitBreaker
	logger  *zap.Logger
	config  CircuitBreakerConfig
}

// NewCircuitBreaker creates a new circuit breaker
func NewCircuitBreaker(config CircuitBreakerConfig, logger *zap.Logger) *CircuitBreaker {
	settings := gobreaker.Settings{
		Name:        config.Name,
		MaxRequests: config.MaxRequests,
		Interval:    config.Interval,
		Timeout:     config.Timeout,
		ReadyToTrip: func(counts gobreaker.Counts) bool {
			failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
			return counts.Requests >= config.ConsecutiveFailures && failureRatio >= config.FailureRatio
		},
		OnStateChange: func(name string, from gobreaker.State, to gobreaker.State) {
			logger.Warn("Circuit breaker state change",
				zap.String("name", name),
				zap.String("from", from.String()),
				zap.String("to", to.String()),
			)
		},
	}

	return &CircuitBreaker{
		breaker: gobreaker.NewCircuitBreaker(settings),
		logger:  logger,
		config:  config,
	}
}

// Execute runs a function through the circuit breaker
func (cb *CircuitBreaker) Execute(fn func() (interface{}, error)) (interface{}, error) {
	return cb.breaker.Execute(fn)
}

// ExecuteWithContext runs a function with context through the circuit breaker
func (cb *CircuitBreaker) ExecuteWithContext(ctx context.Context, fn func(context.Context) (interface{}, error)) (interface{}, error) {
	return cb.breaker.Execute(func() (interface{}, error) {
		return fn(ctx)
	})
}

// State returns the current state of the circuit breaker
func (cb *CircuitBreaker) State() string {
	return cb.breaker.State().String()
}

// IsOpen returns true if the circuit breaker is open
func (cb *CircuitBreaker) IsOpen() bool {
	return cb.breaker.State() == gobreaker.StateOpen
}

// Counts returns the current counts
func (cb *CircuitBreaker) Counts() gobreaker.Counts {
	return cb.breaker.Counts()
}

// HTTPClient wraps an HTTP client with circuit breaker functionality
type HTTPClientWithBreaker struct {
	client  HTTPDoer
	Breaker *CircuitBreaker
	logger  *zap.Logger
}

// HTTPDoer interface for HTTP client
type HTTPDoer interface {
	Do(req *http.Request) (*http.Response, error)
}

// NewHTTPClientWithBreaker creates a new HTTP client with circuit breaker
func NewHTTPClientWithBreaker(client HTTPDoer, config CircuitBreakerConfig, logger *zap.Logger) *HTTPClientWithBreaker {
	return &HTTPClientWithBreaker{
		client:  client,
		Breaker: NewCircuitBreaker(config, logger),
		logger:  logger,
	}
}

// Do executes an HTTP request through the circuit breaker
func (c *HTTPClientWithBreaker) Do(req *http.Request) (*http.Response, error) {
	result, err := c.Breaker.Execute(func() (interface{}, error) {
		return c.client.Do(req)
	})

	if err != nil {
		return nil, err
	}

	return result.(*http.Response), nil
}

// State returns the current state of the circuit breaker
func (c *HTTPClientWithBreaker) State() string {
	return c.Breaker.State()
}

// Counts returns the current counts
func (c *HTTPClientWithBreaker) Counts() gobreaker.Counts {
	return c.Breaker.Counts()
}

// IsCircuitBreakerError checks if an error is from a circuit breaker
func IsCircuitBreakerError(err error) bool {
	if err == nil {
		return false
	}
	return err == gobreaker.ErrOpenState || err == gobreaker.ErrTooManyRequests
}
-------------------------------------------------
filepath = ./platform/storage/s3.go
// FILE: platform/storage/s3.go
package storage

import (
	"context"
	"fmt"
	"io"
	"os"
	"strings"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	platform_config "github.com/gqls/agentchassis/platform/config"
)

// S3Client implements the Client interface for S3-compatible services
type S3Client struct {
	client *s3.Client
	bucket string
}

// NewS3Client creates a new client for interacting with S3 or MinIO
func NewS3Client(ctx context.Context, cfg platform_config.ObjectStorageConfig) (*S3Client, error) {
	accessKey := os.Getenv(cfg.AccessKeyEnvVar)
	secretKey := os.Getenv(cfg.SecretKeyEnvVar)

	if accessKey == "" || secretKey == "" {
		return nil, fmt.Errorf("object storage credentials not found in environment variables (%s, %s)",
			cfg.AccessKeyEnvVar, cfg.SecretKeyEnvVar)
	}

	resolver := aws.EndpointResolverWithOptionsFunc(func(service, region string, options ...interface{}) (aws.Endpoint, error) {
		return aws.Endpoint{
			URL:           cfg.Endpoint,
			SigningRegion: "us-east-1", // This can be anything for MinIO
		}, nil
	})

	awsCfg, err := config.LoadDefaultConfig(ctx,
		config.WithEndpointResolverWithOptions(resolver),
		config.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(accessKey, secretKey, "")),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to load s3 config: %w", err)
	}

	// For MinIO, you must use path-style addressing
	s3Client := s3.NewFromConfig(awsCfg, func(o *s3.Options) {
		o.UsePathStyle = true
	})

	return &S3Client{
		client: s3Client,
		bucket: cfg.Bucket,
	}, nil
}

// Upload puts a new object into the storage bucket
func (c *S3Client) Upload(ctx context.Context, key, contentType string, body io.Reader) (string, error) {
	_, err := c.client.PutObject(ctx, &s3.PutObjectInput{
		Bucket:      aws.String(c.bucket),
		Key:         aws.String(key),
		Body:        body,
		ContentType: aws.String(contentType),
	})
	if err != nil {
		return "", fmt.Errorf("failed to upload object to s3: %w", err)
	}
	// Return the S3 URI for the object
	return fmt.Sprintf("s3://%s/%s", c.bucket, key), nil
}

// Download retrieves an object from the storage bucket
func (c *S3Client) Download(ctx context.Context, key string) (io.ReadCloser, error) {
	output, err := c.client.GetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(c.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to download object from s3: %w", err)
	}
	return output.Body, nil
}

// Delete removes an object from storage
func (c *S3Client) Delete(ctx context.Context, key string) error {
	_, err := c.client.DeleteObject(ctx, &s3.DeleteObjectInput{
		Bucket: aws.String(c.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		return fmt.Errorf("failed to delete object: %w", err)
	}
	return nil
}

// Exists checks if an object exists
func (c *S3Client) Exists(ctx context.Context, key string) (bool, error) {
	_, err := c.client.HeadObject(ctx, &s3.HeadObjectInput{
		Bucket: aws.String(c.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		// Check if it's a not found error
		if strings.Contains(err.Error(), "NotFound") {
			return false, nil
		}
		return false, fmt.Errorf("failed to check object existence: %w", err)
	}
	return true, nil
}

// ListObjects lists objects with a given prefix
func (c *S3Client) ListObjects(ctx context.Context, prefix string) ([]ObjectInfo, error) {
	paginator := s3.NewListObjectsV2Paginator(c.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(c.bucket),
		Prefix: aws.String(prefix),
	})

	var objects []ObjectInfo
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			return nil, fmt.Errorf("failed to list objects: %w", err)
		}

		for _, obj := range page.Contents {
			objects = append(objects, ObjectInfo{
				Key:          aws.ToString(obj.Key),
				Size:         aws.ToInt64(obj.Size),
				LastModified: aws.ToTime(obj.LastModified),
				ETag:         aws.ToString(obj.ETag),
			})
		}
	}

	return objects, nil
}

// GetPresignedURL generates a temporary access URL
func (c *S3Client) GetPresignedURL(ctx context.Context, key string, expiryMinutes int) (string, error) {
	presignClient := s3.NewPresignClient(c.client)

	request, err := presignClient.PresignGetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(c.bucket),
		Key:    aws.String(key),
	}, func(opts *s3.PresignOptions) {
		opts.Expires = time.Duration(expiryMinutes) * time.Minute
	})

	if err != nil {
		return "", fmt.Errorf("failed to create presigned URL: %w", err)
	}

	return request.URL, nil
}
-------------------------------------------------
filepath = ./platform/storage/interface.go
// FILE: platform/storage/interface.go
package storage

import (
	"context"
	"io"
	"time"
)

// Client defines the interface for object storage operations
type Client interface {
	// Upload stores an object and returns its URI
	Upload(ctx context.Context, key, contentType string, body io.Reader) (string, error)

	// Download retrieves an object by its key
	Download(ctx context.Context, key string) (io.ReadCloser, error)

	// Delete removes an object
	Delete(ctx context.Context, key string) error

	// Exists checks if an object exists
	Exists(ctx context.Context, key string) (bool, error)

	// ListObjects lists objects with a given prefix
	ListObjects(ctx context.Context, prefix string) ([]ObjectInfo, error)

	// GetPresignedURL generates a temporary access URL
	GetPresignedURL(ctx context.Context, key string, expiry int) (string, error)
}

// ObjectInfo contains metadata about a stored object
type ObjectInfo struct {
	Key          string
	Size         int64
	LastModified time.Time
	ContentType  string
	ETag         string
}
-------------------------------------------------
filepath = ./platform/errors/errors.go
// FILE: platform/errors/errors.go
package errors

import (
	"encoding/json"
	"fmt"
	"net/http"
	"time"
)

// ErrorCode represents standardized error codes across the platform
type ErrorCode string

const (
	// General errors
	ErrInternal     ErrorCode = "INTERNAL_ERROR"
	ErrValidation   ErrorCode = "VALIDATION_ERROR"
	ErrNotFound     ErrorCode = "NOT_FOUND"
	ErrUnauthorized ErrorCode = "UNAUTHORIZED"
	ErrForbidden    ErrorCode = "FORBIDDEN"
	ErrConflict     ErrorCode = "CONFLICT"
	ErrRateLimited  ErrorCode = "RATE_LIMITED"

	// Workflow errors
	ErrWorkflowInvalid  ErrorCode = "WORKFLOW_INVALID"
	ErrWorkflowTimeout  ErrorCode = "WORKFLOW_TIMEOUT"
	ErrWorkflowFailed   ErrorCode = "WORKFLOW_FAILED"
	ErrInsufficientFuel ErrorCode = "INSUFFICIENT_FUEL"

	// Agent errors
	ErrAgentNotFound   ErrorCode = "AGENT_NOT_FOUND"
	ErrAgentTimeout    ErrorCode = "AGENT_TIMEOUT"
	ErrAgentOverloaded ErrorCode = "AGENT_OVERLOADED"

	// External service errors
	ErrExternalService ErrorCode = "EXTERNAL_SERVICE_ERROR"
	ErrAIServiceError  ErrorCode = "AI_SERVICE_ERROR"
)

// DomainError represents a standardized error in the platform
type DomainError struct {
	Code       ErrorCode              `json:"code"`
	Message    string                 `json:"message"`
	Details    map[string]interface{} `json:"details,omitempty"`
	Cause      error                  `json:"-"`
	Timestamp  time.Time              `json:"timestamp"`
	TraceID    string                 `json:"trace_id,omitempty"`
	Retryable  bool                   `json:"retryable"`
	RetryAfter *time.Duration         `json:"retry_after,omitempty"`
}

// Error implements the error interface
func (e *DomainError) Error() string {
	if e.Cause != nil {
		return fmt.Sprintf("%s: %s (caused by: %v)", e.Code, e.Message, e.Cause)
	}
	return fmt.Sprintf("%s: %s", e.Code, e.Message)
}

// Unwrap allows for error chain inspection
func (e *DomainError) Unwrap() error {
	return e.Cause
}

// HTTPStatus returns the appropriate HTTP status code for the error
func (e *DomainError) HTTPStatus() int {
	switch e.Code {
	case ErrValidation:
		return http.StatusBadRequest
	case ErrUnauthorized:
		return http.StatusUnauthorized
	case ErrForbidden:
		return http.StatusForbidden
	case ErrNotFound:
		return http.StatusNotFound
	case ErrConflict:
		return http.StatusConflict
	case ErrRateLimited:
		return http.StatusTooManyRequests
	case ErrAgentOverloaded:
		return http.StatusServiceUnavailable
	default:
		return http.StatusInternalServerError
	}
}

// MarshalJSON customizes JSON serialization
func (e *DomainError) MarshalJSON() ([]byte, error) {
	type Alias DomainError
	return json.Marshal(&struct {
		*Alias
		HTTPStatus int `json:"http_status"`
	}{
		Alias:      (*Alias)(e),
		HTTPStatus: e.HTTPStatus(),
	})
}

// ErrorBuilder provides a fluent interface for building errors
type ErrorBuilder struct {
	err *DomainError
}

// New creates a new error builder
func New(code ErrorCode, message string) *ErrorBuilder {
	return &ErrorBuilder{
		err: &DomainError{
			Code:      code,
			Message:   message,
			Timestamp: time.Now().UTC(),
			Retryable: false,
		},
	}
}

// WithCause adds an underlying cause
func (b *ErrorBuilder) WithCause(cause error) *ErrorBuilder {
	b.err.Cause = cause
	return b
}

// WithDetails adds additional details
func (b *ErrorBuilder) WithDetails(details map[string]interface{}) *ErrorBuilder {
	b.err.Details = details
	return b
}

// WithDetail adds a single detail
func (b *ErrorBuilder) WithDetail(key string, value interface{}) *ErrorBuilder {
	if b.err.Details == nil {
		b.err.Details = make(map[string]interface{})
	}
	b.err.Details[key] = value
	return b
}

// WithTraceID adds a trace ID for correlation
func (b *ErrorBuilder) WithTraceID(traceID string) *ErrorBuilder {
	b.err.TraceID = traceID
	return b
}

// AsRetryable marks the error as retryable
func (b *ErrorBuilder) AsRetryable(retryAfter *time.Duration) *ErrorBuilder {
	b.err.Retryable = true
	b.err.RetryAfter = retryAfter
	return b
}

// Build returns the constructed error
func (b *ErrorBuilder) Build() *DomainError {
	return b.err
}

// Helper functions for common errors

// NotFound creates a not found error
func NotFound(resource string, id string) *DomainError {
	return New(ErrNotFound, fmt.Sprintf("%s not found", resource)).
		WithDetail("resource", resource).
		WithDetail("id", id).
		Build()
}

// ValidationError creates a validation error
func ValidationError(field string, issue string) *DomainError {
	return New(ErrValidation, "Validation failed").
		WithDetail("field", field).
		WithDetail("issue", issue).
		Build()
}

// InternalError creates an internal error
func InternalError(message string, cause error) *DomainError {
	return New(ErrInternal, message).
		WithCause(cause).
		Build()
}

// InsufficientFuel creates an insufficient fuel error
func InsufficientFuel(required, available int, action string) *DomainError {
	return New(ErrInsufficientFuel, "Insufficient fuel for operation").
		WithDetail("required", required).
		WithDetail("available", available).
		WithDetail("action", action).
		Build()
}

// IsRetryable checks if an error is retryable
func IsRetryable(err error) bool {
	if domainErr, ok := err.(*DomainError); ok {
		return domainErr.Retryable
	}
	return false
}

// GetRetryAfter gets the retry after duration if available
func GetRetryAfter(err error) *time.Duration {
	if domainErr, ok := err.(*DomainError); ok {
		return domainErr.RetryAfter
	}
	return nil
}
-------------------------------------------------
filepath = ./platform/aiservice/anthropic.go
// FILE: platform/aiservice/anthropic.go
package aiservice

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
)

// AnthropicClient implements the AIService interface for Anthropic's Claude
type AnthropicClient struct {
	apiKey     string
	model      string
	httpClient *http.Client
}

// NewAnthropicClient creates a new Anthropic client
func NewAnthropicClient(ctx context.Context, config map[string]interface{}) (*AnthropicClient, error) {
	apiKeyEnvVar := config["api_key_env_var"].(string)
	apiKey := os.Getenv(apiKeyEnvVar)
	if apiKey == "" {
		return nil, fmt.Errorf("API key not found in environment variable %s", apiKeyEnvVar)
	}

	model := config["model"].(string)

	return &AnthropicClient{
		apiKey:     apiKey,
		model:      model,
		httpClient: &http.Client{},
	}, nil
}

// GenerateText generates text using Claude
func (c *AnthropicClient) GenerateText(ctx context.Context, prompt string, options map[string]interface{}) (string, error) {
	// Build request
	requestBody := map[string]interface{}{
		"model":       c.model,
		"max_tokens":  2048,
		"temperature": 0.7,
		"messages": []map[string]string{
			{
				"role":    "user",
				"content": prompt,
			},
		},
	}

	// Override with provided options
	if options != nil {
		if maxTokens, ok := options["max_tokens"]; ok {
			requestBody["max_tokens"] = maxTokens
		}
		if temperature, ok := options["temperature"]; ok {
			requestBody["temperature"] = temperature
		}
	}

	jsonBody, err := json.Marshal(requestBody)
	if err != nil {
		return "", fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	req, err := http.NewRequestWithContext(ctx, "POST", "https://api.anthropic.com/v1/messages", bytes.NewBuffer(jsonBody))
	if err != nil {
		return "", fmt.Errorf("failed to create request: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("x-api-key", c.apiKey)
	req.Header.Set("anthropic-version", "2023-06-01")

	// Execute request
	resp, err := c.httpClient.Do(req)
	if err != nil {
		return "", fmt.Errorf("failed to execute request: %w", err)
	}
	defer resp.Body.Close()

	// Read response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", fmt.Errorf("failed to read response: %w", err)
	}

	if resp.StatusCode != http.StatusOK {
		return "", fmt.Errorf("API request failed with status %d: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var response struct {
		Content []struct {
			Text string `json:"text"`
		} `json:"content"`
	}

	if err := json.Unmarshal(body, &response); err != nil {
		return "", fmt.Errorf("failed to parse response: %w", err)
	}

	if len(response.Content) == 0 {
		return "", fmt.Errorf("no content in response")
	}

	return response.Content[0].Text, nil
}

// GenerateEmbedding generates embeddings (not implemented for Anthropic)
func (c *AnthropicClient) GenerateEmbedding(ctx context.Context, text string) ([]float32, error) {
	return nil, fmt.Errorf("embedding generation not supported by Anthropic")
}
-------------------------------------------------
filepath = ./platform/aiservice/interface.go
// FILE: platform/aiservice/interface.go
package aiservice

import "context"

// AIService defines the interface for AI providers
type AIService interface {
	GenerateText(ctx context.Context, prompt string, options map[string]interface{}) (string, error)
	GenerateEmbedding(ctx context.Context, text string) ([]float32, error)
}

// TextGenerationOptions contains common options for text generation
type TextGenerationOptions struct {
	Temperature float64
	MaxTokens   int
	Model       string
}
-------------------------------------------------
filepath = ./platform/memory/service.go
// FILE: platform/memory/service.go
package memory

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/aiservice"
	"github.com/gqls/agentchassis/platform/database"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// Service handles memory operations for agents
type Service struct {
	pool       *pgxpool.Pool
	aiClient   aiservice.AIService
	logger     *zap.Logger
	memoryRepo *database.MemoryRepository
}

// NewService creates a new memory service
func NewService(pool *pgxpool.Pool, aiClient aiservice.AIService, logger *zap.Logger) *Service {
	return &Service{
		pool:       pool,
		aiClient:   aiClient,
		logger:     logger,
		memoryRepo: database.NewMemoryRepository(pool, logger),
	}
}

// StoreMemory stores a memory entry for an agent
func (s *Service) StoreMemory(ctx context.Context, agentID uuid.UUID, entry models.MemoryEntry) error {
	// Generate embedding
	embedding, err := s.aiClient.GenerateEmbedding(ctx, entry.Content)
	if err != nil {
		s.logger.Error("Failed to generate embedding", zap.Error(err))
		return fmt.Errorf("failed to generate embedding: %w", err)
	}

	// Add timestamp to metadata
	if entry.Metadata == nil {
		entry.Metadata = make(map[string]interface{})
	}
	entry.Metadata["timestamp"] = entry.Timestamp
	entry.Metadata["type"] = entry.Type

	// Store in database
	return s.memoryRepo.StoreMemory(ctx, agentID, entry.Content, embedding, entry.Metadata)
}

// RetrieveRelevantMemories retrieves memories relevant to a query
func (s *Service) RetrieveRelevantMemories(ctx context.Context, agentID uuid.UUID, query string, limit int) ([]models.MemoryEntry, error) {
	// Generate query embedding
	queryEmbedding, err := s.aiClient.GenerateEmbedding(ctx, query)
	if err != nil {
		s.logger.Error("Failed to generate query embedding", zap.Error(err))
		return nil, fmt.Errorf("failed to generate query embedding: %w", err)
	}

	// Search for similar memories
	records, err := s.memoryRepo.SearchMemory(ctx, agentID, queryEmbedding, limit)
	if err != nil {
		return nil, err
	}

	// Convert to memory entries
	entries := make([]models.MemoryEntry, len(records))
	for i, record := range records {
		entries[i] = models.MemoryEntry{
			Content:  record.Content,
			Metadata: record.Metadata,
		}

		// Extract type and timestamp from metadata
		if typeStr, ok := record.Metadata["type"].(string); ok {
			entries[i].Type = typeStr
		}
		if timestampStr, ok := record.Metadata["timestamp"].(string); ok {
			if t, err := time.Parse(time.RFC3339, timestampStr); err == nil {
				entries[i].Timestamp = t
			}
		}
	}

	return entries, nil
}

// ProcessWorkflowMemory handles memory storage for workflow steps
func (s *Service) ProcessWorkflowMemory(ctx context.Context, agentID uuid.UUID, config models.MemoryConfiguration, step models.Step, input, output interface{}) error {
	// Check if memory is enabled and this step should store memory
	if !config.Enabled || !step.StoreMemory {
		return nil
	}

	// Create memory entry
	entry := models.MemoryEntry{
		Type:      "workflow_step",
		Timestamp: time.Now(),
		Metadata: map[string]interface{}{
			"step_action":      step.Action,
			"step_description": step.Description,
		},
	}

	// Format content based on input and output
	content := map[string]interface{}{
		"action": step.Action,
		"input":  input,
		"output": output,
	}

	contentBytes, err := json.MarshalIndent(content, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal memory content: %w", err)
	}
	entry.Content = string(contentBytes)

	// Store the memory
	return s.StoreMemory(ctx, agentID, entry)
}

// GetMemoryContext retrieves relevant memories for a given context
func (s *Service) GetMemoryContext(ctx context.Context, agentID uuid.UUID, config models.MemoryConfiguration, currentContext string) ([]models.MemoryEntry, error) {
	if !config.Enabled {
		return nil, nil
	}

	count := config.RetrievalCount
	if count == 0 {
		count = 5 // default
	}

	return s.RetrieveRelevantMemories(ctx, agentID, currentContext, count)
}
-------------------------------------------------
filepath = ./platform/messaging/context.go
// FILE: platform/messaging/context.go
package messaging

import (
	"encoding/json"
	"fmt"
	"time"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/platform/kafka"
	"go.uber.org/zap"
)

// MessageContext holds the context for processing a single message
type MessageContext struct {
	Message   kafka.Message
	Headers   map[string]string
	Action    string
	StartTime time.Time
	Logger    *zap.Logger
}

// ExtractAction extracts the action from the message payload
func (m *MessageContext) ExtractAction() error {
	var payload struct {
		Action string `json:"action"`
	}
	if err := json.Unmarshal(m.Message.Value, &payload); err != nil {
		return fmt.Errorf("failed to extract action: %w", err)
	}
	m.Action = payload.Action
	return nil
}

// ValidateHeaders ensures required headers are present
func (m *MessageContext) ValidateHeaders() error {
	required := []string{"correlation_id", "request_id", "client_id", "agent_instance_id"}
	for _, key := range required {
		if m.Headers[key] == "" {
			return fmt.Errorf("missing required header: %s", key)
		}
	}
	return nil
}

// CreateResponseHeaders creates headers for a response message
func (m *MessageContext) CreateResponseHeaders(agentType string) map[string]string {
	return map[string]string{
		"correlation_id": m.Headers["correlation_id"],
		"causation_id":   m.Headers["request_id"],
		"request_id":     uuid.NewString(),
		"client_id":      m.Headers["client_id"],
		"agent_type":     agentType,
		"timestamp":      time.Now().UTC().Format(time.RFC3339),
	}
}
-------------------------------------------------
filepath = ./platform/messaging/processor.go
// FILE: platform/messaging/processor.go
package messaging

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/errors"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/gqls/agentchassis/platform/observability"
	"github.com/gqls/agentchassis/platform/orchestration"
	"github.com/gqls/agentchassis/platform/validation"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// MessageProcessor handles processing of Kafka messages for agents
type MessageProcessor struct {
	agentType    string
	db           *pgxpool.Pool
	producer     kafka.Producer
	orchestrator *orchestration.SagaCoordinator
	validator    *validation.WorkflowValidator
	configLoader *config.AgentConfigLoader
	logger       *zap.Logger
}

// NewMessageProcessor creates a new message processor
func NewMessageProcessor(
	agentType string,
	db *pgxpool.Pool,
	producer kafka.Producer,
	orchestrator *orchestration.SagaCoordinator,
	validator *validation.WorkflowValidator,
	logger *zap.Logger,
) *MessageProcessor {
	return &MessageProcessor{
		agentType:    agentType,
		db:           db,
		producer:     producer,
		orchestrator: orchestrator,
		validator:    validator,
		configLoader: config.NewAgentConfigLoader(logger),
		logger:       logger,
	}
}

// ProcessMessage handles a single message
func (p *MessageProcessor) ProcessMessage(ctx context.Context, msg kafka.Message) error {
	startTime := time.Now()
	headers := kafka.HeadersToMap(msg.Headers)

	// Create a message context for this specific message
	msgCtx := &MessageContext{
		Message:   msg,
		Headers:   headers,
		StartTime: startTime,
		Logger: p.logger.With(
			zap.String("correlation_id", headers["correlation_id"]),
			zap.String("request_id", headers["request_id"]),
			zap.String("client_id", headers["client_id"]),
			zap.String("agent_instance_id", headers["agent_instance_id"]),
		),
	}

	// Extract action
	if err := msgCtx.ExtractAction(); err != nil {
		return p.handleError(ctx, msgCtx, err, "invalid_payload")
	}

	// Record metrics
	observability.AgentTasksReceived.WithLabelValues(p.agentType, msgCtx.Action).Inc()
	defer func() {
		observability.AgentProcessingDuration.WithLabelValues(p.agentType, msgCtx.Action).
			Observe(time.Since(startTime).Seconds())
	}()

	// Process the message
	if err := p.process(ctx, msgCtx); err != nil {
		return p.handleError(ctx, msgCtx, err, "processing_failed")
	}

	// Success
	observability.AgentTasksProcessed.WithLabelValues(p.agentType, msgCtx.Action, "success").Inc()
	return nil
}

func (p *MessageProcessor) process(ctx context.Context, msgCtx *MessageContext) error {
	// Validate headers
	if err := msgCtx.ValidateHeaders(); err != nil {
		return errors.ValidationError("headers", err.Error())
	}

	// Load agent configuration
	agentConfig, err := p.configLoader.LoadFromDatabase(
		ctx,
		p.db,
		msgCtx.Headers["client_id"],
		msgCtx.Headers["agent_instance_id"],
		p.agentType,
	)
	if err != nil {
		return errors.InternalError("Failed to load configuration", err)
	}

	// Validate workflow
	if err := p.validator.ValidateWorkflowPlan(agentConfig.Workflow); err != nil {
		return errors.New(errors.ErrWorkflowInvalid, "Invalid workflow configuration").
			WithCause(err).
			WithDetail("workflow_metrics", p.validator.GetWorkflowMetrics(agentConfig.Workflow)).
			Build()
	}

	// Execute workflow
	return p.executeWorkflow(ctx, msgCtx, agentConfig)
}

func (p *MessageProcessor) executeWorkflow(ctx context.Context, msgCtx *MessageContext, config *models.AgentConfig) error {
	// Start workflow timer
	workflowTimer := observability.StartWorkflowTimer(p.agentType, config.Workflow.StartStep)
	defer workflowTimer.Complete("success")

	// Update metrics
	observability.WorkflowsStarted.WithLabelValues(p.agentType, config.Workflow.StartStep, msgCtx.Headers["client_id"]).Inc()
	observability.ActiveWorkflows.WithLabelValues(p.agentType).Inc()
	defer observability.ActiveWorkflows.WithLabelValues(p.agentType).Dec()

	// Execute through orchestrator
	return p.orchestrator.ExecuteWorkflow(ctx, config.Workflow, msgCtx.Headers, msgCtx.Message.Value)
}

func (p *MessageProcessor) handleError(ctx context.Context, msgCtx *MessageContext, err error, errorType string) error {
	msgCtx.Logger.Error("Processing failed", zap.Error(err))
	observability.AgentTasksProcessed.WithLabelValues(p.agentType, msgCtx.Action, errorType).Inc()

	// Check for specific error types
	if domainErr, ok := err.(*errors.DomainError); ok {
		if domainErr.Code == errors.ErrInsufficientFuel {
			observability.FuelExhausted.WithLabelValues(p.agentType, msgCtx.Action, msgCtx.Headers["client_id"]).Inc()
		}
		p.sendErrorResponse(ctx, msgCtx, domainErr)
	} else {
		p.sendErrorResponse(ctx, msgCtx, errors.InternalError("Processing failed", err))
	}

	return err
}

func (p *MessageProcessor) sendErrorResponse(ctx context.Context, msgCtx *MessageContext, domainErr *errors.DomainError) {
	responseHeaders := msgCtx.CreateResponseHeaders(p.agentType)
	domainErr.TraceID = msgCtx.Headers["correlation_id"]

	errorResponse := map[string]interface{}{
		"success": false,
		"error":   domainErr,
		"agent":   p.agentType,
	}

	responseBytes, _ := json.Marshal(errorResponse)
	errorTopic := fmt.Sprintf("system.errors.%s", p.agentType)

	if err := p.producer.Produce(ctx, errorTopic, responseHeaders,
		[]byte(msgCtx.Headers["correlation_id"]), responseBytes); err != nil {
		msgCtx.Logger.Error("Failed to send error response", zap.Error(err))
		observability.SystemErrors.WithLabelValues(p.agentType, "produce_error").Inc()
	} else {
		observability.KafkaMessagesProduced.WithLabelValues(errorTopic).Inc()
	}
}
-------------------------------------------------
filepath = ./platform/infrastructure/connections.go
// FILE: platform/infrastructure/connections.go
package infrastructure

import (
	"context"
	"fmt"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/database"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// Connections holds all infrastructure connections
type Connections struct {
	ClientsDB     *pgxpool.Pool
	TemplatesDB   *pgxpool.Pool
	KafkaConsumer *kafka.Consumer
	KafkaProducer kafka.Producer
}

// Manager handles infrastructure lifecycle
type Manager struct {
	logger      *zap.Logger
	connections *Connections
}

// NewManager creates a new infrastructure manager
func NewManager(logger *zap.Logger) *Manager {
	return &Manager{
		logger:      logger,
		connections: &Connections{},
	}
}

// Initialize sets up all infrastructure connections
func (m *Manager) Initialize(ctx context.Context, cfg *config.ServiceConfig, topic, consumerGroup string) error {
	// Initialize database
	clientsPool, err := database.NewPostgresConnection(ctx, cfg.Infrastructure.ClientsDatabase, m.logger)
	if err != nil {
		return fmt.Errorf("failed to connect to clients database: %w", err)
	}
	m.connections.ClientsDB = clientsPool

	// Initialize Templates DB if needed
	if cfg.Infrastructure.TemplatesDatabase.Host != "" {
		templatesPool, err := database.NewPostgresConnection(ctx, cfg.Infrastructure.TemplatesDatabase, m.logger)
		if err != nil {
			m.Close()
			return fmt.Errorf("failed to connect to templates database: %w", err)
		}
		m.connections.TemplatesDB = templatesPool
	}

	// Initialize Kafka
	consumer, err := kafka.NewConsumer(cfg.Infrastructure.KafkaBrokers, topic, consumerGroup, m.logger)
	if err != nil {
		m.Close()
		return fmt.Errorf("failed to create consumer: %w", err)
	}
	m.connections.KafkaConsumer = consumer

	producer, err := kafka.NewProducer(cfg.Infrastructure.KafkaBrokers, m.logger)
	if err != nil {
		m.Close()
		return fmt.Errorf("failed to create producer: %w", err)
	}
	m.connections.KafkaProducer = producer

	return nil
}

// GetConnections returns the infrastructure connections
func (m *Manager) GetConnections() *Connections {
	return m.connections
}

// Close gracefully closes all connections
func (m *Manager) Close() error {
	var errs []error

	if m.connections.KafkaConsumer != nil {
		if err := m.connections.KafkaConsumer.Close(); err != nil {
			errs = append(errs, fmt.Errorf("failed to close kafka consumer: %w", err))
		}
	}

	if m.connections.KafkaProducer != nil {
		if err := m.connections.KafkaProducer.Close(); err != nil {
			errs = append(errs, fmt.Errorf("failed to close kafka producer: %w", err))
		}
	}

	if m.connections.ClientsDB != nil {
		m.connections.ClientsDB.Close()
	}

	if m.connections.TemplatesDB != nil {
		m.connections.TemplatesDB.Close()
	}

	if len(errs) > 0 {
		return fmt.Errorf("infrastructure shutdown errors: %v", errs)
	}
	return nil
}
-------------------------------------------------
filepath = ./platform/observability/metrics.go
// FILE: platform/observability/metrics.go
package observability

import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"net/http"
	"time"
)

var (
	// Workflow metrics
	WorkflowsStarted = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_workflows_started_total",
		Help: "Total number of workflows started",
	}, []string{"agent_type", "workflow_type", "client_id"})

	WorkflowsCompleted = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_workflows_completed_total",
		Help: "Total number of workflows completed",
	}, []string{"agent_type", "workflow_type", "status", "client_id"})

	WorkflowDuration = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_workflow_duration_seconds",
		Help:    "Duration of workflow execution",
		Buckets: prometheus.ExponentialBuckets(0.1, 2, 10), // 0.1s to ~100s
	}, []string{"agent_type", "workflow_type", "status"})

	// Agent metrics
	AgentTasksReceived = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_agent_tasks_received_total",
		Help: "Total number of tasks received by agents",
	}, []string{"agent_type", "action"})

	AgentTasksProcessed = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_agent_tasks_processed_total",
		Help: "Total number of tasks processed by agents",
	}, []string{"agent_type", "action", "status"})

	AgentProcessingDuration = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_agent_processing_duration_seconds",
		Help:    "Duration of agent task processing",
		Buckets: prometheus.ExponentialBuckets(0.01, 2, 10), // 10ms to ~10s
	}, []string{"agent_type", "action"})

	// Fuel metrics
	FuelConsumed = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_fuel_consumed_total",
		Help: "Total fuel consumed by operations",
	}, []string{"agent_type", "action", "client_id"})

	FuelExhausted = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_fuel_exhausted_total",
		Help: "Total number of operations that failed due to insufficient fuel",
	}, []string{"agent_type", "action", "client_id"})

	// Kafka metrics
	KafkaMessagesProduced = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_kafka_messages_produced_total",
		Help: "Total number of messages produced to Kafka",
	}, []string{"topic"})

	KafkaMessagesConsumed = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_kafka_messages_consumed_total",
		Help: "Total number of messages consumed from Kafka",
	}, []string{"topic", "consumer_group"})

	KafkaConsumerLag = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "ai_persona_kafka_consumer_lag",
		Help: "Current consumer lag in messages",
	}, []string{"topic", "consumer_group", "partition"})

	// Database metrics
	DatabaseQueries = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_database_queries_total",
		Help: "Total number of database queries",
	}, []string{"database", "operation", "table"})

	DatabaseQueryDuration = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_database_query_duration_seconds",
		Help:    "Duration of database queries",
		Buckets: prometheus.ExponentialBuckets(0.001, 2, 10), // 1ms to ~1s
	}, []string{"database", "operation"})

	// AI Service metrics
	AIServiceRequests = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_ai_service_requests_total",
		Help: "Total number of AI service requests",
	}, []string{"provider", "model", "operation"})

	AIServiceTokensUsed = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_ai_service_tokens_used_total",
		Help: "Total number of tokens used by AI services",
	}, []string{"provider", "model", "type"}) // type: input/output

	AIServiceLatency = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_ai_service_latency_seconds",
		Help:    "Latency of AI service requests",
		Buckets: prometheus.ExponentialBuckets(0.1, 2, 10), // 100ms to ~100s
	}, []string{"provider", "model", "operation"})

	// Memory/Vector DB metrics
	VectorSearchQueries = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_vector_search_queries_total",
		Help: "Total number of vector similarity searches",
	}, []string{"agent_type"})

	VectorSearchLatency = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_vector_search_latency_seconds",
		Help:    "Latency of vector similarity searches",
		Buckets: prometheus.ExponentialBuckets(0.01, 2, 8), // 10ms to ~2.5s
	}, []string{"agent_type"})

	MemoriesStored = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_memories_stored_total",
		Help: "Total number of memories stored",
	}, []string{"agent_type", "memory_type"})

	// Circuit breaker metrics
	CircuitBreakerState = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "ai_persona_circuit_breaker_state",
		Help: "Current state of circuit breakers (0=closed, 1=open, 2=half-open)",
	}, []string{"service"})

	CircuitBreakerTrips = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_circuit_breaker_trips_total",
		Help: "Total number of circuit breaker trips",
	}, []string{"service"})

	// System health metrics
	ActiveWorkflows = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "ai_persona_active_workflows",
		Help: "Number of currently active workflows",
	}, []string{"agent_type"})

	AgentPoolSize = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "ai_persona_agent_pool_size",
		Help: "Current size of agent pools",
	}, []string{"agent_type"})

	SystemErrors = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_system_errors_total",
		Help: "Total number of system errors",
	}, []string{"service", "error_code"})
)

// MetricsServer provides an HTTP server for Prometheus metrics
type MetricsServer struct {
	port string
}

// NewMetricsServer creates a new metrics server
func NewMetricsServer(port string) *MetricsServer {
	return &MetricsServer{port: port}
}

// Start starts the metrics HTTP server
func (m *MetricsServer) Start() error {
	http.Handle("/metrics", promhttp.Handler())
	http.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		w.Write([]byte("OK"))
	})

	return http.ListenAndServe(":"+m.port, nil)
}

// WorkflowTimer helps track workflow execution time
type WorkflowTimer struct {
	agentType    string
	workflowType string
	startTime    time.Time
	timer        *prometheus.Timer
}

// StartWorkflowTimer starts timing a workflow execution
func StartWorkflowTimer(agentType, workflowType string) *WorkflowTimer {
	return &WorkflowTimer{
		agentType:    agentType,
		workflowType: workflowType,
		timer: prometheus.NewTimer(prometheus.ObserverFunc(func(v float64) {
			WorkflowDuration.WithLabelValues(agentType, workflowType, "unknown").Observe(v)
		})),
	}
}

// Complete marks the workflow as completed with the given status
func (wt *WorkflowTimer) Complete(status string) {
	// The ObserveDuration method records the observation on the histogram provided to NewTimer.
	// It's designed to be called once. By calling it here, we override the "unknown" status.
	duration := wt.timer.ObserveDuration()
	WorkflowDuration.WithLabelValues(wt.agentType, wt.workflowType, status).Observe(duration.Seconds())
}

// CircuitBreakerStateValue converts circuit breaker state to numeric value
func CircuitBreakerStateValue(state string) float64 {
	switch state {
	case "closed":
		return 0
	case "open":
		return 1
	case "half-open":
		return 2
	default:
		return -1
	}
}
-------------------------------------------------
filepath = ./platform/observability/tracing.go
// FILE: platform/observability/tracing.go
package observability

import (
	"context"
	"fmt"

	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
	"go.opentelemetry.io/otel/propagation"
	"go.opentelemetry.io/otel/sdk/resource"
	sdktrace "go.opentelemetry.io/otel/sdk/trace"
	semconv "go.opentelemetry.io/otel/semconv/v1.17.0"
	"go.opentelemetry.io/otel/trace"
	"go.uber.org/zap"
)

// TracingConfig holds configuration for tracing
type TracingConfig struct {
	ServiceName    string
	ServiceVersion string
	Environment    string
	Endpoint       string
}

// InitTracing initializes OpenTelemetry tracing
func InitTracing(ctx context.Context, cfg TracingConfig, logger *zap.Logger) (func(), error) {
	// Create OTLP exporter
	exporter, err := otlptrace.New(
		ctx,
		otlptracegrpc.NewClient(
			otlptracegrpc.WithEndpoint(cfg.Endpoint),
			otlptracegrpc.WithInsecure(),
		),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create OTLP exporter: %w", err)
	}

	// Create resource
	res, err := resource.New(ctx,
		resource.WithAttributes(
			semconv.ServiceNameKey.String(cfg.ServiceName),
			semconv.ServiceVersionKey.String(cfg.ServiceVersion),
			attribute.String("environment", cfg.Environment),
		),
		resource.WithHost(),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create resource: %w", err)
	}

	// Create tracer provider
	tracerProvider := sdktrace.NewTracerProvider(
		sdktrace.WithBatcher(exporter),
		sdktrace.WithResource(res),
		sdktrace.WithSampler(sdktrace.AlwaysSample()),
	)

	// Set global tracer provider
	otel.SetTracerProvider(tracerProvider)

	// Set global propagator
	otel.SetTextMapPropagator(
		propagation.NewCompositeTextMapPropagator(
			propagation.TraceContext{},
			propagation.Baggage{},
		),
	)

	logger.Info("Tracing initialized",
		zap.String("service", cfg.ServiceName),
		zap.String("endpoint", cfg.Endpoint),
	)

	// Return cleanup function
	cleanup := func() {
		if err := tracerProvider.Shutdown(ctx); err != nil {
			logger.Error("Failed to shutdown tracer provider", zap.Error(err))
		}
	}

	return cleanup, nil
}

// StartSpan creates a new span with standard attributes
func StartSpan(ctx context.Context, name string, opts ...trace.SpanStartOption) (context.Context, trace.Span) {
	tracer := otel.Tracer("ai-persona-system")
	return tracer.Start(ctx, name, opts...)
}

// AddSpanAttributes adds attributes to the current span
func AddSpanAttributes(ctx context.Context, attrs ...attribute.KeyValue) {
	span := trace.SpanFromContext(ctx)
	span.SetAttributes(attrs...)
}

// RecordError records an error on the current span
func RecordError(ctx context.Context, err error) {
	span := trace.SpanFromContext(ctx)
	span.RecordError(err)
}
-------------------------------------------------
filepath = ./platform/health/server.go
// FILE: platform/health/server.go
package health

import (
	"context"
	"encoding/json"
	"net/http"

	"github.com/prometheus/client_golang/prometheus/promhttp"
	"go.uber.org/zap"
)

// CheckFunc is a health check function
type CheckFunc func(ctx context.Context) error

// Checkers is a map of named health checks
type Checkers map[string]CheckFunc

// Config for health server
type Config struct {
	HealthPort  string
	MetricsPort string
}

// Server handles health and metrics endpoints
type Server struct {
	serviceName string
	config      Config
	checkers    Checkers
	logger      *zap.Logger
}

// NewServer creates a new health server
func NewServer(serviceName string, config Config, checkers Checkers, logger *zap.Logger) *Server {
	return &Server{
		serviceName: serviceName,
		config:      config,
		checkers:    checkers,
		logger:      logger,
	}
}

// Start starts the health and metrics servers
func (s *Server) Start() {
	go s.startMetricsServer()
	go s.startHealthServer()
}

func (s *Server) startMetricsServer() {
	mux := http.NewServeMux()
	mux.Handle("/metrics", promhttp.Handler())

	s.logger.Info("Starting metrics server", zap.String("port", s.config.MetricsPort))
	if err := http.ListenAndServe(":"+s.config.MetricsPort, mux); err != nil {
		s.logger.Error("Metrics server failed", zap.Error(err))
	}
}

func (s *Server) startHealthServer() {
	mux := http.NewServeMux()
	mux.HandleFunc("/health", s.handleHealth)
	mux.HandleFunc("/ready", s.handleReady)

	s.logger.Info("Starting health server", zap.String("port", s.config.HealthPort))
	if err := http.ListenAndServe(":"+s.config.HealthPort, mux); err != nil {
		s.logger.Error("Health server failed", zap.Error(err))
	}
}

func (s *Server) handleHealth(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	checks := make(map[string]interface{})
	healthy := true

	for name, checker := range s.checkers {
		if err := checker(ctx); err != nil {
			checks[name] = map[string]interface{}{
				"status": "unhealthy",
				"error":  err.Error(),
			}
			healthy = false
		} else {
			checks[name] = map[string]interface{}{
				"status": "healthy",
			}
		}
	}

	response := map[string]interface{}{
		"service": s.serviceName,
		"status":  "healthy",
		"checks":  checks,
	}

	if !healthy {
		response["status"] = "unhealthy"
		w.WriteHeader(http.StatusServiceUnavailable)
	} else {
		w.WriteHeader(http.StatusOK)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

func (s *Server) handleReady(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()

	for _, checker := range s.checkers {
		if err := checker(ctx); err != nil {
			w.WriteHeader(http.StatusServiceUnavailable)
			w.Write([]byte("NOT READY"))
			return
		}
	}

	w.WriteHeader(http.StatusOK)
	w.Write([]byte("READY"))
}
-------------------------------------------------
filepath = ./platform/validation/workflow.go
// FILE: platform/validation/workflow.go
package validation

import (
	"fmt"
	"github.com/gqls/agentchassis/pkg/models"
)

// WorkflowValidator provides validation for workflow plans
type WorkflowValidator struct{}

// NewWorkflowValidator creates a new workflow validator
func NewWorkflowValidator() *WorkflowValidator {
	return &WorkflowValidator{}
}

// ValidateWorkflowPlan validates a workflow plan for correctness
func (v *WorkflowValidator) ValidateWorkflowPlan(plan models.WorkflowPlan) error {
	if plan.StartStep == "" {
		return fmt.Errorf("workflow plan must have a start step")
	}

	if len(plan.Steps) == 0 {
		return fmt.Errorf("workflow plan must have at least one step")
	}

	// Check start step exists
	if _, ok := plan.Steps[plan.StartStep]; !ok {
		return fmt.Errorf("start step '%s' not found in steps", plan.StartStep)
	}

	// Validate each step
	for stepName, step := range plan.Steps {
		if err := v.validateStep(stepName, step, plan); err != nil {
			return err
		}
	}

	// Check for cycles
	if err := v.checkForCycles(plan); err != nil {
		return err
	}

	// Check all dependencies exist
	if err := v.validateDependencies(plan); err != nil {
		return err
	}

	return nil
}

// validateStep validates an individual step
func (v *WorkflowValidator) validateStep(name string, step models.Step, plan models.WorkflowPlan) error {
	if step.Action == "" {
		return fmt.Errorf("step '%s' must have an action", name)
	}

	// Validate based on action type
	switch step.Action {
	case "fan_out":
		if len(step.SubTasks) == 0 {
			return fmt.Errorf("fan_out step '%s' must have at least one sub-task", name)
		}
		for i, subTask := range step.SubTasks {
			if subTask.StepName == "" {
				return fmt.Errorf("sub-task %d in step '%s' must have a step name", i, name)
			}
			if subTask.Topic == "" {
				return fmt.Errorf("sub-task %d in step '%s' must have a topic", i, name)
			}
		}
	case "complete_workflow":
		if step.NextStep != "" {
			return fmt.Errorf("complete_workflow step '%s' should not have a next step", name)
		}
	default:
		// For standard actions, ensure topic is set if not internal actions
		if step.Topic == "" && !v.isInternalAction(step.Action) {
			return fmt.Errorf("step '%s' with action '%s' requires a topic", name, step.Action)
		}
	}

	// Validate next step exists
	if step.NextStep != "" {
		if _, ok := plan.Steps[step.NextStep]; !ok {
			return fmt.Errorf("step '%s' references non-existent next step '%s'", name, step.NextStep)
		}
	}

	return nil
}

// isInternalAction checks if an action is handled internally
func (v *WorkflowValidator) isInternalAction(action string) bool {
	internalActions := map[string]bool{
		"complete_workflow":     true,
		"pause_for_human_input": true,
		"store_memory":          true,
		"retrieve_memory":       true,
	}
	return internalActions[action]
}

// validateDependencies ensures all dependencies exist
func (v *WorkflowValidator) validateDependencies(plan models.WorkflowPlan) error {
	for stepName, step := range plan.Steps {
		for _, dep := range step.Dependencies {
			if _, ok := plan.Steps[dep]; !ok {
				return fmt.Errorf("step '%s' has dependency on non-existent step '%s'", stepName, dep)
			}
		}
	}
	return nil
}

// checkForCycles detects cycles in the workflow
func (v *WorkflowValidator) checkForCycles(plan models.WorkflowPlan) error {
	visited := make(map[string]bool)
	recStack := make(map[string]bool)

	var hasCycle func(string) bool
	hasCycle = func(stepName string) bool {
		visited[stepName] = true
		recStack[stepName] = true

		step, ok := plan.Steps[stepName]
		if !ok {
			return false
		}

		// Check next step
		if step.NextStep != "" {
			if !visited[step.NextStep] {
				if hasCycle(step.NextStep) {
					return true
				}
			} else if recStack[step.NextStep] {
				return true
			}
		}

		// Check dependencies
		for _, dep := range step.Dependencies {
			if !visited[dep] {
				if hasCycle(dep) {
					return true
				}
			} else if recStack[dep] {
				return true
			}
		}

		recStack[stepName] = false
		return false
	}

	// Start from the start step
	if hasCycle(plan.StartStep) {
		return fmt.Errorf("workflow contains a cycle")
	}

	// Check any unvisited steps (disconnected components)
	for stepName := range plan.Steps {
		if !visited[stepName] {
			if hasCycle(stepName) {
				return fmt.Errorf("workflow contains a cycle")
			}
		}
	}

	return nil
}

// GetWorkflowMetrics calculates metrics about a workflow
func (v *WorkflowValidator) GetWorkflowMetrics(plan models.WorkflowPlan) map[string]interface{} {
	metrics := map[string]interface{}{
		"total_steps":    len(plan.Steps),
		"fan_out_steps":  0,
		"external_calls": 0,
		"max_depth":      0,
	}

	for _, step := range plan.Steps {
		if step.Action == "fan_out" {
			metrics["fan_out_steps"] = metrics["fan_out_steps"].(int) + 1
		}
		if step.Topic != "" {
			metrics["external_calls"] = metrics["external_calls"].(int) + 1
		}
	}

	// Calculate max depth
	metrics["max_depth"] = v.calculateMaxDepth(plan)

	return metrics
}

// calculateMaxDepth calculates the maximum depth of the workflow
func (v *WorkflowValidator) calculateMaxDepth(plan models.WorkflowPlan) int {
	depths := make(map[string]int)

	var calculateDepth func(string) int
	calculateDepth = func(stepName string) int {
		if depth, ok := depths[stepName]; ok {
			return depth
		}

		step, ok := plan.Steps[stepName]
		if !ok {
			return 0
		}

		maxDepth := 0

		// Check dependencies
		for _, dep := range step.Dependencies {
			depDepth := calculateDepth(dep)
			if depDepth > maxDepth {
				maxDepth = depDepth
			}
		}

		// Check next step
		if step.NextStep != "" {
			nextDepth := calculateDepth(step.NextStep)
			if nextDepth > maxDepth {
				maxDepth = nextDepth
			}
		}

		depths[stepName] = maxDepth + 1
		return maxDepth + 1
	}

	return calculateDepth(plan.StartStep)
}
-------------------------------------------------
filepath = ./platform/config/agent_config_loader.go
// FILE: platform/config/agent_config_loader.go
package config

import (
	"context"
	"encoding/json"
	"fmt"

	"github.com/gqls/agentchassis/pkg/models"
	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// AgentConfigLoader handles loading agent configurations from the database
type AgentConfigLoader struct {
	logger *zap.Logger
}

// NewAgentConfigLoader creates a new agent config loader
func NewAgentConfigLoader(logger *zap.Logger) *AgentConfigLoader {
	return &AgentConfigLoader{logger: logger}
}

// LoadFromDatabase fetches agent configuration from a client-specific schema
func (l *AgentConfigLoader) LoadFromDatabase(ctx context.Context, db *pgxpool.Pool, clientID, agentInstanceID, agentType string) (*models.AgentConfig, error) {
	query := fmt.Sprintf(`
		SELECT name, config, template_id 
		FROM client_%s.agent_instances 
		WHERE id = $1 AND is_active = true
	`, clientID)

	var name string
	var configJSON []byte
	var templateID string

	err := db.QueryRow(ctx, query, agentInstanceID).Scan(&name, &configJSON, &templateID)
	if err != nil {
		if err == pgx.ErrNoRows {
			l.logger.Warn("Agent instance not found, using default configuration",
				zap.String("agent_instance_id", agentInstanceID))
			return l.GetDefaultConfig(agentInstanceID, agentType), nil
		}
		return nil, fmt.Errorf("failed to query agent instance: %w", err)
	}

	// Parse and build config
	return l.parseConfig(configJSON, agentInstanceID, agentType)
}

// LoadFromJSON loads agent configuration from JSON data
func (l *AgentConfigLoader) LoadFromJSON(data []byte, agentInstanceID, agentType string) (*models.AgentConfig, error) {
	return l.parseConfig(data, agentInstanceID, agentType)
}

// GetDefaultConfig returns a default configuration for an agent type
func (l *AgentConfigLoader) GetDefaultConfig(agentInstanceID, agentType string) *models.AgentConfig {
	return &models.AgentConfig{
		AgentID:   agentInstanceID,
		AgentType: agentType,
		Version:   1,
		CoreLogic: l.getDefaultCoreLogic(agentType),
		Workflow:  l.getDefaultWorkflow(agentType),
	}
}

func (l *AgentConfigLoader) parseConfig(configJSON []byte, agentInstanceID, agentType string) (*models.AgentConfig, error) {
	var config map[string]interface{}
	if err := json.Unmarshal(configJSON, &config); err != nil {
		return nil, fmt.Errorf("failed to parse agent config: %w", err)
	}

	// Extract workflow
	var workflow models.WorkflowPlan
	if workflowData, ok := config["workflow"]; ok {
		workflowBytes, _ := json.Marshal(workflowData)
		if err := json.Unmarshal(workflowBytes, &workflow); err != nil {
			l.logger.Warn("Failed to parse workflow, using default", zap.Error(err))
			workflow = l.getDefaultWorkflow(agentType)
		}
	} else {
		workflow = l.getDefaultWorkflow(agentType)
	}

	// Extract memory configuration
	var memoryConfig models.MemoryConfiguration
	if memData, ok := config["memory_config"]; ok {
		memBytes, _ := json.Marshal(memData)
		json.Unmarshal(memBytes, &memoryConfig)
	}

	return &models.AgentConfig{
		AgentID:      agentInstanceID,
		AgentType:    agentType,
		Version:      1,
		CoreLogic:    config,
		Workflow:     workflow,
		MemoryConfig: memoryConfig,
	}, nil
}

func (l *AgentConfigLoader) getDefaultCoreLogic(agentType string) map[string]interface{} {
	// Different defaults for different agent types
	switch agentType {
	case "copywriter":
		return map[string]interface{}{
			"model":       "claude-3-sonnet",
			"temperature": 0.7,
			"max_tokens":  2000,
		}
	case "researcher":
		return map[string]interface{}{
			"model":       "claude-3-opus",
			"temperature": 0.3,
			"max_tokens":  4000,
		}
	default:
		return map[string]interface{}{
			"model":       "claude-3-haiku",
			"temperature": 0.5,
			"max_tokens":  1000,
		}
	}
}

func (l *AgentConfigLoader) getDefaultWorkflow(agentType string) models.WorkflowPlan {
	// Could have type-specific default workflows
	return models.WorkflowPlan{
		StartStep: "process",
		Steps: map[string]models.Step{
			"process": {
				Action:      "ai_text_generate",
				Description: "Process the request",
				NextStep:    "complete",
			},
			"complete": {
				Action:      "complete_workflow",
				Description: "Mark workflow as complete",
			},
		},
	}
}
-------------------------------------------------
filepath = ./platform/config/loader.go
// FILE: platform/config/loader.go
package config

import (
	"fmt"
	"log"
	"strings"

	"github.com/spf13/viper"
)

// ServiceConfig is the top-level configuration struct for any service
type ServiceConfig struct {
	ServiceInfo    ServiceInfoConfig      `mapstructure:"service_info"`
	Server         ServerConfig           `mapstructure:"server"`
	Logging        LoggingConfig          `mapstructure:"logging"`
	Observability  ObservabilityConfig    `mapstructure:"observability"`
	Infrastructure InfrastructureConfig   `mapstructure:"infrastructure"`
	Custom         map[string]interface{} `mapstructure:"custom"`
}

type ServiceInfoConfig struct {
	Name        string `mapstructure:"name"`
	Version     string `mapstructure:"version"`
	Environment string `mapstructure:"environment"`
}

type ServerConfig struct {
	Port string `mapstructure:"port"`
}

type LoggingConfig struct {
	Level string `mapstructure:"level"`
}

type ObservabilityConfig struct {
	TracingEndpoint string `mapstructure:"tracing_endpoint"`
}

type InfrastructureConfig struct {
	KafkaBrokers      []string            `mapstructure:"kafka_brokers"`
	ClientsDatabase   DatabaseConfig      `mapstructure:"clients_database"`
	TemplatesDatabase DatabaseConfig      `mapstructure:"templates_database"`
	AuthDatabase      DatabaseConfig      `mapstructure:"auth_database"`
	ObjectStorage     ObjectStorageConfig `mapstructure:"object_storage"`
}

type DatabaseConfig struct {
	Host           string `mapstructure:"host"`
	Port           int    `mapstructure:"port"`
	User           string `mapstructure:"user"`
	PasswordEnvVar string `mapstructure:"password_env_var"`
	DBName         string `mapstructure:"db_name"`
	SSLMode        string `mapstructure:"sslmode"`
}

type ObjectStorageConfig struct {
	Provider        string `mapstructure:"provider"`
	Endpoint        string `mapstructure:"endpoint"`
	Bucket          string `mapstructure:"bucket"`
	AccessKeyEnvVar string `mapstructure:"access_key_env_var"`
	SecretKeyEnvVar string `mapstructure:"secret_key_env_var"`
}

// Load reads a YAML config file and overrides with environment variables
func Load(path string) (*ServiceConfig, error) {
	v := viper.New()
	v.SetDefault("server.port", "8080")
	v.SetDefault("logging.level", "info")
	v.SetDefault("infrastructure.database.sslmode", "disable")

	v.SetConfigFile(path)
	v.SetConfigType("yaml")
	if err := v.ReadInConfig(); err != nil {
		if _, ok := err.(viper.ConfigFileNotFoundError); ok {
			log.Printf("config: file not found at %s, relying on defaults and environment variables", path)
		} else {
			return nil, fmt.Errorf("config: error reading config file %s: %w", path, err)
		}
	}

	v.SetEnvPrefix("SERVICE")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
	v.AutomaticEnv()

	var cfg ServiceConfig
	if err := v.Unmarshal(&cfg); err != nil {
		return nil, fmt.Errorf("config: unable to unmarshal config: %w", err)
	}

	return &cfg, nil
}
-------------------------------------------------
filepath = ./platform/agentbase/agent_test.go
// FILE: platform/agentbase/agent_test.go
package agentbase

import (
	"context"
	"testing"

	"github.com/segmentio/kafka-go"
	"github.com/stretchr/testify/mock"
)

// MockKafkaConsumer for testing
type MockKafkaConsumer struct {
	mock.Mock
}

func (m *MockKafkaConsumer) FetchMessage(ctx context.Context) (kafka.Message, error) {
	args := m.Called(ctx)
	return args.Get(0).(kafka.Message), args.Error(1)
}

func (m *MockKafkaConsumer) CommitMessages(ctx context.Context, msgs ...kafka.Message) error {
	args := m.Called(ctx, msgs)
	return args.Error(0)
}

func (m *MockKafkaConsumer) Close() error {
	return nil
}

func TestAgentHandleMessage(t *testing.T) {
	// This test needs to be redesigned since handleMessage is private
	// and the Agent struct expects real Kafka connections
	// For now, we'll skip this test or make it integration-only
	t.Skip("Skipping unit test that requires real Kafka connections")
}
-------------------------------------------------
filepath = ./platform/agentbase/agent.go
// FILE: platform/agentbase/agent.go (refactored)
package agentbase

import (
	"context"
	"fmt"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/health"
	"github.com/gqls/agentchassis/platform/infrastructure"
	"github.com/gqls/agentchassis/platform/messaging"
	"github.com/gqls/agentchassis/platform/observability"
	"github.com/gqls/agentchassis/platform/orchestration"
	"github.com/gqls/agentchassis/platform/validation"
	"github.com/jackc/pgx/v5/stdlib"
	"go.uber.org/zap"
)

// Agent represents a generic agent chassis
type Agent struct {
	ctx           context.Context
	cfg           *config.ServiceConfig
	logger        *zap.Logger
	agentType     string
	consumerGroup string

	// Managers
	infraManager  *infrastructure.Manager
	messageRunner *MessageRunner
	healthServer  *health.Server
}

// New creates a new agent with defaults from config
func New(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger) (*Agent, error) {
	agentType := "generic"
	topic := "system.agent.generic.process"

	if cfg.Custom != nil {
		if at, ok := cfg.Custom["agent_type"].(string); ok {
			agentType = at
		}
		if t, ok := cfg.Custom["topic"].(string); ok {
			topic = t
		}
	}

	return NewWithType(ctx, cfg, logger, agentType, topic)
}

// NewWithType creates an agent with specific type and topic
func NewWithType(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger, agentType string, topic string) (*Agent, error) {
	// Consumer group
	consumerGroup := fmt.Sprintf("%s-group", agentType)
	if cfg.Custom != nil {
		if cg, ok := cfg.Custom["kafka_consumer_group"].(string); ok {
			consumerGroup = cg
		}
	}

	// Initialize infrastructure
	infraManager := infrastructure.NewManager(logger)
	if err := infraManager.Initialize(ctx, cfg, topic, consumerGroup); err != nil {
		return nil, fmt.Errorf("failed to initialize infrastructure: %w", err)
	}

	connections := infraManager.GetConnections()

	// Create components
	components, err := createComponents(connections, agentType, logger)
	if err != nil {
		infraManager.Close()
		return nil, fmt.Errorf("failed to create components: %w", err)
	}

	// Create message runner
	messageRunner := NewMessageRunner(
		ctx,
		logger,
		connections.KafkaConsumer,
		components.messageProcessor,
		consumerGroup,
		agentType,
	)

	// Create health server
	healthServer := createHealthServer(cfg, connections, agentType, logger)

	// Record metrics
	observability.AgentPoolSize.WithLabelValues(agentType).Inc()

	return &Agent{
		ctx:           ctx,
		cfg:           cfg,
		logger:        logger,
		agentType:     agentType,
		consumerGroup: consumerGroup,
		infraManager:  infraManager,
		messageRunner: messageRunner,
		healthServer:  healthServer,
	}, nil
}

// Components holds the processing components
type Components struct {
	messageProcessor *messaging.MessageProcessor
	orchestrator     *orchestration.SagaCoordinator
	validator        *validation.WorkflowValidator
}

func createComponents(connections *infrastructure.Connections, agentType string, logger *zap.Logger) (*Components, error) {
	// Create orchestrator
	connConfig := connections.ClientsDB.Config().ConnConfig.Copy()
	stdDB := stdlib.OpenDB(*connConfig)
	orchestrator := orchestration.NewSagaCoordinator(stdDB, connections.KafkaProducer, logger)

	// Create validator
	validator := validation.NewWorkflowValidator()

	// Create message processor
	messageProcessor := messaging.NewMessageProcessor(
		agentType,
		connections.ClientsDB,
		connections.KafkaProducer,
		orchestrator,
		validator,
		logger,
	)

	return &Components{
		messageProcessor: messageProcessor,
		orchestrator:     orchestrator,
		validator:        validator,
	}, nil
}

func createHealthServer(cfg *config.ServiceConfig, connections *infrastructure.Connections, agentType string, logger *zap.Logger) *health.Server {
	return health.NewServer(
		agentType,
		health.Config{
			HealthPort:  "8080",
			MetricsPort: "9090",
		},
		health.Checkers{
			"database": func(ctx context.Context) error {
				return connections.ClientsDB.Ping(ctx)
			},
			"kafka": func(ctx context.Context) error {
				// Simplified check - could be enhanced
				return nil
			},
		},
		logger,
	)
}

// Run starts the agent
func (a *Agent) Run() error {
	a.logger.Info("Agent starting", zap.String("type", a.agentType))

	// Start health server
	a.healthServer.Start()

	// Run message processing
	return a.messageRunner.Run()
}

// Shutdown gracefully shuts down the agent
func (a *Agent) Shutdown() error {
	a.logger.Info("Agent shutting down")
	observability.AgentPoolSize.WithLabelValues(a.agentType).Dec()
	return a.infraManager.Close()
}
-------------------------------------------------
filepath = ./platform/agentbase/runner.go
// FILE: platform/agentbase/runner.go
package agentbase

import (
	"context"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/gqls/agentchassis/platform/messaging"
	"github.com/gqls/agentchassis/platform/observability"
	"go.uber.org/zap"
	"time"
)

// MessageRunner handles the message processing loop
type MessageRunner struct {
	ctx           context.Context
	logger        *zap.Logger
	consumer      *kafka.Consumer
	processor     *messaging.MessageProcessor
	consumerGroup string
	agentType     string
}

// NewMessageRunner creates a new message runner
func NewMessageRunner(
	ctx context.Context,
	logger *zap.Logger,
	consumer *kafka.Consumer,
	processor *messaging.MessageProcessor,
	consumerGroup string,
	agentType string,
) *MessageRunner {
	return &MessageRunner{
		ctx:           ctx,
		logger:        logger,
		consumer:      consumer,
		processor:     processor,
		consumerGroup: consumerGroup,
		agentType:     agentType,
	}
}

// Run starts the message processing loop
func (r *MessageRunner) Run() error {
	r.logger.Info("Starting message runner", zap.String("agent_type", r.agentType))

	for {
		select {
		case <-r.ctx.Done():
			r.logger.Info("Message runner shutting down")
			return nil
		default:
			msg, err := r.consumer.FetchMessage(r.ctx)
			if err != nil {
				if err == context.Canceled {
					continue
				}
				r.logger.Error("Failed to fetch message", zap.Error(err))
				observability.SystemErrors.WithLabelValues(r.agentType, "fetch_message").Inc()
				time.Sleep(1 * time.Second)
				continue
			}

			// Record metric
			observability.KafkaMessagesConsumed.WithLabelValues(msg.Topic, r.consumerGroup).Inc()

			// Process asynchronously
			go r.processMessage(msg)
		}
	}
}

func (r *MessageRunner) processMessage(msg kafka.Message) {
	if err := r.processor.ProcessMessage(r.ctx, msg); err != nil {
		r.logger.Error("Failed to process message", zap.Error(err))
	}

	// Always commit
	if err := r.consumer.CommitMessages(context.Background(), msg); err != nil {
		r.logger.Error("Failed to commit message", zap.Error(err))
		observability.SystemErrors.WithLabelValues(r.agentType, "commit_message").Inc()
	}
}
-------------------------------------------------
filepath = ./platform/contracts/contracts.go
// FILE: platform/contracts/contracts.go
// This package defines the core data structures used for agent configuration
// and communication throughout the entire system. It is the "shared language"
// that all services and agents will use.
package contracts

// AgentConfig is the master configuration for a single agent instance.
// This is stored as a JSONB object in the `agent_instances` table and
// loaded by the Agent Chassis at runtime to determine its behavior.
type AgentConfig struct {
	AgentID   string          `json:"agent_id"`
	AgentType string          `json:"agent_type"` // e.g., "copywriter", "orchestrator", "reasoning-agent"
	Version   int             `json:"version"`
	CoreLogic CoreLogicConfig `json:"core_logic"` // The "Who" - parameters for its main skill
	Workflow  WorkflowPlan    `json:"workflow"`   // The "How" - the plan it follows to do its job
}

// CoreLogicConfig is a generic map to hold the specific parameters
// for an agent's primary function.
// For an AI agent, this would contain prompts and model info.
// For an adapter, it might contain API endpoints and credentials.
type CoreLogicConfig map[string]interface{}

// WorkflowPlan defines the orchestration logic for an agent.
// It is a declarative, directed graph of steps.
type WorkflowPlan struct {
	StartStep string          `json:"start_step"`
	Steps     map[string]Step `json:"steps"`
}

// Step represents a single node in the workflow graph. It can be either
// a direct action for the agent to perform, or a call to another agent.
type Step struct {
	// Action is the specific function this agent should perform for this step.
	// e.g., "generate_text", "fan_out", "pause_for_human_input".
	Action string `json:"action"`

	// Description provides a human-readable explanation of the step's purpose.
	Description string `json:"description"`

	// Topic is the Kafka topic to send a message to if this step involves
	// calling another agent or service.
	Topic string `json:"topic,omitempty"`

	// Dependencies lists the `step_name`s that must be completed before
	// this step can begin. The orchestrator will not execute this step
	// until it has received responses from all dependencies.
	Dependencies []string `json:"dependencies,omitempty"`

	// NextStep defines the name of the next step to execute upon successful
	// completion of this one, for simple linear workflows.
	NextStep string `json:"next_step,omitempty"`

	// SubTasks is used for "fan_out" actions, defining a list of parallel
	// tasks to be executed.
	SubTasks []SubTask `json:"sub_tasks,omitempty"`
}

// SubTask defines a single task to be executed in parallel within a "fan_out" step.
type SubTask struct {
	// StepName is the logical name for this sub-task, used for dependency tracking.
	StepName string `json:"step_name"`
	// Topic is the Kafka topic to which the request for this sub-task will be sent.
	Topic string `json:"topic"`
}
-------------------------------------------------
filepath = ./platform/orchestration/coordinator_test.go
// FILE: platform/orchestration/coordinator_test.go
package orchestration

import (
	"context"
	"database/sql"
	"encoding/json"
	"testing"
	"time"

	"github.com/DATA-DOG/go-sqlmock"
	"github.com/google/uuid"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/governance"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
	"github.com/stretchr/testify/require"
	"go.uber.org/zap"
)

// MockKafkaProducer allows us to test the coordinator without a real Kafka connection.
type MockKafkaProducer struct {
	mock.Mock
}

func (m *MockKafkaProducer) Produce(ctx context.Context, topic string, headers map[string]string, key, value []byte) error {
	args := m.Called(ctx, topic, headers, key, value)
	return args.Error(0)
}

func (m *MockKafkaProducer) Close() error {
	args := m.Called()
	return args.Error(0)
}

// setupTest creates the coordinator with mocked dependencies for testing.
func setupTest(t *testing.T) (*SagaCoordinator, *MockKafkaProducer, *sql.DB, sqlmock.Sqlmock) {
	db, mockDB, err := sqlmock.New()
	require.NoError(t, err)

	mockProducer := new(MockKafkaProducer)
	logger := zap.NewNop()

	coordinator := NewSagaCoordinator(db, mockProducer, logger)
	require.NotNil(t, coordinator)

	return coordinator, mockProducer, db, mockDB
}

// TestExecuteWorkflow_InitialStep verifies the start of a new workflow.
func TestExecuteWorkflow_InitialStep(t *testing.T) {
	coordinator, mockProducer, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id":      correlationID,
		"request_id":          uuid.NewString(),
		"client_id":           clientID,
		governance.FuelHeader: "1000",
	}
	initialData, _ := json.Marshal(map[string]string{"goal": "test"})

	plan := models.WorkflowPlan{
		StartStep: "step1",
		Steps: map[string]models.Step{
			"step1":  {Action: "do_something", Topic: "topic.do_something", NextStep: "finish"},
			"finish": {Action: "complete_workflow"},
		},
	}

	// First, expect check if state exists (it won't)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnError(sql.ErrNoRows)

	// Then expect creation of initial state
	mockDB.ExpectExec("INSERT INTO orchestrator_state").
		WithArgs(
			correlationID,    // correlation_id
			clientID,         // client_id
			StatusRunning,    // status
			"step1",          // current_step
			sqlmock.AnyArg(), // awaited_steps
			sqlmock.AnyArg(), // collected_data
			initialData,      // initial_request_data
			sqlmock.AnyArg(), // created_at
			sqlmock.AnyArg(), // updated_at
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Then expect fetch of the newly created state
	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, clientID, StatusRunning, "step1", "[]",
		"{}", initialData, nil, nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Expect Kafka message production - verify headers are properly set
	mockProducer.On("Produce", ctx, "topic.do_something", mock.MatchedBy(func(h map[string]string) bool {
		// Verify required headers are present and correct
		return h["correlation_id"] == correlationID &&
			h["causation_id"] == headers["request_id"] &&
			h["request_id"] != "" && h["request_id"] != headers["request_id"] &&
			h[governance.FuelHeader] == "999" // 1000 - 1 for default_step cost
	}), []byte(correlationID), mock.AnythingOfType("[]uint8")).Return(nil).Once()

	// Expect state update
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,           // WHERE correlation_id = $1
			StatusAwaitingResponses, // status = $2
			"finish",                // current_step = $3
			sqlmock.AnyArg(),        // awaited_steps = $4
			sqlmock.AnyArg(),        // collected_data = $5
			sqlmock.AnyArg(),        // final_result = $6
			"",                      // error = $7
			sqlmock.AnyArg(),        // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	err := coordinator.ExecuteWorkflow(ctx, plan, headers, initialData)
	require.NoError(t, err)

	mockProducer.AssertExpectations(t)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestExecuteWorkflow_AlreadyCompleted verifies handling of already completed workflows.
func TestExecuteWorkflow_AlreadyCompleted(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id": correlationID,
		"client_id":      clientID,
	}

	plan := models.WorkflowPlan{
		StartStep: "step1",
		Steps: map[string]models.Step{
			"step1": {Action: "do_something"},
		},
	}

	// State already exists and is completed
	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, clientID, StatusCompleted, "step1", "[]",
		"{}", nil, []byte(`{"result": "done"}`), nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Should not do anything else
	err := coordinator.ExecuteWorkflow(ctx, plan, headers, nil)
	require.NoError(t, err)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestExecuteWorkflow_MissingClientID verifies error when client_id is missing.
func TestExecuteWorkflow_MissingClientID(t *testing.T) {
	coordinator, _, db, _ := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	headers := map[string]string{
		"correlation_id": uuid.NewString(),
		// client_id is missing
	}

	plan := models.WorkflowPlan{
		StartStep: "step1",
		Steps:     map[string]models.Step{},
	}

	err := coordinator.ExecuteWorkflow(ctx, plan, headers, nil)
	require.Error(t, err)
	assert.Contains(t, err.Error(), "client_id header is required")
}

// TestExecuteWorkflow_DependenciesNotMet verifies the workflow waits correctly.
func TestExecuteWorkflow_DependenciesNotMet(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id":      correlationID,
		"client_id":           clientID,
		governance.FuelHeader: "1000",
	}

	plan := models.WorkflowPlan{
		StartStep: "step2",
		Steps: map[string]models.Step{
			"step1": {Action: "do_something"},
			"step2": {Action: "do_something_else", Dependencies: []string{"step1"}},
		},
	}

	// First check - state doesn't exist
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnError(sql.ErrNoRows)

	// Create initial state
	mockDB.ExpectExec("INSERT INTO orchestrator_state").
		WithArgs(
			correlationID,    // correlation_id
			clientID,         // client_id
			StatusRunning,    // status
			"step2",          // current_step
			sqlmock.AnyArg(), // awaited_steps
			sqlmock.AnyArg(), // collected_data
			sqlmock.AnyArg(), // initial_request_data (nil)
			sqlmock.AnyArg(), // created_at
			sqlmock.AnyArg(), // updated_at
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Fetch state - missing step1 dependency
	stateJSON := `{}` // No step1 data
	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, clientID, StatusRunning, "step2", "[]",
		stateJSON, nil, nil, nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Should not produce any messages or update state
	err := coordinator.ExecuteWorkflow(ctx, plan, headers, nil)
	require.NoError(t, err, "Waiting for dependencies should not be an error")

	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestExecuteWorkflow_FuelCheckFail verifies that a workflow stops if out of fuel.
func TestExecuteWorkflow_FuelCheckFail(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id":      correlationID,
		"client_id":           clientID,
		governance.FuelHeader: "5", // Low fuel (need 50 for claude opus)
	}

	plan := models.WorkflowPlan{
		StartStep: "step1",
		Steps: map[string]models.Step{
			"step1": {Action: "ai_text_generate_claude_opus", Topic: "topic.expensive"},
		},
	}

	// First check - state doesn't exist
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnError(sql.ErrNoRows)

	// Create initial state
	mockDB.ExpectExec("INSERT INTO orchestrator_state").
		WithArgs(
			correlationID,    // correlation_id
			clientID,         // client_id
			StatusRunning,    // status
			"step1",          // current_step
			sqlmock.AnyArg(), // awaited_steps
			sqlmock.AnyArg(), // collected_data
			sqlmock.AnyArg(), // initial_request_data
			sqlmock.AnyArg(), // created_at
			sqlmock.AnyArg(), // updated_at
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Fetch state
	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, clientID, StatusRunning, "step1", "[]",
		"{}", nil, nil, nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Expect update to FAILED status
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,    // WHERE correlation_id = $1
			StatusFailed,     // status = $2
			"step1",          // current_step = $3
			sqlmock.AnyArg(), // awaited_steps = $4
			sqlmock.AnyArg(), // collected_data = $5
			sqlmock.AnyArg(), // final_result = $6
			sqlmock.AnyArg(), // error = $7 (will contain "insufficient fuel")
			sqlmock.AnyArg(), // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Execute the workflow - it should fail with insufficient fuel error
	err := coordinator.ExecuteWorkflow(ctx, plan, headers, nil)

	// Assert that we got an error
	require.Error(t, err, "Expected an error for insufficient fuel")
	assert.Contains(t, err.Error(), "insufficient fuel", "Error should mention insufficient fuel")

	// Verify all expectations were met
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestHandleFanOut verifies that multiple messages are sent in parallel.
func TestHandleFanOut(t *testing.T) {
	coordinator, mockProducer, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id":      correlationID,
		"client_id":           clientID,
		"request_id":          "parent_req_1",
		governance.FuelHeader: "995", // Already deducted by ExecuteWorkflow (1000 - 5)
	}

	step := models.Step{
		Action:   "fan_out",
		NextStep: "aggregate_results",
		SubTasks: []models.SubTask{
			{StepName: "get_research", Topic: "topic.research"},
			{StepName: "get_style", Topic: "topic.style"},
		},
	}
	state := &OrchestrationState{
		CorrelationID: correlationID,
		ClientID:      clientID,
		CollectedData: make(map[string]interface{}),
	}

	// Expect messages to be produced with validation
	var capturedRequestIDs []string
	mockProducer.On("Produce", ctx, "topic.research", mock.MatchedBy(func(h map[string]string) bool {
		if h["causation_id"] == "parent_req_1" && h["request_id"] != "parent_req_1" &&
			h[governance.FuelHeader] == "995" { // Fuel already deducted
			capturedRequestIDs = append(capturedRequestIDs, h["request_id"])
			return true
		}
		return false
	}), []byte(correlationID), mock.AnythingOfType("[]uint8")).Return(nil).Once()

	mockProducer.On("Produce", ctx, "topic.style", mock.MatchedBy(func(h map[string]string) bool {
		if h["causation_id"] == "parent_req_1" && h["request_id"] != "parent_req_1" &&
			h[governance.FuelHeader] == "995" { // Fuel already deducted
			capturedRequestIDs = append(capturedRequestIDs, h["request_id"])
			return true
		}
		return false
	}), []byte(correlationID), mock.AnythingOfType("[]uint8")).Return(nil).Once()

	// Expect state update
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,           // WHERE correlation_id = $1
			StatusAwaitingResponses, // status = $2
			"aggregate_results",     // current_step = $3
			sqlmock.AnyArg(),        // awaited_steps = $4
			sqlmock.AnyArg(),        // collected_data = $5
			sqlmock.AnyArg(),        // final_result = $6
			"",                      // error = $7
			sqlmock.AnyArg(),        // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	err := coordinator.handleFanOut(ctx, headers, step, state)
	require.NoError(t, err)

	// Verify state was updated correctly
	assert.Equal(t, StatusAwaitingResponses, state.Status)
	assert.Equal(t, "aggregate_results", state.CurrentStep)
	assert.Len(t, state.AwaitedSteps, 2)

	mockProducer.AssertExpectations(t)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestHandlePauseForHumanInput verifies human approval pause functionality.
func TestHandlePauseForHumanInput(t *testing.T) {
	coordinator, mockProducer, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	projectID := "project_123"
	clientID := "client_123"
	headers := map[string]string{
		"correlation_id": correlationID,
		"project_id":     projectID,
		"client_id":      clientID,
	}

	step := models.Step{
		Action:      "pause_for_human_input",
		NextStep:    "after_approval",
		Description: "Review generated content",
	}
	state := &OrchestrationState{
		CorrelationID: correlationID,
		ClientID:      clientID,
		CollectedData: map[string]interface{}{
			"generated_content": "Some content to review",
		},
	}

	// Expect state update
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,        // WHERE correlation_id = $1
			StatusPausedForHuman, // status = $2
			"after_approval",     // current_step = $3
			sqlmock.AnyArg(),     // awaited_steps = $4
			sqlmock.AnyArg(),     // collected_data = $5
			sqlmock.AnyArg(),     // final_result = $6
			"",                   // error = $7
			sqlmock.AnyArg(),     // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Expect notification to be sent
	mockProducer.On("Produce", ctx, NotificationTopic, headers, []byte(correlationID), mock.MatchedBy(func(payload []byte) bool {
		var notification map[string]interface{}
		json.Unmarshal(payload, &notification)
		return notification["event_type"] == "WORKFLOW_PAUSED_FOR_APPROVAL" &&
			notification["correlation_id"] == correlationID &&
			notification["project_id"] == projectID &&
			notification["client_id"] == clientID
	})).Return(nil).Once()

	err := coordinator.handlePauseForHumanInput(ctx, headers, step, state)
	require.NoError(t, err)

	assert.Equal(t, StatusPausedForHuman, state.Status)
	assert.Equal(t, "after_approval", state.CurrentStep)

	mockProducer.AssertExpectations(t)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestHandleResponse verifies processing of sub-task responses.
func TestHandleResponse(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	causationID := "request_123"
	headers := map[string]string{
		"correlation_id": correlationID,
		"causation_id":   causationID,
	}

	taskResponse := models.TaskResponse{
		Success: true,
		Data: map[string]interface{}{
			"result": "task completed",
		},
	}
	responseBytes, _ := json.Marshal(taskResponse)

	// Existing state with awaited steps
	existingData := map[string]interface{}{
		"existing": "data",
	}
	existingDataJSON, _ := json.Marshal(existingData)
	awaitedSteps := []string{causationID, "another_request"}
	awaitedStepsJSON, _ := json.Marshal(awaitedSteps)

	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, "client_123", StatusAwaitingResponses, "aggregate", string(awaitedStepsJSON),
		string(existingDataJSON), nil, nil, nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Expect state update with response data added
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,           // WHERE correlation_id = $1
			StatusAwaitingResponses, // status = $2 (still awaiting one more)
			"aggregate",             // current_step = $3
			sqlmock.AnyArg(),        // awaited_steps = $4 (should have one less)
			sqlmock.AnyArg(),        // collected_data = $5 (should include new data)
			sqlmock.AnyArg(),        // final_result = $6
			"",                      // error = $7
			sqlmock.AnyArg(),        // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	err := coordinator.HandleResponse(ctx, headers, responseBytes)
	require.NoError(t, err)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestResumeWorkflow verifies resuming after human approval.
func TestResumeWorkflow(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	headers := map[string]string{
		"correlation_id": correlationID,
	}

	t.Run("approved", func(t *testing.T) {
		resumePayload := struct {
			Approved bool                   `json:"approved"`
			Feedback map[string]interface{} `json:"feedback,omitempty"`
		}{
			Approved: true,
			Feedback: map[string]interface{}{
				"comment": "Looks good!",
			},
		}
		resumeData, _ := json.Marshal(resumePayload)

		// Existing paused state
		rows := sqlmock.NewRows([]string{
			"correlation_id", "client_id", "status", "current_step", "awaited_steps",
			"collected_data", "initial_request_data", "final_result", "error",
			"created_at", "updated_at",
		}).AddRow(
			correlationID, "client_123", StatusPausedForHuman, "after_approval", "[]",
			"{}", nil, nil, nil,
			time.Now(), time.Now(),
		)
		mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
			WithArgs(correlationID).
			WillReturnRows(rows)

		// Expect state update to running
		mockDB.ExpectExec("UPDATE orchestrator_state SET").
			WithArgs(
				correlationID,    // WHERE correlation_id = $1
				StatusRunning,    // status = $2
				"after_approval", // current_step = $3
				sqlmock.AnyArg(), // awaited_steps = $4
				sqlmock.AnyArg(), // collected_data = $5 (should include feedback)
				sqlmock.AnyArg(), // final_result = $6
				"",               // error = $7
				sqlmock.AnyArg(), // updated_at = $8
			).WillReturnResult(sqlmock.NewResult(1, 1))

		err := coordinator.ResumeWorkflow(ctx, headers, resumeData)
		require.NoError(t, err)
		require.NoError(t, mockDB.ExpectationsWereMet())
	})

	t.Run("rejected", func(t *testing.T) {
		// Create fresh mocks for this subtest
		db2, mockDB2, err := sqlmock.New()
		require.NoError(t, err)
		defer db2.Close()

		// Create a new coordinator with the fresh DB
		coordinator2 := NewSagaCoordinator(db2, coordinator.producer, coordinator.logger)

		resumePayload := struct {
			Approved bool `json:"approved"`
		}{
			Approved: false,
		}
		resumeData, _ := json.Marshal(resumePayload)

		// Existing paused state
		rows := sqlmock.NewRows([]string{
			"correlation_id", "client_id", "status", "current_step", "awaited_steps",
			"collected_data", "initial_request_data", "final_result", "error",
			"created_at", "updated_at",
		}).AddRow(
			correlationID, "client_123", StatusPausedForHuman, "after_approval", "[]",
			"{}", nil, nil, nil,
			time.Now(), time.Now(),
		)
		mockDB2.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
			WithArgs(correlationID).
			WillReturnRows(rows)

		// Expect state update to failed
		mockDB2.ExpectExec("UPDATE orchestrator_state SET").
			WithArgs(
				correlationID,               // WHERE correlation_id = $1
				StatusFailed,                // status = $2
				"after_approval",            // current_step = $3
				sqlmock.AnyArg(),            // awaited_steps = $4
				sqlmock.AnyArg(),            // collected_data = $5
				sqlmock.AnyArg(),            // final_result = $6
				"Workflow rejected by user", // error = $7
				sqlmock.AnyArg(),            // updated_at = $8
			).WillReturnResult(sqlmock.NewResult(1, 1))

		err = coordinator2.ResumeWorkflow(ctx, headers, resumeData)
		require.NoError(t, err)
		require.NoError(t, mockDB2.ExpectationsWereMet())
	})
}

// TestCompleteWorkflow verifies workflow completion.
func TestCompleteWorkflow(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()

	state := &OrchestrationState{
		CorrelationID: correlationID,
		ClientID:      "client_123",
		Status:        StatusRunning,
		CurrentStep:   "final",
		CollectedData: map[string]interface{}{
			"step1_result": "data1",
			"step2_result": "data2",
		},
	}

	// Expect state update to completed
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,    // WHERE correlation_id = $1
			StatusCompleted,  // status = $2
			"final",          // current_step = $3
			sqlmock.AnyArg(), // awaited_steps = $4
			sqlmock.AnyArg(), // collected_data = $5
			sqlmock.AnyArg(), // final_result = $6 (should be marshaled collected_data)
			"",               // error = $7
			sqlmock.AnyArg(), // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	err := coordinator.completeWorkflow(ctx, state)
	require.NoError(t, err)

	assert.Equal(t, StatusCompleted, state.Status)
	assert.NotNil(t, state.FinalResult)

	require.NoError(t, mockDB.ExpectationsWereMet())
}
-------------------------------------------------
filepath = ./platform/orchestration/coordinator.go
// FILE: platform/orchestration/coordinator.go
package orchestration

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"github.com/google/uuid"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/governance"
	"github.com/gqls/agentchassis/platform/kafka"
	"go.uber.org/zap"
)

const (
	// Topic for notifications to the UI
	NotificationTopic = "system.notifications.ui"
	// Topic for receiving resume commands
	ResumeWorkflowTopic = "system.commands.workflow.resume"
)

// SagaCoordinator manages the execution of complex workflows
type SagaCoordinator struct {
	db          *sql.DB
	producer    kafka.Producer
	logger      *zap.Logger
	fuelManager *governance.FuelManager
}

// NewSagaCoordinator creates a new coordinator instance
func NewSagaCoordinator(db *sql.DB, producer kafka.Producer, logger *zap.Logger) *SagaCoordinator {
	return &SagaCoordinator{
		db:          db,
		producer:    producer,
		logger:      logger,
		fuelManager: governance.NewFuelManager(),
	}
}

// ExecuteWorkflow manages the execution of a workflow plan
func (s *SagaCoordinator) ExecuteWorkflow(ctx context.Context, plan models.WorkflowPlan, headers map[string]string, initialData []byte) error {
	correlationID := headers["correlation_id"]
	l := s.logger.With(zap.String("correlation_id", correlationID))

	// Get clientID from headers to pass to state creation
	clientID := headers["client_id"]
	if clientID == "" {
		return fmt.Errorf("client_id header is required to execute a workflow")
	}

	// Get or create state
	state, err := s.getOrCreateState(ctx, correlationID, clientID, plan, initialData)
	if err != nil {
		return err
	}

	// Check if workflow is already complete
	if state.Status == StatusCompleted || state.Status == StatusFailed {
		l.Info("Workflow already finished", zap.String("status", string(state.Status)))
		return nil
	}

	// Get current step configuration
	currentStepConfig, ok := plan.Steps[state.CurrentStep]
	if !ok {
		return s.failWorkflow(ctx, state, fmt.Sprintf("step '%s' not found in plan", state.CurrentStep))
	}

	// Check dependencies
	if !s.dependenciesMet(currentStepConfig.Dependencies, state) {
		l.Info("Dependencies not met, waiting", zap.Strings("dependencies", currentStepConfig.Dependencies))
		return nil
	}

	// Check fuel budget
	fuel, err := governance.GetFuelFromHeader(headers)
	if err != nil {
		return s.failWorkflow(ctx, state, fmt.Sprintf("failed to get fuel from headers: %v", err))
	}

	if !s.fuelManager.HasEnoughFuel(fuel, currentStepConfig.Action) {
		return s.failWorkflow(ctx, state, fmt.Sprintf("insufficient fuel for action '%s': have %d, need %d",
			currentStepConfig.Action, fuel, s.fuelManager.GetCost(currentStepConfig.Action)))
	}

	// Deduct fuel and update headers
	remainingFuel := s.fuelManager.DeductFuel(fuel, currentStepConfig.Action)
	governance.SetFuelHeader(headers, remainingFuel)

	// Execute the action
	switch currentStepConfig.Action {
	case "fan_out":
		return s.handleFanOut(ctx, headers, currentStepConfig, state)
	case "pause_for_human_input":
		return s.handlePauseForHumanInput(ctx, headers, currentStepConfig, state)
	case "complete_workflow":
		return s.completeWorkflow(ctx, state)
	default:
		return s.handleStandardAction(ctx, headers, currentStepConfig, state)
	}
}

// getOrCreateState retrieves existing state or creates new one
func (s *SagaCoordinator) getOrCreateState(ctx context.Context, correlationID string, clientID string, plan models.WorkflowPlan, initialData []byte) (*OrchestrationState, error) {
	repo := NewStateRepository(s.db, s.logger)

	state, err := repo.GetState(ctx, correlationID)
	if err != nil {
		// State doesn't exist, create it
		if err := repo.CreateInitialState(ctx, correlationID, clientID, plan.StartStep, initialData); err != nil {
			return nil, fmt.Errorf("failed to create initial state: %w", err)
		}
		return repo.GetState(ctx, correlationID)
	}

	return state, nil
}

// dependenciesMet checks if all required dependencies have been completed
func (s *SagaCoordinator) dependenciesMet(dependencies []string, state *OrchestrationState) bool {
	for _, dep := range dependencies {
		if _, ok := state.CollectedData[dep]; !ok {
			return false
		}
	}
	return true
}

// handleStandardAction sends a message to the specified topic
func (s *SagaCoordinator) handleStandardAction(ctx context.Context, headers map[string]string, step models.Step, state *OrchestrationState) error {
	l := s.logger.With(zap.String("correlation_id", state.CorrelationID))

	// Prepare the message payload
	payload := models.TaskRequest{
		Action: step.Action,
		Data:   state.CollectedData,
	}
	payloadBytes, _ := json.Marshal(payload)

	// Create new request ID for this sub-task
	newRequestID := uuid.NewString()
	outHeaders := make(map[string]string)
	for k, v := range headers {
		outHeaders[k] = v
	}
	outHeaders["causation_id"] = headers["request_id"]
	outHeaders["request_id"] = newRequestID

	// Send the message
	if err := s.producer.Produce(ctx, step.Topic, outHeaders, []byte(state.CorrelationID), payloadBytes); err != nil {
		return fmt.Errorf("failed to produce message: %w", err)
	}

	// Update state to await response
	state.Status = StatusAwaitingResponses
	state.CurrentStep = step.NextStep
	state.AwaitedSteps = []string{newRequestID}

	repo := NewStateRepository(s.db, s.logger)
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	l.Info("Standard action executed", zap.String("action", step.Action), zap.String("topic", step.Topic))
	return nil
}

// handleFanOut sends multiple parallel requests
func (s *SagaCoordinator) handleFanOut(ctx context.Context, headers map[string]string, step models.Step, state *OrchestrationState) error {
	l := s.logger.With(zap.String("correlation_id", state.CorrelationID))

	awaitedSteps := make([]string, 0, len(step.SubTasks))

	for _, subTask := range step.SubTasks {
		payload := models.TaskRequest{
			Action: subTask.StepName,
			Data:   state.CollectedData,
		}
		payloadBytes, _ := json.Marshal(payload)

		newRequestID := uuid.NewString()
		outHeaders := make(map[string]string)
		for k, v := range headers {
			outHeaders[k] = v
		}
		outHeaders["causation_id"] = headers["request_id"]
		outHeaders["request_id"] = newRequestID

		if err := s.producer.Produce(ctx, subTask.Topic, outHeaders, []byte(state.CorrelationID), payloadBytes); err != nil {
			return fmt.Errorf("failed to produce fan-out message: %w", err)
		}

		awaitedSteps = append(awaitedSteps, newRequestID)
	}

	// Update state
	state.Status = StatusAwaitingResponses
	state.CurrentStep = step.NextStep
	state.AwaitedSteps = awaitedSteps

	repo := NewStateRepository(s.db, s.logger)
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	l.Info("Fan-out executed", zap.Int("subtasks", len(step.SubTasks)))
	return nil
}

// handlePauseForHumanInput pauses the workflow and notifies the UI
func (s *SagaCoordinator) handlePauseForHumanInput(ctx context.Context, headers map[string]string, step models.Step, state *OrchestrationState) error {
	l := s.logger.With(zap.String("correlation_id", state.CorrelationID))

	state.Status = StatusPausedForHuman
	state.CurrentStep = step.NextStep

	repo := NewStateRepository(s.db, s.logger)
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	// Send notification
	notification := map[string]interface{}{
		"event_type":      "WORKFLOW_PAUSED_FOR_APPROVAL",
		"correlation_id":  state.CorrelationID,
		"project_id":      headers["project_id"],
		"client_id":       headers["client_id"],
		"message":         fmt.Sprintf("Step '%s' requires your approval", step.Description),
		"data_for_review": state.CollectedData,
	}
	notificationBytes, _ := json.Marshal(notification)

	if err := s.producer.Produce(ctx, NotificationTopic, headers, []byte(state.CorrelationID), notificationBytes); err != nil {
		return fmt.Errorf("failed to send notification: %w", err)
	}

	l.Info("Workflow paused for human input")
	return nil
}

// HandleResponse processes a response from a sub-task
func (s *SagaCoordinator) HandleResponse(ctx context.Context, headers map[string]string, response []byte) error {
	correlationID := headers["correlation_id"]
	causationID := headers["causation_id"]

	l := s.logger.With(
		zap.String("correlation_id", correlationID),
		zap.String("causation_id", causationID),
	)

	repo := NewStateRepository(s.db, s.logger)
	state, err := repo.GetState(ctx, correlationID)
	if err != nil {
		return fmt.Errorf("failed to get state: %w", err)
	}

	// Parse response
	var taskResponse models.TaskResponse
	if err := json.Unmarshal(response, &taskResponse); err != nil {
		return fmt.Errorf("failed to unmarshal response: %w", err)
	}

	// Store response data
	state.CollectedData[causationID] = taskResponse.Data

	// Remove from awaited steps
	newAwaitedSteps := make([]string, 0)
	for _, step := range state.AwaitedSteps {
		if step != causationID {
			newAwaitedSteps = append(newAwaitedSteps, step)
		}
	}
	state.AwaitedSteps = newAwaitedSteps

	// If all responses received, set status back to running
	if len(state.AwaitedSteps) == 0 {
		state.Status = StatusRunning
	}

	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	l.Info("Response processed", zap.Int("remaining_awaited", len(state.AwaitedSteps)))

	// If all responses received, continue workflow
	if len(state.AwaitedSteps) == 0 {
		// Need to reload the workflow plan - this would come from the agent config
		// For now, we'll need to pass it through somehow
		// This is a limitation we'll address in the actual implementation
	}

	return nil
}

// ResumeWorkflow resumes a paused workflow after human input
func (s *SagaCoordinator) ResumeWorkflow(ctx context.Context, headers map[string]string, resumeData []byte) error {
	correlationID := headers["correlation_id"]
	l := s.logger.With(zap.String("correlation_id", correlationID))

	var resumePayload struct {
		Approved bool                   `json:"approved"`
		Feedback map[string]interface{} `json:"feedback,omitempty"`
	}
	if err := json.Unmarshal(resumeData, &resumePayload); err != nil {
		return fmt.Errorf("failed to unmarshal resume payload: %w", err)
	}

	repo := NewStateRepository(s.db, s.logger)
	state, err := repo.GetState(ctx, correlationID)
	if err != nil {
		return fmt.Errorf("failed to get state: %w", err)
	}

	if state.Status != StatusPausedForHuman {
		return fmt.Errorf("workflow not in paused state: %s", state.Status)
	}

	if !resumePayload.Approved {
		state.Status = StatusFailed
		state.Error = "Workflow rejected by user"
		return repo.UpdateState(ctx, state)
	}

	// Add feedback to collected data
	if resumePayload.Feedback != nil {
		state.CollectedData["human_feedback"] = resumePayload.Feedback
	}

	state.Status = StatusRunning
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	l.Info("Workflow resumed after human approval")

	// Continue workflow execution
	// This would trigger re-execution with the current state

	return nil
}

// completeWorkflow marks the workflow as completed
func (s *SagaCoordinator) completeWorkflow(ctx context.Context, state *OrchestrationState) error {
	state.Status = StatusCompleted
	finalResult, _ := json.Marshal(state.CollectedData)
	state.FinalResult = finalResult

	repo := NewStateRepository(s.db, s.logger)
	return repo.UpdateState(ctx, state)
}

// failWorkflow marks the workflow as failed
func (s *SagaCoordinator) failWorkflow(ctx context.Context, state *OrchestrationState, errorMsg string) error {
	state.Status = StatusFailed
	state.Error = errorMsg

	repo := NewStateRepository(s.db, s.logger)
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state to failed: %w", err)
	}

	// IMPORTANT: Return the error message as an error
	return fmt.Errorf(errorMsg)
}
-------------------------------------------------
filepath = ./platform/orchestration/state.go
// FILE: platform/orchestration/state.go
package orchestration

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"time"

	"go.uber.org/zap"
)

// OrchestrationStatus represents the current state of a workflow
type OrchestrationStatus string

const (
	StatusRunning           OrchestrationStatus = "RUNNING"
	StatusAwaitingResponses OrchestrationStatus = "AWAITING_RESPONSES"
	StatusPausedForHuman    OrchestrationStatus = "PAUSED_FOR_HUMAN_INPUT"
	StatusCompleted         OrchestrationStatus = "COMPLETED"
	StatusFailed            OrchestrationStatus = "FAILED"
)

// OrchestrationState is the database model for a Saga instance
type OrchestrationState struct {
	CorrelationID      string                 `db:"correlation_id"`
	ClientID           string                 `db:"client_id"`
	Status             OrchestrationStatus    `db:"status"`
	CurrentStep        string                 `db:"current_step"`
	AwaitedSteps       []string               `db:"awaited_steps"`
	CollectedData      map[string]interface{} `db:"collected_data"`
	InitialRequestData json.RawMessage        `db:"initial_request_data"`
	FinalResult        json.RawMessage        `db:"final_result"`
	Error              string                 `db:"error"`
	CreatedAt          time.Time              `db:"created_at"`
	UpdatedAt          time.Time              `db:"updated_at"`
}

// StateRepository provides an interface for persisting and retrieving workflow state
type StateRepository struct {
	db     *sql.DB
	logger *zap.Logger
}

// NewStateRepository creates a new state repository
func NewStateRepository(db *sql.DB, logger *zap.Logger) *StateRepository {
	return &StateRepository{db: db, logger: logger}
}

// CreateInitialState creates a new record for a workflow
func (r *StateRepository) CreateInitialState(ctx context.Context, correlationID, clientID, startStep string, initialData []byte) error {
	awaitedStepsJSON, _ := json.Marshal([]string{})
	collectedDataJSON, _ := json.Marshal(map[string]interface{}{})

	query := `
        INSERT INTO orchestrator_state 
        (correlation_id, client_id, status, current_step, awaited_steps, collected_data, initial_request_data, created_at, updated_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
    `

	now := time.Now().UTC()
	_, err := r.db.ExecContext(ctx, query,
		correlationID, clientID, StatusRunning, startStep, awaitedStepsJSON, collectedDataJSON, initialData, now, now)

	if err != nil {
		r.logger.Error("Failed to create initial orchestration state", zap.Error(err))
		return fmt.Errorf("failed to create initial state: %w", err)
	}

	r.logger.Info("Initial orchestration state created", zap.String("correlation_id", correlationID))
	return nil
}

// GetState retrieves the current state of a workflow
func (r *StateRepository) GetState(ctx context.Context, correlationID string) (*OrchestrationState, error) {
	query := `
        SELECT correlation_id, client_id, status, current_step, awaited_steps, collected_data, 
               initial_request_data, final_result, error, created_at, updated_at
        FROM orchestrator_state
        WHERE correlation_id = $1
    `

	var state OrchestrationState
	var awaitedStepsJSON, collectedDataJSON []byte
	var initialRequestDataNull sql.NullString // Handle NULL for initial_request_data
	var finalResultNull sql.NullString
	var errorNull sql.NullString

	err := r.db.QueryRowContext(ctx, query, correlationID).Scan(
		&state.CorrelationID,
		&state.ClientID,
		&state.Status,
		&state.CurrentStep,
		&awaitedStepsJSON,
		&collectedDataJSON,
		&initialRequestDataNull, // Scan into NullString
		&finalResultNull,
		&errorNull,
		&state.CreatedAt,
		&state.UpdatedAt,
	)

	if err != nil {
		if err == sql.ErrNoRows {
			return nil, fmt.Errorf("state not found for correlation_id: %s", correlationID)
		}
		return nil, fmt.Errorf("failed to get state: %w", err)
	}

	// Handle nullable fields
	if initialRequestDataNull.Valid {
		state.InitialRequestData = json.RawMessage(initialRequestDataNull.String)
	} else {
		state.InitialRequestData = json.RawMessage("{}") // Default to empty JSON
	}

	if finalResultNull.Valid {
		state.FinalResult = json.RawMessage(finalResultNull.String)
	} else {
		state.FinalResult = json.RawMessage("{}") // Default to empty JSON
	}

	if errorNull.Valid {
		state.Error = errorNull.String
	}

	// Unmarshal JSON fields
	if err := json.Unmarshal(awaitedStepsJSON, &state.AwaitedSteps); err != nil {
		return nil, fmt.Errorf("failed to unmarshal awaited_steps: %w", err)
	}
	if err := json.Unmarshal(collectedDataJSON, &state.CollectedData); err != nil {
		return nil, fmt.Errorf("failed to unmarshal collected_data: %w", err)
	}

	return &state, nil
}

// UpdateState persists changes to a workflow's state
func (r *StateRepository) UpdateState(ctx context.Context, state *OrchestrationState) error {
	awaitedStepsJSON, _ := json.Marshal(state.AwaitedSteps)
	collectedDataJSON, _ := json.Marshal(state.CollectedData)

	query := `
        UPDATE orchestrator_state 
        SET status = $2, current_step = $3, awaited_steps = $4, collected_data = $5, 
            final_result = $6, error = $7, updated_at = $8
        WHERE correlation_id = $1
    `

	_, err := r.db.ExecContext(ctx, query,
		state.CorrelationID,
		state.Status,
		state.CurrentStep,
		awaitedStepsJSON,
		collectedDataJSON,
		state.FinalResult,
		state.Error,
		time.Now().UTC(),
	)

	if err != nil {
		r.logger.Error("Failed to update orchestration state", zap.Error(err))
		return fmt.Errorf("failed to update state: %w", err)
	}

	r.logger.Debug("Orchestration state updated",
		zap.String("correlation_id", state.CorrelationID),
		zap.String("status", string(state.Status)))
	return nil
}

// GetOrchestratorStateTableSchema returns the SQL for creating the state table
func GetOrchestratorStateTableSchema() string {
	return `
CREATE TABLE IF NOT EXISTS orchestrator_state (
    correlation_id UUID PRIMARY KEY,
    status VARCHAR(50) NOT NULL,
    current_step VARCHAR(255) NOT NULL,
    awaited_steps JSONB DEFAULT '[]',
    collected_data JSONB DEFAULT '{}',
    initial_request_data JSONB,
    final_result JSONB,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_orchestrator_state_status ON orchestrator_state(status);
CREATE INDEX idx_orchestrator_state_updated_at ON orchestrator_state(updated_at);
`
}
-------------------------------------------------
filepath = ./platform/governance/fuel.go
// FILE: platform/governance/fuel.go
package governance

import (
	"fmt"
	"strconv"
)

const (
	// FuelHeader is the standard Kafka message header key for the fuel budget
	FuelHeader = "fuel_budget"
)

// CostTable defines the "price" in fuel units for various agent actions
var CostTable = map[string]int{
	"default_step":                   1,
	"fan_out":                        5,
	"ai_text_generate_claude_haiku":  10,
	"ai_text_generate_claude_sonnet": 25,
	"ai_text_generate_claude_opus":   50,
	"ai_image_generate_sdxl":         40,
	"web_search":                     5,
	"database_query":                 1,
	"memory_store":                   2,
	"memory_search":                  2,
	"pause_for_human_input":          0, // No cost for waiting
}

// FuelManager provides methods for checking and managing task fuel
type FuelManager struct{}

// NewFuelManager creates a new fuel manager
func NewFuelManager() *FuelManager {
	return &FuelManager{}
}

// GetCost returns the fuel cost for a given action
func (fm *FuelManager) GetCost(action string) int {
	if cost, ok := CostTable[action]; ok {
		return cost
	}
	// Return a default cost if the specific action isn't priced
	return CostTable["default_step"]
}

// HasEnoughFuel checks if the current budget is sufficient for an action
func (fm *FuelManager) HasEnoughFuel(currentFuel int, action string) bool {
	cost := fm.GetCost(action)
	return currentFuel >= cost
}

// DeductFuel subtracts the cost of an action from the current budget
func (fm *FuelManager) DeductFuel(currentFuel int, action string) int {
	cost := fm.GetCost(action)
	return currentFuel - cost
}

// GetFuelFromHeader safely parses the fuel value from Kafka message headers
func GetFuelFromHeader(headers map[string]string) (int, error) {
	fuelStr, ok := headers[FuelHeader]
	if !ok {
		return 0, fmt.Errorf("'%s' header not found", FuelHeader)
	}
	fuel, err := strconv.Atoi(fuelStr)
	if err != nil {
		return 0, fmt.Errorf("invalid fuel value in header: %w", err)
	}
	return fuel, nil
}

// SetFuelHeader sets the fuel budget in the headers map
func SetFuelHeader(headers map[string]string, fuel int) {
	headers[FuelHeader] = strconv.Itoa(fuel)
}
-------------------------------------------------
filepath = ./platform/database/migrations/001_enable_pgvector.sql
-- FILE: platform/database/migrations/001_enable_pgvector.sql
-- Run this on the clients database as superuser
CREATE EXTENSION IF NOT EXISTS vector;
-------------------------------------------------
filepath = ./platform/database/migrations/005_projects_schema.sql
// FILE: platform/database/migrations/005_projects_schema.sql
-- Projects table for auth database
CREATE TABLE IF NOT EXISTS projects (
                                        id VARCHAR(36) PRIMARY KEY,
    client_id VARCHAR(100) NOT NULL,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    owner_id VARCHAR(36) NOT NULL,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    FOREIGN KEY (owner_id) REFERENCES users(id),
    INDEX idx_projects_client (client_id),
    INDEX idx_projects_owner (owner_id)
    );

-- Subscriptions table
CREATE TABLE IF NOT EXISTS subscriptions (
                                             id VARCHAR(36) PRIMARY KEY,
    user_id VARCHAR(36) NOT NULL UNIQUE,
    tier VARCHAR(50) NOT NULL,
    status VARCHAR(50) NOT NULL,
    start_date TIMESTAMPTZ NOT NULL,
    end_date TIMESTAMPTZ,
    trial_ends_at TIMESTAMPTZ,
    cancelled_at TIMESTAMPTZ,
    payment_method VARCHAR(100),
    stripe_customer_id VARCHAR(255),
    stripe_subscription_id VARCHAR(255),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    FOREIGN KEY (user_id) REFERENCES users(id),
    INDEX idx_subscriptions_status (status)
    );

-- Subscription tiers table
CREATE TABLE IF NOT EXISTS subscription_tiers (
                                                  id VARCHAR(36) PRIMARY KEY,
    name VARCHAR(50) NOT NULL UNIQUE,
    display_name VARCHAR(100) NOT NULL,
    description TEXT,
    price_monthly DECIMAL(10,2) NOT NULL,
    price_yearly DECIMAL(10,2) NOT NULL,
    max_personas INT NOT NULL,
    max_projects INT NOT NULL,
    max_content_items INT NOT NULL,
    features JSON,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

-- Insert default tiers
INSERT INTO subscription_tiers (id, name, display_name, description, price_monthly, price_yearly, max_personas, max_projects, max_content_items, features) VALUES
                                                                                                                                                               ('00000000-0000-0000-0000-000000000001', 'free', 'Free', 'Basic features for getting started', 0.00, 0.00, 1, 3, 10, '["Basic personas", "Limited content generation"]'),
                                                                                                                                                               ('00000000-0000-0000-0000-000000000002', 'basic', 'Basic', 'For individual users', 9.99, 99.99, 5, 10, 100, '["All persona types", "Priority support", "Advanced templates"]'),
                                                                                                                                                               ('00000000-0000-0000-0000-000000000003', 'premium', 'Premium', 'For power users', 29.99, 299.99, 20, 50, 1000, '["All basic features", "Custom personas", "API access", "Analytics"]'),
                                                                                                                                                               ('00000000-0000-0000-0000-000000000004', 'enterprise', 'Enterprise', 'For organizations', 99.99, 999.99, -1, -1, -1, '["All premium features", "Unlimited usage", "Dedicated support", "Custom integrations"]');

-- User profiles table
CREATE TABLE IF NOT EXISTS user_profiles (
                                             user_id VARCHAR(36) PRIMARY KEY,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    company VARCHAR(255),
    phone VARCHAR(50),
    avatar_url VARCHAR(500),
    preferences JSON,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    FOREIGN KEY (user_id) REFERENCES users(id)
    );

-- Permissions table
CREATE TABLE IF NOT EXISTS permissions (
                                           id VARCHAR(36) PRIMARY KEY,
    name VARCHAR(100) NOT NULL UNIQUE,
    description TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

-- User permissions junction table
CREATE TABLE IF NOT EXISTS user_permissions (
                                                user_id VARCHAR(36) NOT NULL,
    permission_id VARCHAR(36) NOT NULL,
    granted_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    PRIMARY KEY (user_id, permission_id),
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (permission_id) REFERENCES permissions(id)
    );

-- Insert default permissions
INSERT INTO permissions (id, name, description) VALUES
                                                    ('00000000-0000-0000-0000-000000000001', 'personas.create', 'Create new personas'),
                                                    ('00000000-0000-0000-0000-000000000002', 'personas.delete', 'Delete personas'),
                                                    ('00000000-0000-0000-0000-000000000003', 'projects.manage', 'Manage all projects'),
                                                    ('00000000-0000-0000-0000-000000000004', 'admin.users', 'Manage users'),
                                                    ('00000000-0000-0000-0000-000000000005', 'admin.subscriptions', 'Manage subscriptions'),
                                                    ('00000000-0000-0000-0000-000000000006', '*', 'Super admin - all permissions');-------------------------------------------------
filepath = ./platform/database/migrations/003_create_client_schema.sql
-- FILE: platform/database/migrations/003_create_client_schema.sql


-- Enable required extensions

-- Global agent definitions table (shared across all clients)
CREATE TABLE IF NOT EXISTS agent_definitions (
                                                 id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    type VARCHAR(100) NOT NULL UNIQUE,
    display_name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(50) NOT NULL CHECK (category IN ('data-driven', 'code-driven', 'adapter')),
    default_config JSONB NOT NULL DEFAULT '{}',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    deleted_at TIMESTAMPTZ
    );

-- Index for active agent types
CREATE INDEX IF NOT EXISTS idx_agent_definitions_type_active
    ON agent_definitions(type, is_active) WHERE deleted_at IS NULL;

-- Global orchestrator state table (shared across all clients)
CREATE TABLE IF NOT EXISTS orchestrator_state (
                                                  correlation_id UUID PRIMARY KEY,
                                                  client_id VARCHAR(100) NOT NULL,
    status VARCHAR(50) NOT NULL,
    current_step VARCHAR(255) NOT NULL,
    awaited_steps JSONB DEFAULT '[]',
    collected_data JSONB DEFAULT '{}',
    initial_request_data JSONB,
    final_result JSONB,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

-- Indexes for orchestrator state
CREATE INDEX IF NOT EXISTS idx_orchestrator_state_status ON orchestrator_state(status);
CREATE INDEX IF NOT EXISTS idx_orchestrator_state_client ON orchestrator_state(client_id);
CREATE INDEX IF NOT EXISTS idx_orchestrator_state_updated_at ON orchestrator_state(updated_at);

-- Function to create client-specific schema
CREATE OR REPLACE FUNCTION create_client_schema(client_id TEXT)
RETURNS VOID AS $$
DECLARE
schema_name TEXT := 'client_' || client_id;
BEGIN
    -- Create schema
EXECUTE format('CREATE SCHEMA IF NOT EXISTS %I', schema_name);

-- Agent instances table for this client
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.agent_instances (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            template_id UUID NOT NULL,
            owner_user_id VARCHAR(255) NOT NULL,
            name VARCHAR(255) NOT NULL,
            config JSONB NOT NULL DEFAULT ''{}''::jsonb,
            is_active BOOLEAN DEFAULT true,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
        )', schema_name);

-- Indexes for agent instances
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_instances_owner
        ON %I.agent_instances(owner_user_id)', schema_name);

EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_instances_template
        ON %I.agent_instances(template_id)', schema_name);

-- Agent memory table with vector support
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.agent_memory (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            agent_instance_id UUID NOT NULL REFERENCES %I.agent_instances(id),
            content TEXT NOT NULL,
            embedding vector(1536) NOT NULL,
            metadata JSONB DEFAULT ''{}''::jsonb,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
        )', schema_name, schema_name);

-- Vector index for similarity search
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_memory_embedding
        ON %I.agent_memory USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100)', schema_name);

-- Index for agent memory queries
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_memory_agent_created
        ON %I.agent_memory(agent_instance_id, created_at DESC)', schema_name);

-- Projects table for this client
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.projects (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name VARCHAR(255) NOT NULL,
            description TEXT,
            owner_user_id VARCHAR(255) NOT NULL,
            settings JSONB DEFAULT ''{}''::jsonb,
            is_active BOOLEAN DEFAULT true,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
        )', schema_name);

-- Index for project queries
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_projects_owner
        ON %I.projects(owner_user_id)', schema_name);

-- Workflow executions table for this client
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.workflow_executions (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            correlation_id UUID NOT NULL,
            project_id UUID REFERENCES %I.projects(id),
            agent_instance_id UUID REFERENCES %I.agent_instances(id),
            status VARCHAR(50) NOT NULL,
            input_data JSONB,
            output_data JSONB,
            error_message TEXT,
            started_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            completed_at TIMESTAMPTZ,
            created_by VARCHAR(255) NOT NULL
        )', schema_name, schema_name, schema_name);

-- Indexes for workflow executions
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_workflow_executions_correlation
        ON %I.workflow_executions(correlation_id)', schema_name);

EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_workflow_executions_status
        ON %I.workflow_executions(status)', schema_name);

EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_workflow_executions_project
        ON %I.workflow_executions(project_id)', schema_name);

-- Usage analytics table for this client
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.usage_analytics (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id VARCHAR(255) NOT NULL,
            agent_type VARCHAR(100) NOT NULL,
            action VARCHAR(100) NOT NULL,
            fuel_consumed INTEGER NOT NULL DEFAULT 0,
            execution_time_ms INTEGER,
            success BOOLEAN NOT NULL,
            metadata JSONB DEFAULT ''{}''::jsonb,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
        )', schema_name);

-- Indexes for analytics
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_usage_analytics_user_date
        ON %I.usage_analytics(user_id, created_at)', schema_name);

EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_usage_analytics_agent_type
        ON %I.usage_analytics(agent_type, created_at)', schema_name);

END;
$$ LANGUAGE plpgsql;

-- Insert default agent definitions
INSERT INTO agent_definitions (type, display_name, description, category, default_config) VALUES
                                                                                              ('copywriter', 'Copywriter', 'Creates compelling marketing and content copy', 'data-driven', '{"model": "claude-3-sonnet", "temperature": 0.7}'),
                                                                                              ('researcher', 'Research Assistant', 'Conducts thorough research and analysis', 'data-driven', '{"model": "claude-3-opus", "temperature": 0.3}'),
                                                                                              ('reasoning', 'Reasoning Agent', 'Performs logical analysis and decision making', 'code-driven', '{"model": "claude-3-opus", "temperature": 0.2}'),
                                                                                              ('image-generator', 'Image Generator', 'Creates images using AI generation', 'adapter', '{"provider": "stability_ai", "model": "sdxl"}'),
                                                                                              ('web-search', 'Web Search', 'Searches the internet for information', 'adapter', '{"provider": "serpapi", "max_results": 10}')
    ON CONFLICT (type) DO UPDATE SET
    display_name = EXCLUDED.display_name,
                              description = EXCLUDED.description,
                              category = EXCLUDED.category,
                              default_config = EXCLUDED.default_config,
                              updated_at = NOW();

-- Create a demo client schema for testing
SELECT create_client_schema('demo_client');


-- This should be run for each new client
-- Replace {client_id} with actual client ID

CREATE SCHEMA IF NOT EXISTS client_{client_id};

-- Agent instances table
CREATE TABLE IF NOT EXISTS client_{client_id}.agent_instances (
                                                                  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    template_id UUID NOT NULL,
    owner_user_id VARCHAR(255) NOT NULL,
    name VARCHAR(255) NOT NULL,
    config JSONB NOT NULL DEFAULT '{}',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_instances_owner ON client_{client_id}.agent_instances(owner_user_id);
CREATE INDEX idx_instances_template ON client_{client_id}.agent_instances(template_id);

-- Agent memory table with vector support
CREATE TABLE IF NOT EXISTS client_{client_id}.agent_memory (
                                                               id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    agent_instance_id UUID NOT NULL REFERENCES client_{client_id}.agent_instances(id),
    content TEXT NOT NULL,
    embedding vector(1536) NOT NULL,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

-- Create vector index for similarity search
CREATE INDEX ON client_{client_id}.agent_memory USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- Orchestrator state table
CREATE TABLE IF NOT EXISTS client_{client_id}.orchestrator_state (
                                                                     correlation_id UUID PRIMARY KEY,
                                                                     status VARCHAR(50) NOT NULL,
    current_step VARCHAR(255) NOT NULL,
    awaited_steps JSONB DEFAULT '[]',
    collected_data JSONB DEFAULT '{}',
    initial_request_data JSONB,
    final_result JSONB,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_orchestrator_status ON client_{client_id}.orchestrator_state(status);

CREATE INDEX idx_memory_agent_created ON client_{client_id}.agent_memory(agent_instance_id, created_at DESC);
-------------------------------------------------
filepath = ./platform/database/migrations/004_auth_schema.sql
-- FILE: platform/database/migrations/004_auth_schema.sql
-- Auth database schema
CREATE TABLE IF NOT EXISTS users (
                                     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    role VARCHAR(50) DEFAULT 'user',
    client_id VARCHAR(100) NOT NULL,
    subscription_tier VARCHAR(50) DEFAULT 'free',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_client ON users(client_id);

CREATE TABLE IF NOT EXISTS auth_tokens (
                                           id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id),
    token_hash VARCHAR(255) NOT NULL,
    expires_at TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_tokens_user ON auth_tokens(user_id);
CREATE INDEX idx_tokens_expires ON auth_tokens(expires_at);

CREATE INDEX idx_users_email_active ON users(email, is_active);-------------------------------------------------
filepath = ./platform/database/migrations/002_create_templates_schema.sql
-- FILE: platform/database/migrations/002_create_templates_schema.sql
-- Templates database schema
CREATE TABLE IF NOT EXISTS persona_templates (
                                                 id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100),
    config JSONB NOT NULL DEFAULT '{}',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_templates_category ON persona_templates(category);
CREATE INDEX idx_templates_active ON persona_templates(is_active);
-------------------------------------------------
filepath = ./platform/database/postgres.go
// FILE: platform/database/postgres.go
package database

import (
	"context"
	"fmt"
	"os"
	"time"

	"github.com/gqls/agentchassis/platform/config"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// NewPostgresConnection creates a new PostgreSQL connection pool with retry logic
func NewPostgresConnection(ctx context.Context, dbCfg config.DatabaseConfig, logger *zap.Logger) (*pgxpool.Pool, error) {
	password := os.Getenv(dbCfg.PasswordEnvVar)
	if password == "" {
		return nil, fmt.Errorf("database password environment variable %s is not set", dbCfg.PasswordEnvVar)
	}

	connStr := fmt.Sprintf("postgresql://%s:%s@%s:%d/%s?sslmode=%s",
		dbCfg.User, password, dbCfg.Host, dbCfg.Port, dbCfg.DBName, dbCfg.SSLMode)

	poolConfig, err := pgxpool.ParseConfig(connStr)
	if err != nil {
		return nil, fmt.Errorf("failed to parse postgres connection string: %w", err)
	}

	poolConfig.MaxConns = 10
	poolConfig.MinConns = 2
	poolConfig.MaxConnLifetime = time.Hour
	poolConfig.MaxConnIdleTime = 30 * time.Minute

	var pool *pgxpool.Pool
	for i := 0; i < 5; i++ {
		pool, err = pgxpool.NewWithConfig(ctx, poolConfig)
		if err == nil {
			pingCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
			defer cancel()
			if err = pool.Ping(pingCtx); err == nil {
				logger.Info("Successfully connected to PostgreSQL database.", zap.String("database", dbCfg.DBName))
				return pool, nil
			}
		}
		logger.Warn("Failed to connect to PostgreSQL, retrying...",
			zap.Int("attempt", i+1),
			zap.String("database", dbCfg.DBName),
			zap.Error(err),
		)
		time.Sleep(5 * time.Second)
	}

	return nil, fmt.Errorf("failed to connect to postgres after multiple attempts: %w", err)
}
-------------------------------------------------
filepath = ./platform/database/mysql.go
// FILE: platform/database/mysql.go
package database

import (
	"context"
	"database/sql"
	"fmt"
	"os"
	"time"

	_ "github.com/go-sql-driver/mysql"
	"github.com/gqls/agentchassis/platform/config"
	"go.uber.org/zap"
)

// NewMySQLConnection creates a new MySQL database connection pool with retry logic
func NewMySQLConnection(ctx context.Context, dbCfg config.DatabaseConfig, logger *zap.Logger) (*sql.DB, error) {
	password := os.Getenv(dbCfg.PasswordEnvVar)
	if password == "" {
		return nil, fmt.Errorf("database password environment variable %s is not set", dbCfg.PasswordEnvVar)
	}

	// DSN format for MySQL
	dsn := fmt.Sprintf("%s:%s@tcp(%s:%d)/%s?parseTime=true",
		dbCfg.User, password, dbCfg.Host, dbCfg.Port, dbCfg.DBName)

	var db *sql.DB
	var err error

	// Retry loop for initial connection
	for i := 0; i < 5; i++ {
		db, err = sql.Open("mysql", dsn)
		if err == nil {
			pingCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
			defer cancel()
			if err = db.PingContext(pingCtx); err == nil {
				// Set connection pool parameters
				db.SetMaxOpenConns(10)
				db.SetMaxIdleConns(5)
				db.SetConnMaxLifetime(time.Hour)

				logger.Info("Successfully connected to MySQL database.", zap.String("database", dbCfg.DBName))
				return db, nil
			}
		}
		logger.Warn("Failed to connect to MySQL, retrying...",
			zap.Int("attempt", i+1),
			zap.String("database", dbCfg.DBName),
			zap.Error(err),
		)
		time.Sleep(5 * time.Second)
	}

	return nil, fmt.Errorf("failed to connect to mysql after multiple attempts: %w", err)
}
-------------------------------------------------
filepath = ./platform/database/pgvector.go
// FILE: platform/database/pgvector.go
package database

import (
	"context"
	"fmt"

	"github.com/google/uuid"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/pgvector/pgvector-go"
	"go.uber.org/zap"
)

// MemoryRecord represents a single entry in the agent_memory table
type MemoryRecord struct {
	ID              uuid.UUID
	AgentInstanceID uuid.UUID
	Content         string
	Embedding       []float32
	Metadata        map[string]interface{}
}

// MemoryRepository provides methods for storing and retrieving agent memories
type MemoryRepository struct {
	pool   *pgxpool.Pool
	logger *zap.Logger
}

// NewMemoryRepository creates a new repository for memory operations
func NewMemoryRepository(pool *pgxpool.Pool, logger *zap.Logger) *MemoryRepository {
	return &MemoryRepository{pool: pool, logger: logger}
}

// StoreMemory saves a new memory record to the database for a specific agent
func (r *MemoryRepository) StoreMemory(ctx context.Context, agentID uuid.UUID, content string, embedding []float32, metadata map[string]interface{}) error {
	l := r.logger.With(zap.String("agent_id", agentID.String()))
	l.Info("Storing new memory")

	query := `
        INSERT INTO agent_memory (agent_instance_id, content, embedding, metadata)
        VALUES ($1, $2, $3, $4)
    `
	_, err := r.pool.Exec(ctx, query, agentID, content, pgvector.NewVector(embedding), metadata)
	if err != nil {
		l.Error("Failed to store agent memory", zap.Error(err))
		return fmt.Errorf("failed to insert memory record: %w", err)
	}

	l.Debug("Successfully stored memory record")
	return nil
}

// SearchMemory performs a semantic similarity search to find the most relevant memories
func (r *MemoryRepository) SearchMemory(ctx context.Context, agentID uuid.UUID, queryEmbedding []float32, limit int) ([]MemoryRecord, error) {
	l := r.logger.With(zap.String("agent_id", agentID.String()))
	l.Info("Searching for relevant memories", zap.Int("limit", limit))

	query := `
        SELECT id, content, metadata
        FROM agent_memory
        WHERE agent_instance_id = $1
        ORDER BY embedding <=> $2
        LIMIT $3
    `
	rows, err := r.pool.Query(ctx, query, agentID, pgvector.NewVector(queryEmbedding), limit)
	if err != nil {
		l.Error("Failed to execute memory search query", zap.Error(err))
		return nil, fmt.Errorf("failed to search memory: %w", err)
	}
	defer rows.Close()

	var results []MemoryRecord
	for rows.Next() {
		var record MemoryRecord
		record.AgentInstanceID = agentID
		if err := rows.Scan(&record.ID, &record.Content, &record.Metadata); err != nil {
			l.Error("Failed to scan memory search result", zap.Error(err))
			continue
		}
		results = append(results, record)
	}

	l.Info("Memory search completed", zap.Int("results_found", len(results)))
	return results, nil
}
-------------------------------------------------
filepath = ./pkg/models/database.go
// FILE: pkg/models/database.go
package models

import (
	"context"
	"github.com/google/uuid"
	"time"
)

// Persona represents both templates and instances
type Persona struct {
	ID          uuid.UUID              `json:"id"`
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Category    string                 `json:"category"`
	Config      map[string]interface{} `json:"config"`
	IsTemplate  bool                   `json:"is_template"`
	IsActive    bool                   `json:"is_active"`
	CreatedAt   time.Time              `json:"created_at"`
	UpdatedAt   time.Time              `json:"updated_at"`
}

// PersonaRepository defines the interface for persona data access
type PersonaRepository interface {
	// Template methods
	CreateTemplate(ctx context.Context, template *Persona) (*Persona, error)
	GetTemplateByID(ctx context.Context, id string) (*Persona, error)
	ListTemplates(ctx context.Context) ([]Persona, error)
	UpdateTemplate(ctx context.Context, template *Persona) (*Persona, error)
	DeleteTemplate(ctx context.Context, id string) error

	// Instance methods
	CreateInstanceFromTemplate(ctx context.Context, templateID string, userID string, instanceName string) (*Persona, error)
	GetInstanceByID(ctx context.Context, id string) (*Persona, error)
	ListInstances(ctx context.Context, userID string) ([]Persona, error)
	UpdateInstance(ctx context.Context, id string, name *string, config map[string]interface{}) (*Persona, error)
	DeleteInstance(ctx context.Context, id string) error

	AdminUpdateInstanceConfig(ctx context.Context, clientID, instanceID string, config map[string]interface{}) error
}
-------------------------------------------------
filepath = ./pkg/models/contracts.go
// FILE: pkg/models/contracts.go (updated)
package models

import "time"

// AgentConfig defines the "mind" of an agent, loaded from the database
type AgentConfig struct {
	AgentID      string                 `json:"agent_id"`
	AgentType    string                 `json:"agent_type"`
	Version      int                    `json:"version"`
	CoreLogic    map[string]interface{} `json:"core_logic"`
	Workflow     WorkflowPlan           `json:"workflow"`
	MemoryConfig MemoryConfiguration    `json:"memory_config,omitempty"`
}

// MemoryConfiguration controls how the agent uses long-term memory
type MemoryConfiguration struct {
	Enabled            bool     `json:"enabled"`
	AutoStore          bool     `json:"auto_store"`
	AutoStoreThreshold float64  `json:"auto_store_threshold"`
	MaxMemories        int      `json:"max_memories"`
	RetrievalCount     int      `json:"retrieval_count"`
	EmbeddingModel     string   `json:"embedding_model"`
	IncludeTypes       []string `json:"include_types"`
}

// MemoryEntry represents a single memory to be stored
type MemoryEntry struct {
	Content   string                 `json:"content"`
	Type      string                 `json:"type"`
	Metadata  map[string]interface{} `json:"metadata"`
	Timestamp time.Time              `json:"timestamp"`
}

// WorkflowPlan defines the orchestration steps for an agent
type WorkflowPlan struct {
	StartStep string          `json:"start_step"`
	Steps     map[string]Step `json:"steps"`
}

// Step represents a single action or sub-workflow within a plan
type Step struct {
	Action       string    `json:"action"`
	Description  string    `json:"description"`
	Topic        string    `json:"topic,omitempty"`
	Dependencies []string  `json:"dependencies,omitempty"`
	NextStep     string    `json:"next_step,omitempty"`
	SubTasks     []SubTask `json:"sub_tasks,omitempty"`
	StoreMemory  bool      `json:"store_memory,omitempty"` // New field
}

// SubTask for fan-out operations
type SubTask struct {
	StepName string `json:"step_name"`
	Topic    string `json:"topic"`
}

// Standard message payloads
type TaskRequest struct {
	Action string                 `json:"action"`
	Data   map[string]interface{} `json:"data"`
}

type TaskResponse struct {
	Success bool                   `json:"success"`
	Data    map[string]interface{} `json:"data"`
	Error   string                 `json:"error,omitempty"`
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/variables.tf
variable "kustomize_path" {
  description = "The path to the Kustomize overlay to apply."
  type        = string
}

variable "service_name" {
  description = "The name of the Kubernetes deployment resource."
  type        = string
}

variable "namespace" {
  description = "The Kubernetes namespace to deploy into."
  type        = string
}

variable "image_tag" {
  description = "The Docker image tag to apply to the deployment."
  type        = string
  default     = "latest"
}

variable "image_repository" {
  description = "The Docker image repository (e.g., 'aqls/personae-auth-service')."
  type        = string
}

variable "config_sha" {
  description = "A hash of the service's config file to trigger updates."
  type        = string
  default     = ""
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/main.tf
resource "null_resource" "apply_kustomization" {
  triggers = {
    image_tag_trigger = var.image_tag
    config_sha_trigger = var.config_sha
  }

  provisioner "local-exec" {
    command = <<-EOT
      set -e
      echo "Applying Kustomize overlay at ${var.kustomize_path}"
      kubectl apply -k ${var.kustomize_path}

      echo "Setting image for deployment/${var.service_name} to ${var.image_repository}:${var.image_tag}"
      kubectl set image deployment/${var.service_name} ${var.service_name}=${var.image_repository}:${var.image_tag} -n ${var.namespace}

      echo "Waiting for rollout of deployment/${var.service_name}..."
      kubectl rollout status deployment/${var.service_name} -n ${var.namespace} --timeout=5m
    EOT
  }
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/outputs.tf
-------------------------------------------------
filepath = ./deployments/kustomize/base/configmap-common.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: common-config
  namespace: ai-persona-system
data:
  # Kafka configuration
  kafka_brokers: "kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092"

  # Database hosts
  clients_db_host: "postgres-clients"
  clients_db_port: "5432"
  clients_db_name: "clients_db"
  clients_db_user: "clients_user"

  templates_db_host: "postgres-templates"
  templates_db_port: "5432"
  templates_db_name: "templates_db"
  templates_db_user: "templates_user"

  auth_db_host: "mysql-auth"
  auth_db_port: "3306"
  auth_db_name: "auth_db"
  auth_db_user: "auth_user"

  # Object storage
  minio_endpoint: "http://minio:9000"
  minio_bucket: "agent-artifacts"

  # Service URLs
  core_manager_url: "http://core-manager:8088"
  auth_service_url: "http://auth-service:8081"

  # Observability
  tracing_endpoint: "otel-collector.monitoring.svc.cluster.local:4317"-------------------------------------------------
filepath = ./deployments/kustomize/base/network-policies.yaml
# Default deny all ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress

---
# Allow ingress from same namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector: {}

---
# Allow ingress from ingress controller
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-controller
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: auth-service
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8081

---
# Allow Prometheus scraping
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-prometheus
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090-------------------------------------------------
filepath = ./deployments/kustomize/base/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ai-persona-system
  labels:
    name: ai-persona-system
    monitoring: enabled-------------------------------------------------
filepath = ./deployments/kustomize/base/rbac-security.yaml
# FILE: k8s/rbac-security.yaml
# Service Account for applications
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ai-persona-app
  namespace: ai-persona-system
  labels:
    app: ai-persona-system

---
# Role for application access
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: ai-persona-system
  name: ai-persona-app-role
rules:
  # Allow reading secrets for configuration
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  # Allow reading configmaps
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list"]
  # Allow pod operations for health checks
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]

---
# Bind the role to the service account
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ai-persona-app-binding
  namespace: ai-persona-system
subjects:
  - kind: ServiceAccount
    name: ai-persona-app
    namespace: ai-persona-system
roleRef:
  kind: Role
  name: ai-persona-app-role
  apiGroup: rbac.authorization.k8s.io

---
# Pod Security Policy (if using older Kubernetes versions)
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: ai-persona-restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  runAsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'

---
# Network Policy - Default deny all ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress

---
# Network Policy - Allow same namespace communication
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector: {}

---
# Network Policy - Allow ingress controller access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-controller
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: auth-service
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8081

---
# Network Policy - Allow monitoring
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090

---
# Network Policy - Database access (only from specific apps)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: postgres-clients
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: core-manager
        - podSelector:
            matchLabels:
              app: agent-chassis
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 5432

---
# Network Policy - MySQL access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mysql-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: mysql-auth
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: auth-service
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 3306

---
# Network Policy - Kafka access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kafka-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: kafka
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: agent-chassis
        - podSelector:
            matchLabels:
              app: reasoning-agent
        - podSelector:
            matchLabels:
              app: image-generator-adapter
        - podSelector:
            matchLabels:
              app: web-search-adapter
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 9092-------------------------------------------------
filepath = ./deployments/kustomize/base/kustomization.yaml
-------------------------------------------------
filepath = ./build/docker/backend/reasoning-agent.dockerfile
# Dockerfile for the reasoning-agent service

# --- Build Stage ---
FROM golang:1.24-alpine AS builder

# Set the working directory inside the container
WORKDIR /app

# Copy go.mod and go.sum to download dependencies first, leveraging Docker cache
COPY go.mod go.sum ./
RUN go mod download

# Copy the entire source code
COPY . .

# Build the reasoning-agent binary
# The output path is /app/reasoning-agent
RUN CGO_ENABLED=0 GOOS=linux go build -v -o reasoning-agent ./cmd/reasoning-agent

# --- Final Stage ---
FROM alpine:latest

# Set the working directory
WORKDIR /root/

# Copy the built binary from the builder stage
COPY --from=builder /app/reasoning-agent .

# Expose the port the service might use for health checks (if any)
# EXPOSE 8080

# The command to run when the container starts
ENTRYPOINT ["./reasoning-agent"]-------------------------------------------------
filepath = ./configs/reasoning-agent.yaml
# FILE: configs/reasoning-agent.yaml
service_info:
  name: "reasoning-agent"
  version: "1.0.0"
  environment: "development"

server:
  port: "8082"

logging:
  level: "info"

infrastructure:
  kafka_brokers:
    - "kafka-0.kafka-headless:9092"
    - "kafka-1.kafka-headless:9092"
    - "kafka-2.kafka-headless:9092"

  clients_database: {}
  templates_database: {}
  auth_database: {}
  object_storage: {}

custom:
  ai_service:
    provider: "anthropic"
    model: "claude-3-opus-20240229"
    temperature: 0.2
    max_tokens: 2048
    api_key_env_var: "ANTHROPIC_API_KEY"-------------------------------------------------
filepath = ./go.mod
// FILE: go.mod
module github.com/gqls/agentchassis

go 1.23.0

toolchain go1.24.4

require (
	github.com/DATA-DOG/go-sqlmock v1.5.2
	github.com/aws/aws-sdk-go-v2 v1.25.1
	github.com/aws/aws-sdk-go-v2/config v1.27.0
	github.com/aws/aws-sdk-go-v2/credentials v1.17.0
	github.com/aws/aws-sdk-go-v2/service/s3 v1.51.0
	github.com/gin-gonic/gin v1.10.1
	github.com/go-sql-driver/mysql v1.7.1
	github.com/golang-jwt/jwt/v5 v5.2.1
	github.com/google/uuid v1.6.0
	github.com/gorilla/websocket v1.5.1
	github.com/jackc/pgx/v5 v5.5.5
	github.com/pgvector/pgvector-go v0.1.1
	github.com/prometheus/client_golang v1.22.0
	github.com/rs/cors v1.10.1
	github.com/segmentio/kafka-go v0.4.47
	github.com/sony/gobreaker v1.0.0
	github.com/spf13/viper v1.18.2
	github.com/stretchr/testify v1.10.0
	go.opentelemetry.io/otel v1.29.0
	go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.29.0
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.24.0
	go.opentelemetry.io/otel/sdk v1.29.0
	go.opentelemetry.io/otel/trace v1.29.0
	go.uber.org/zap v1.27.0
	golang.org/x/crypto v0.40.0
)

require (
	github.com/KyleBanks/depth v1.2.1 // indirect
	github.com/PuerkitoBio/purell v1.2.1 // indirect
	github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578 // indirect
	github.com/aws/aws-sdk-go-v2/aws/protocol/eventstream v1.6.1 // indirect
	github.com/aws/aws-sdk-go-v2/feature/ec2/imds v1.15.0 // indirect
	github.com/aws/aws-sdk-go-v2/internal/configsources v1.3.1 // indirect
	github.com/aws/aws-sdk-go-v2/internal/endpoints/v2 v2.6.1 // indirect
	github.com/aws/aws-sdk-go-v2/internal/ini v1.8.0 // indirect
	github.com/aws/aws-sdk-go-v2/internal/v4a v1.3.1 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/accept-encoding v1.11.1 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/checksum v1.3.1 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/presigned-url v1.11.1 // indirect
	github.com/aws/aws-sdk-go-v2/service/internal/s3shared v1.17.1 // indirect
	github.com/aws/aws-sdk-go-v2/service/sso v1.19.0 // indirect
	github.com/aws/aws-sdk-go-v2/service/ssooidc v1.22.0 // indirect
	github.com/aws/aws-sdk-go-v2/service/sts v1.27.0 // indirect
	github.com/aws/smithy-go v1.20.1 // indirect
	github.com/beorn7/perks v1.0.1 // indirect
	github.com/bytedance/sonic v1.13.3 // indirect
	github.com/bytedance/sonic/loader v0.3.0 // indirect
	github.com/cenkalti/backoff/v4 v4.2.1 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/chenzhuoyu/base64x v0.0.0-20230717121745-296ad89f973d // indirect
	github.com/cloudwego/base64x v0.1.5 // indirect
	github.com/cpuguy83/go-md2man/v2 v2.0.7 // indirect
	github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect
	github.com/fsnotify/fsnotify v1.7.0 // indirect
	github.com/gabriel-vasile/mimetype v1.4.9 // indirect
	github.com/gin-contrib/sse v1.1.0 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/go-openapi/jsonpointer v0.21.1 // indirect
	github.com/go-openapi/jsonreference v0.21.0 // indirect
	github.com/go-openapi/spec v0.21.0 // indirect
	github.com/go-openapi/swag v0.23.1 // indirect
	github.com/go-playground/locales v0.14.1 // indirect
	github.com/go-playground/universal-translator v0.18.1 // indirect
	github.com/go-playground/validator/v10 v10.27.0 // indirect
	github.com/goccy/go-json v0.10.5 // indirect
	github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0 // indirect
	github.com/hashicorp/hcl v1.0.0 // indirect
	github.com/jackc/pgpassfile v1.0.0 // indirect
	github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a // indirect
	github.com/jackc/puddle/v2 v2.2.1 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/klauspost/compress v1.18.0 // indirect
	github.com/klauspost/cpuid/v2 v2.3.0 // indirect
	github.com/leodido/go-urn v1.4.0 // indirect
	github.com/magiconair/properties v1.8.7 // indirect
	github.com/mailru/easyjson v0.9.0 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mitchellh/mapstructure v1.5.0 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/pelletier/go-toml/v2 v2.2.4 // indirect
	github.com/pierrec/lz4/v4 v4.1.22 // indirect
	github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 // indirect
	github.com/prometheus/client_model v0.6.1 // indirect
	github.com/prometheus/common v0.62.0 // indirect
	github.com/prometheus/procfs v0.15.1 // indirect
	github.com/russross/blackfriday/v2 v2.1.0 // indirect
	github.com/sagikazarmark/locafero v0.4.0 // indirect
	github.com/sagikazarmark/slog-shim v0.1.0 // indirect
	github.com/shurcooL/sanitized_anchor_name v1.0.0 // indirect
	github.com/sourcegraph/conc v0.3.0 // indirect
	github.com/spf13/afero v1.11.0 // indirect
	github.com/spf13/cast v1.6.0 // indirect
	github.com/spf13/pflag v1.0.5 // indirect
	github.com/stretchr/objx v0.5.2 // indirect
	github.com/subosito/gotenv v1.6.0 // indirect
	github.com/swaggo/files v1.0.1 // indirect
	github.com/swaggo/gin-swagger v1.6.0 // indirect
	github.com/swaggo/swag v1.16.5 // indirect
	github.com/twitchyliquid64/golang-asm v0.15.1 // indirect
	github.com/ugorji/go/codec v1.3.0 // indirect
	github.com/urfave/cli/v2 v2.27.7 // indirect
	github.com/xrash/smetrics v0.0.0-20250705151800-55b8f293f342 // indirect
	go.opentelemetry.io/otel/metric v1.29.0 // indirect
	go.opentelemetry.io/proto/otlp v1.3.1 // indirect
	go.uber.org/multierr v1.10.0 // indirect
	go.yaml.in/yaml/v2 v2.4.2 // indirect
	golang.org/x/arch v0.19.0 // indirect
	golang.org/x/exp v0.0.0-20230905200255-921286631fa9 // indirect
	golang.org/x/mod v0.26.0 // indirect
	golang.org/x/net v0.42.0 // indirect
	golang.org/x/sync v0.16.0 // indirect
	golang.org/x/sys v0.34.0 // indirect
	golang.org/x/text v0.27.0 // indirect
	golang.org/x/tools v0.35.0 // indirect
	google.golang.org/genproto/googleapis/api v0.0.0-20240513163218-0867130af1f8 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20240513163218-0867130af1f8 // indirect
	google.golang.org/grpc v1.64.1 // indirect
	google.golang.org/protobuf v1.36.6 // indirect
	gopkg.in/ini.v1 v1.67.0 // indirect
	gopkg.in/yaml.v2 v2.4.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
	sigs.k8s.io/yaml v1.5.0 // indirect
)
-------------------------------------------------
filepath = ./go.sum
github.com/DATA-DOG/go-sqlmock v1.5.2 h1:OcvFkGmslmlZibjAjaHm3L//6LiuBgolP7OputlJIzU=
github.com/DATA-DOG/go-sqlmock v1.5.2/go.mod h1:88MAG/4G7SMwSE3CeA0ZKzrT5CiOU3OJ+JlNzwDqpNU=
github.com/KyleBanks/depth v1.2.1 h1:5h8fQADFrWtarTdtDudMmGsC7GPbOAu6RVB3ffsVFHc=
github.com/KyleBanks/depth v1.2.1/go.mod h1:jzSb9d0L43HxTQfT+oSA1EEp2q+ne2uh6XgeJcm8brE=
github.com/PuerkitoBio/purell v1.2.1 h1:QsZ4TjvwiMpat6gBCBxEQI0rcS9ehtkKtSpiUnd9N28=
github.com/PuerkitoBio/purell v1.2.1/go.mod h1:ZwHcC/82TOaovDi//J/804umJFFmbOHPngi8iYYv/Eo=
github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578 h1:d+Bc7a5rLufV/sSk/8dngufqelfh6jnri85riMAaF/M=
github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578/go.mod h1:uGdkoq3SwY9Y+13GIhn11/XLaGBb4BfwItxLd5jeuXE=
github.com/aws/aws-sdk-go-v2 v1.25.1 h1:P7hU6A5qEdmajGwvae/zDkOq+ULLC9tQBTwqqiwFGpI=
github.com/aws/aws-sdk-go-v2 v1.25.1/go.mod h1:Evoc5AsmtveRt1komDwIsjHFyrP5tDuF1D1U+6z6pNo=
github.com/aws/aws-sdk-go-v2/aws/protocol/eventstream v1.6.1 h1:gTK2uhtAPtFcdRRJilZPx8uJLL2J85xK11nKtWL0wfU=
github.com/aws/aws-sdk-go-v2/aws/protocol/eventstream v1.6.1/go.mod h1:sxpLb+nZk7tIfCWChfd+h4QwHNUR57d8hA1cleTkjJo=
github.com/aws/aws-sdk-go-v2/config v1.27.0 h1:J5sdGCAHuWKIXLeXiqr8II/adSvetkx0qdZwdbXXpb0=
github.com/aws/aws-sdk-go-v2/config v1.27.0/go.mod h1:cfh8v69nuSUohNFMbIISP2fhmblGmYEOKs5V53HiHnk=
github.com/aws/aws-sdk-go-v2/credentials v1.17.0 h1:lMW2x6sKBsiAJrpi1doOXqWFyEPoE886DTb1X0wb7So=
github.com/aws/aws-sdk-go-v2/credentials v1.17.0/go.mod h1:uT41FIH8cCIxOdUYIL0PYyHlL1NoneDuDSCwg5VE/5o=
github.com/aws/aws-sdk-go-v2/feature/ec2/imds v1.15.0 h1:xWCwjjvVz2ojYTP4kBKUuUh9ZrXfcAXpflhOUUeXg1k=
github.com/aws/aws-sdk-go-v2/feature/ec2/imds v1.15.0/go.mod h1:j3fACuqXg4oMTQOR2yY7m0NmJY0yBK4L4sLsRXq1Ins=
github.com/aws/aws-sdk-go-v2/internal/configsources v1.3.1 h1:evvi7FbTAoFxdP/mixmP7LIYzQWAmzBcwNB/es9XPNc=
github.com/aws/aws-sdk-go-v2/internal/configsources v1.3.1/go.mod h1:rH61DT6FDdikhPghymripNUCsf+uVF4Cnk4c4DBKH64=
github.com/aws/aws-sdk-go-v2/internal/endpoints/v2 v2.6.1 h1:RAnaIrbxPtlXNVI/OIlh1sidTQ3e1qM6LRjs7N0bE0I=
github.com/aws/aws-sdk-go-v2/internal/endpoints/v2 v2.6.1/go.mod h1:nbgAGkH5lk0RZRMh6A4K/oG6Xj11eC/1CyDow+DUAFI=
github.com/aws/aws-sdk-go-v2/internal/ini v1.8.0 h1:hT8rVHwugYE2lEfdFE0QWVo81lF7jMrYJVDWI+f+VxU=
github.com/aws/aws-sdk-go-v2/internal/ini v1.8.0/go.mod h1:8tu/lYfQfFe6IGnaOdrpVgEL2IrrDOf6/m9RQum4NkY=
github.com/aws/aws-sdk-go-v2/internal/v4a v1.3.1 h1:rtYJd3w6IWCTVS8vmMaiXjW198noh2PBm5CiXyJea9o=
github.com/aws/aws-sdk-go-v2/internal/v4a v1.3.1/go.mod h1:zvXu+CTlib30LUy4LTNFc6HTZ/K6zCae5YIHTdX9wIo=
github.com/aws/aws-sdk-go-v2/service/internal/accept-encoding v1.11.1 h1:EyBZibRTVAs6ECHZOw5/wlylS9OcTzwyjeQMudmREjE=
github.com/aws/aws-sdk-go-v2/service/internal/accept-encoding v1.11.1/go.mod h1:JKpmtYhhPs7D97NL/ltqz7yCkERFW5dOlHyVl66ZYF8=
github.com/aws/aws-sdk-go-v2/service/internal/checksum v1.3.1 h1:5Wxh862HkXL9CbQ83BIkWKLIgQapGeuh5zG2G9OZtQk=
github.com/aws/aws-sdk-go-v2/service/internal/checksum v1.3.1/go.mod h1:V7GLA01pNUxMCYSQsibdVrqUrNIYIT/9lCOyR8ExNvQ=
github.com/aws/aws-sdk-go-v2/service/internal/presigned-url v1.11.1 h1:cVP8mng1RjDyI3JN/AXFCn5FHNlsBaBH0/MBtG1bg0o=
github.com/aws/aws-sdk-go-v2/service/internal/presigned-url v1.11.1/go.mod h1:C8sQjoyAsdfjC7hpy4+S6B92hnFzx0d0UAyHicaOTIE=
github.com/aws/aws-sdk-go-v2/service/internal/s3shared v1.17.1 h1:OYmmIcyw19f7x0qLBLQ3XsrCZSSyLhxd9GXng5evsN4=
github.com/aws/aws-sdk-go-v2/service/internal/s3shared v1.17.1/go.mod h1:s5rqdn74Vdg10k61Pwf4ZHEApOSD6CKRe6qpeHDq32I=
github.com/aws/aws-sdk-go-v2/service/s3 v1.51.0 h1:rNVsCe3bqTAhG+qjnHJKgYKdHEsqqo/GMK3gEYY8W6g=
github.com/aws/aws-sdk-go-v2/service/s3 v1.51.0/go.mod h1:lTW7O4iMAnO2o7H3XJTvqaWFZCH6zIPs+eP7RdG/yp0=
github.com/aws/aws-sdk-go-v2/service/sso v1.19.0 h1:u6OkVDxtBPnxPkZ9/63ynEe+8kHbtS5IfaC4PzVxzWM=
github.com/aws/aws-sdk-go-v2/service/sso v1.19.0/go.mod h1:YqbU3RS/pkDVu+v+Nwxvn0i1WB0HkNWEePWbmODEbbs=
github.com/aws/aws-sdk-go-v2/service/ssooidc v1.22.0 h1:6DL0qu5+315wbsAEEmzK+P9leRwNbkp+lGjPC+CEvb8=
github.com/aws/aws-sdk-go-v2/service/ssooidc v1.22.0/go.mod h1:olUAyg+FaoFaL/zFaeQQONjOZ9HXoxgvI/c7mQTYz7M=
github.com/aws/aws-sdk-go-v2/service/sts v1.27.0 h1:cjTRjh700H36MQ8M0LnDn33W3JmwC77mdxIIyPWCdpM=
github.com/aws/aws-sdk-go-v2/service/sts v1.27.0/go.mod h1:nXfOBMWPokIbOY+Gi7a1psWMSvskUCemZzI+SMB7Akc=
github.com/aws/smithy-go v1.20.1 h1:4SZlSlMr36UEqC7XOyRVb27XMeZubNcBNN+9IgEPIQw=
github.com/aws/smithy-go v1.20.1/go.mod h1:krry+ya/rV9RDcV/Q16kpu6ypI4K2czasz0NC3qS14E=
github.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=
github.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=
github.com/bytedance/sonic v1.5.0/go.mod h1:ED5hyg4y6t3/9Ku1R6dU/4KyJ48DZ4jPhfY1O2AihPM=
github.com/bytedance/sonic v1.9.1 h1:6iJ6NqdoxCDr6mbY8h18oSO+cShGSMRGCEo7F2h0x8s=
github.com/bytedance/sonic v1.9.1/go.mod h1:i736AoUSYt75HyZLoJW9ERYxcy6eaN6h4BZXU064P/U=
github.com/bytedance/sonic v1.10.0-rc/go.mod h1:ElCzW+ufi8qKqNW0FY314xriJhyJhuoJ3gFZdAHF7NM=
github.com/bytedance/sonic v1.13.3 h1:MS8gmaH16Gtirygw7jV91pDCN33NyMrPbN7qiYhEsF0=
github.com/bytedance/sonic v1.13.3/go.mod h1:o68xyaF9u2gvVBuGHPlUVCy+ZfmNNO5ETf1+KgkJhz4=
github.com/bytedance/sonic/loader v0.1.1/go.mod h1:ncP89zfokxS5LZrJxl5z0UJcsk4M4yY2JpfqGeCtNLU=
github.com/bytedance/sonic/loader v0.3.0 h1:dskwH8edlzNMctoruo8FPTJDF3vLtDT0sXZwvZJyqeA=
github.com/bytedance/sonic/loader v0.3.0/go.mod h1:N8A3vUdtUebEY2/VQC0MyhYeKUFosQU6FxH2JmUe6VI=
github.com/cenkalti/backoff/v4 v4.2.1 h1:y4OZtCnogmCPw98Zjyt5a6+QwPLGkiQsYW5oUqylYbM=
github.com/cenkalti/backoff/v4 v4.2.1/go.mod h1:Y3VNntkOUPxTVeUxJ/G5vcM//AlwfmyYozVcomhLiZE=
github.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=
github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
github.com/chenzhuoyu/base64x v0.0.0-20211019084208-fb5309c8db06/go.mod h1:DH46F32mSOjUmXrMHnKwZdA8wcEefY7UVqBKYGjpdQY=
github.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311 h1:qSGYFH7+jGhDF8vLC+iwCD4WpbV1EBDSzWkJODFLams=
github.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311/go.mod h1:b583jCggY9gE99b6G5LEC39OIiVsWj+R97kbl5odCEk=
github.com/chenzhuoyu/base64x v0.0.0-20230717121745-296ad89f973d h1:77cEq6EriyTZ0g/qfRdp61a3Uu/AWrgIq2s0ClJV1g0=
github.com/chenzhuoyu/base64x v0.0.0-20230717121745-296ad89f973d/go.mod h1:8EPpVsBuRksnlj1mLy4AWzRNQYxauNi62uWcE3to6eA=
github.com/chenzhuoyu/iasm v0.9.0/go.mod h1:Xjy2NpN3h7aUqeqM+woSuuvxmIe6+DDsiNLIrkAmYog=
github.com/cloudwego/base64x v0.1.5 h1:XPciSp1xaq2VCSt6lF0phncD4koWyULpl5bUxbfCyP4=
github.com/cloudwego/base64x v0.1.5/go.mod h1:0zlkT4Wn5C6NdauXdJRhSKRlJvmclQ1hhJgA0rcu/8w=
github.com/cloudwego/iasm v0.2.0/go.mod h1:8rXZaNYT2n95jn+zTI1sDr+IgcD2GVs0nlbbQPiEFhY=
github.com/cpuguy83/go-md2man/v2 v2.0.7 h1:zbFlGlXEAKlwXpmvle3d8Oe3YnkKIK4xSRTd3sHPnBo=
github.com/cpuguy83/go-md2man/v2 v2.0.7/go.mod h1:oOW0eioCTA6cOiMLiUPZOpcVxMig6NIQQ7OS05n1F4g=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc h1:U9qPSI2PIWSS1VwoXQT9A3Wy9MM3WgvqSxFWenqJduM=
github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=
github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=
github.com/fsnotify/fsnotify v1.7.0 h1:8JEhPFa5W2WU7YfeZzPNqzMP6Lwt7L2715Ggo0nosvA=
github.com/fsnotify/fsnotify v1.7.0/go.mod h1:40Bi/Hjc2AVfZrqy+aj+yEI+/bRxZnMJyTJwOpGvigM=
github.com/gabriel-vasile/mimetype v1.4.2 h1:w5qFW6JKBz9Y393Y4q372O9A7cUSequkh1Q7OhCmWKU=
github.com/gabriel-vasile/mimetype v1.4.2/go.mod h1:zApsH/mKG4w07erKIaJPFiX0Tsq9BFQgN3qGY5GnNgA=
github.com/gabriel-vasile/mimetype v1.4.9 h1:5k+WDwEsD9eTLL8Tz3L0VnmVh9QxGjRmjBvAG7U/oYY=
github.com/gabriel-vasile/mimetype v1.4.9/go.mod h1:WnSQhFKJuBlRyLiKohA/2DtIlPFAbguNaG7QCHcyGok=
github.com/gin-contrib/sse v0.1.0 h1:Y/yl/+YNO8GZSjAhjMsSuLt29uWRFHdHYUb5lYOV9qE=
github.com/gin-contrib/sse v0.1.0/go.mod h1:RHrZQHXnP2xjPF+u1gW/2HnVO7nvIa9PG3Gm+fLHvGI=
github.com/gin-contrib/sse v1.1.0 h1:n0w2GMuUpWDVp7qSpvze6fAu9iRxJY4Hmj6AmBOU05w=
github.com/gin-contrib/sse v1.1.0/go.mod h1:hxRZ5gVpWMT7Z0B0gSNYqqsSCNIJMjzvm6fqCz9vjwM=
github.com/gin-gonic/gin v1.9.1 h1:4idEAncQnU5cB7BeOkPtxjfCSye0AAm1R0RVIqJ+Jmg=
github.com/gin-gonic/gin v1.9.1/go.mod h1:hPrL7YrpYKXt5YId3A/Tnip5kqbEAP+KLuI3SUcPTeU=
github.com/gin-gonic/gin v1.10.1 h1:T0ujvqyCSqRopADpgPgiTT63DUQVSfojyME59Ei63pQ=
github.com/gin-gonic/gin v1.10.1/go.mod h1:4PMNQiOhvDRa013RKVbsiNwoyezlm2rm0uX/T7kzp5Y=
github.com/go-logr/logr v1.2.2/go.mod h1:jdQByPbusPIv2/zmleS9BjJVeZ6kBagPoEUsqbVz/1A=
github.com/go-logr/logr v1.4.2 h1:6pFjapn8bFcIbiKo3XT4j/BhANplGihG6tvd+8rYgrY=
github.com/go-logr/logr v1.4.2/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
github.com/go-logr/stdr v1.2.2 h1:hSWxHoqTgW2S2qGc0LTAI563KZ5YKYRhT3MFKZMbjag=
github.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=
github.com/go-openapi/jsonpointer v0.21.1 h1:whnzv/pNXtK2FbX/W9yJfRmE2gsmkfahjMKB0fZvcic=
github.com/go-openapi/jsonpointer v0.21.1/go.mod h1:50I1STOfbY1ycR8jGz8DaMeLCdXiI6aDteEdRNNzpdk=
github.com/go-openapi/jsonreference v0.21.0 h1:Rs+Y7hSXT83Jacb7kFyjn4ijOuVGSvOdF2+tg1TRrwQ=
github.com/go-openapi/jsonreference v0.21.0/go.mod h1:LmZmgsrTkVg9LG4EaHeY8cBDslNPMo06cago5JNLkm4=
github.com/go-openapi/spec v0.21.0 h1:LTVzPc3p/RzRnkQqLRndbAzjY0d0BCL72A6j3CdL9ZY=
github.com/go-openapi/spec v0.21.0/go.mod h1:78u6VdPw81XU44qEWGhtr982gJ5BWg2c0I5XwVMotYk=
github.com/go-openapi/swag v0.23.1 h1:lpsStH0n2ittzTnbaSloVZLuB5+fvSY/+hnagBjSNZU=
github.com/go-openapi/swag v0.23.1/go.mod h1:STZs8TbRvEQQKUA+JZNAm3EWlgaOBGpyFDqQnDHMef0=
github.com/go-pg/pg/v10 v10.11.0 h1:CMKJqLgTrfpE/aOVeLdybezR2om071Vh38OLZjsyMI0=
github.com/go-pg/pg/v10 v10.11.0/go.mod h1:4BpHRoxE61y4Onpof3x1a2SQvi9c+q1dJnrNdMjsroA=
github.com/go-pg/zerochecker v0.2.0 h1:pp7f72c3DobMWOb2ErtZsnrPaSvHd2W4o9//8HtF4mU=
github.com/go-pg/zerochecker v0.2.0/go.mod h1:NJZ4wKL0NmTtz0GKCoJ8kym6Xn/EQzXRl2OnAe7MmDo=
github.com/go-playground/assert/v2 v2.2.0 h1:JvknZsQTYeFEAhQwI4qEt9cyV5ONwRHC+lYKSsYSR8s=
github.com/go-playground/assert/v2 v2.2.0/go.mod h1:VDjEfimB/XKnb+ZQfWdccd7VUvScMdVu0Titje2rxJ4=
github.com/go-playground/locales v0.14.1 h1:EWaQ/wswjilfKLTECiXz7Rh+3BjFhfDFKv/oXslEjJA=
github.com/go-playground/locales v0.14.1/go.mod h1:hxrqLVvrK65+Rwrd5Fc6F2O76J/NuW9t0sjnWqG1slY=
github.com/go-playground/universal-translator v0.18.1 h1:Bcnm0ZwsGyWbCzImXv+pAJnYK9S473LQFuzCbDbfSFY=
github.com/go-playground/universal-translator v0.18.1/go.mod h1:xekY+UJKNuX9WP91TpwSH2VMlDf28Uj24BCp08ZFTUY=
github.com/go-playground/validator/v10 v10.14.0 h1:vgvQWe3XCz3gIeFDm/HnTIbj6UGmg/+t63MyGU2n5js=
github.com/go-playground/validator/v10 v10.14.0/go.mod h1:9iXMNT7sEkjXb0I+enO7QXmzG6QCsPWY4zveKFVRSyU=
github.com/go-playground/validator/v10 v10.27.0 h1:w8+XrWVMhGkxOaaowyKH35gFydVHOvC0/uWoy2Fzwn4=
github.com/go-playground/validator/v10 v10.27.0/go.mod h1:I5QpIEbmr8On7W0TktmJAumgzX4CA1XNl4ZmDuVHKKo=
github.com/go-sql-driver/mysql v1.7.1 h1:lUIinVbN1DY0xBg0eMOzmmtGoHwWBbvnWubQUrtU8EI=
github.com/go-sql-driver/mysql v1.7.1/go.mod h1:OXbVy3sEdcQ2Doequ6Z5BW6fXNQTmx+9S1MCJN5yJMI=
github.com/goccy/go-json v0.10.2 h1:CrxCmQqYDkv1z7lO7Wbh2HN93uovUHgrECaO5ZrCXAU=
github.com/goccy/go-json v0.10.2/go.mod h1:6MelG93GURQebXPDq3khkgXZkazVtN9CRI+MGFi0w8I=
github.com/goccy/go-json v0.10.5 h1:Fq85nIqj+gXn/S5ahsiTlK3TmC85qgirsdTP/+DeaC4=
github.com/goccy/go-json v0.10.5/go.mod h1:oq7eo15ShAhp70Anwd5lgX2pLfOS3QCiwU/PULtXL6M=
github.com/golang-jwt/jwt/v5 v5.2.1 h1:OuVbFODueb089Lh128TAcimifWaLhJwVflnrgM17wHk=
github.com/golang-jwt/jwt/v5 v5.2.1/go.mod h1:pqrtFR0X4osieyHYxtmOUWsAWrfe1Q5UVIyoH402zdk=
github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=
github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
github.com/gorilla/websocket v1.5.1 h1:gmztn0JnHVt9JZquRuzLw3g4wouNVzKL15iLr/zn/QY=
github.com/gorilla/websocket v1.5.1/go.mod h1:x3kM2JMyaluk02fnUJpQuwD2dCS5NDG2ZHL0uE0tcaY=
github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0 h1:bkypFPDjIYGfCYD5mRBvpqxfYX1YCS1PXdKYWi8FsN0=
github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0/go.mod h1:P+Lt/0by1T8bfcF3z737NnSbmxQAppXMRziHUxPOC8k=
github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0+6sfsiTG8qcWGx4=
github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=
github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a h1:bbPeKD0xmW/Y25WS6cokEszi5g+S0QxI/d45PkRi7Nk=
github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
github.com/jackc/pgx/v5 v5.5.5 h1:amBjrZVmksIdNjxGW/IiIMzxMKZFelXbUoPNb+8sjQw=
github.com/jackc/pgx/v5 v5.5.5/go.mod h1:ez9gk+OAat140fv9ErkZDYFWmXLfV+++K0uAOiwgm1A=
github.com/jackc/puddle/v2 v2.2.1 h1:RhxXJtFG022u4ibrCSMSiu5aOq1i77R3OHKNJj77OAk=
github.com/jackc/puddle/v2 v2.2.1/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
github.com/jinzhu/inflection v1.0.0 h1:K317FqzuhWc8YvSVlFMCCUb36O/S9MCKRDI7QkRKD/E=
github.com/jinzhu/inflection v1.0.0/go.mod h1:h+uFLlag+Qp1Va5pdKtLDYj+kHp5pxUVkryuEj+Srlc=
github.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=
github.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=
github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=
github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=
github.com/kisielk/sqlstruct v0.0.0-20201105191214-5f3e10d3ab46/go.mod h1:yyMNCyc/Ib3bDTKd379tNMpB/7/H5TjM2Y9QJ5THLbE=
github.com/klauspost/compress v1.15.9/go.mod h1:PhcZ0MbTNciWF3rruxRgKxI5NkcHHrHUDtV4Yw2GlzU=
github.com/klauspost/compress v1.18.0 h1:c/Cqfb0r+Yi+JtIEq73FWXVkRonBlf0CRNYc8Zttxdo=
github.com/klauspost/compress v1.18.0/go.mod h1:2Pp+KzxcywXVXMr50+X0Q/Lsb43OQHYWRCY2AiWywWQ=
github.com/klauspost/cpuid/v2 v2.0.9/go.mod h1:FInQzS24/EEf25PyTYn52gqo7WaD8xa0213Md/qVLRg=
github.com/klauspost/cpuid/v2 v2.2.4 h1:acbojRNwl3o09bUq+yDCtZFc1aiwaAAxtcn8YkZXnvk=
github.com/klauspost/cpuid/v2 v2.2.4/go.mod h1:RVVoqg1df56z8g3pUjL/3lE5UfnlrJX8tyFgg4nqhuY=
github.com/klauspost/cpuid/v2 v2.3.0 h1:S4CRMLnYUhGeDFDqkGriYKdfoFlDnMtqTiI/sFzhA9Y=
github.com/klauspost/cpuid/v2 v2.3.0/go.mod h1:hqwkgyIinND0mEev00jJYCxPNVRVXFQeu1XKlok6oO0=
github.com/knz/go-libedit v1.10.1/go.mod h1:MZTVkCWyz0oBc7JOWP3wNAzd002ZbM/5hgShxwh4x8M=
github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/kylelemons/godebug v1.1.0 h1:RPNrshWIDI6G2gRW9EHilWtl7Z6Sb1BR0xunSBf0SNc=
github.com/kylelemons/godebug v1.1.0/go.mod h1:9/0rRGxNHcop5bhtWyNeEfOS8JIWk580+fNqagV/RAw=
github.com/leodido/go-urn v1.2.4 h1:XlAE/cm/ms7TE/VMVoduSpNBoyc2dOxHs5MZSwAN63Q=
github.com/leodido/go-urn v1.2.4/go.mod h1:7ZrI8mTSeBSHl/UaRyKQW1qZeMgak41ANeCNaVckg+4=
github.com/leodido/go-urn v1.4.0 h1:WT9HwE9SGECu3lg4d/dIA+jxlljEa1/ffXKmRjqdmIQ=
github.com/leodido/go-urn v1.4.0/go.mod h1:bvxc+MVxLKB4z00jd1z+Dvzr47oO32F/QSNjSBOlFxI=
github.com/magiconair/properties v1.8.7 h1:IeQXZAiQcpL9mgcAe1Nu6cX9LLw6ExEHKjN0VQdvPDY=
github.com/magiconair/properties v1.8.7/go.mod h1:Dhd985XPs7jluiymwWYZ0G4Z61jb3vdS329zhj2hYo0=
github.com/mailru/easyjson v0.9.0 h1:PrnmzHw7262yW8sTBwxi1PdJA3Iw/EKBa8psRf7d9a4=
github.com/mailru/easyjson v0.9.0/go.mod h1:1+xMtQp2MRNVL/V1bOzuP3aP8VNwRW55fQUto+XFtTU=
github.com/mattn/go-isatty v0.0.19 h1:JITubQf0MOLdlGRuRq+jtsDlekdYPia9ZFsB8h/APPA=
github.com/mattn/go-isatty v0.0.19/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mitchellh/mapstructure v1.5.0 h1:jeMsZIYE/09sWLaz43PL7Gy6RuMjD2eJVyuac5Z2hdY=
github.com/mitchellh/mapstructure v1.5.0/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=
github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=
github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
github.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=
github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=
github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=
github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
github.com/pelletier/go-toml/v2 v2.1.0 h1:FnwAJ4oYMvbT/34k9zzHuZNrhlz48GB3/s6at6/MHO4=
github.com/pelletier/go-toml/v2 v2.1.0/go.mod h1:tJU2Z3ZkXwnxa4DPO899bsyIoywizdUvyaeZurnPPDc=
github.com/pelletier/go-toml/v2 v2.2.4 h1:mye9XuhQ6gvn5h28+VilKrrPoQVanw5PMw/TB0t5Ec4=
github.com/pelletier/go-toml/v2 v2.2.4/go.mod h1:2gIqNv+qfxSVS7cM2xJQKtLSTLUE9V8t9Stt+h56mCY=
github.com/pgvector/pgvector-go v0.1.1 h1:kqJigGctFnlWvskUiYIvJRNwUtQl/aMSUZVs0YWQe+g=
github.com/pgvector/pgvector-go v0.1.1/go.mod h1:wLJgD/ODkdtd2LJK4l6evHXTuG+8PxymYAVomKHOWac=
github.com/pierrec/lz4/v4 v4.1.15/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=
github.com/pierrec/lz4/v4 v4.1.22 h1:cKFw6uJDK+/gfw5BcDL0JL5aBsAFdsIT18eRtLj7VIU=
github.com/pierrec/lz4/v4 v4.1.22/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 h1:Jamvg5psRIccs7FGNTlIRMkT8wgtp5eCXdBlqhYGL6U=
github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/prometheus/client_golang v1.22.0 h1:rb93p9lokFEsctTys46VnV1kLCDpVZ0a/Y92Vm0Zc6Q=
github.com/prometheus/client_golang v1.22.0/go.mod h1:R7ljNsLXhuQXYZYtw6GAE9AZg8Y7vEW5scdCXrWRXC0=
github.com/prometheus/client_model v0.6.1 h1:ZKSh/rekM+n3CeS952MLRAdFwIKqeY8b62p8ais2e9E=
github.com/prometheus/client_model v0.6.1/go.mod h1:OrxVMOVHjw3lKMa8+x6HeMGkHMQyHDk9E3jmP2AmGiY=
github.com/prometheus/common v0.62.0 h1:xasJaQlnWAeyHdUBeGjXmutelfJHWMRr+Fg4QszZ2Io=
github.com/prometheus/common v0.62.0/go.mod h1:vyBcEuLSvWos9B1+CyL7JZ2up+uFzXhkqml0W5zIY1I=
github.com/prometheus/procfs v0.15.1 h1:YagwOFzUgYfKKHX6Dr+sHT7km/hxC76UB0learggepc=
github.com/prometheus/procfs v0.15.1/go.mod h1:fB45yRUv8NstnjriLhBQLuOUt+WW4BsoGhij/e3PBqk=
github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
github.com/rs/cors v1.10.1 h1:L0uuZVXIKlI1SShY2nhFfo44TYvDPQ1w4oFkUJNfhyo=
github.com/rs/cors v1.10.1/go.mod h1:XyqrcTp5zjWr1wsJ8PIRZssZ8b/WMcMf71DJnit4EMU=
github.com/russross/blackfriday/v2 v2.1.0 h1:JIOH55/0cWyOuilr9/qlrm0BSXldqnqwMsf35Ld67mk=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/sagikazarmark/locafero v0.4.0 h1:HApY1R9zGo4DBgr7dqsTH/JJxLTTsOt7u6keLGt6kNQ=
github.com/sagikazarmark/locafero v0.4.0/go.mod h1:Pe1W6UlPYUk/+wc/6KFhbORCfqzgYEpgQ3O5fPuL3H4=
github.com/sagikazarmark/slog-shim v0.1.0 h1:diDBnUNK9N/354PgrxMywXnAwEr1QZcOr6gto+ugjYE=
github.com/sagikazarmark/slog-shim v0.1.0/go.mod h1:SrcSrq8aKtyuqEI1uvTDTK1arOWRIczQRv+GVI1AkeQ=
github.com/segmentio/kafka-go v0.4.47 h1:IqziR4pA3vrZq7YdRxaT3w1/5fvIH5qpCwstUanQQB0=
github.com/segmentio/kafka-go v0.4.47/go.mod h1:HjF6XbOKh0Pjlkr5GVZxt6CsjjwnmhVOfURM5KMd8qg=
github.com/shurcooL/sanitized_anchor_name v1.0.0 h1:PdmoCO6wvbs+7yrJyMORt4/BmY5IYyJwS/kOiWx8mHo=
github.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0+IJeTBvfc=
github.com/sony/gobreaker v1.0.0 h1:feX5fGGXSl3dYd4aHZItw+FpHLvvoaqkawKjVNiFMNQ=
github.com/sony/gobreaker v1.0.0/go.mod h1:ZKptC7FHNvhBz7dN2LGjPVBz2sZJmc0/PkyDJOjmxWY=
github.com/sourcegraph/conc v0.3.0 h1:OQTbbt6P72L20UqAkXXuLOj79LfEanQ+YQFNpLA9ySo=
github.com/sourcegraph/conc v0.3.0/go.mod h1:Sdozi7LEKbFPqYX2/J+iBAM6HpqSLTASQIKqDmF7Mt0=
github.com/spf13/afero v1.11.0 h1:WJQKhtpdm3v2IzqG8VMqrr6Rf3UYpEF239Jy9wNepM8=
github.com/spf13/afero v1.11.0/go.mod h1:GH9Y3pIexgf1MTIWtNGyogA5MwRIDXGUr+hbWNoBjkY=
github.com/spf13/cast v1.6.0 h1:GEiTHELF+vaR5dhz3VqZfFSzZjYbgeKDpBxQVS4GYJ0=
github.com/spf13/cast v1.6.0/go.mod h1:ancEpBxwJDODSW/UG4rDrAqiKolqNNh2DX3mk86cAdo=
github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=
github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/viper v1.18.2 h1:LUXCnvUvSM6FXAsj6nnfc8Q2tp1dIgUfY9Kc8GsSOiQ=
github.com/spf13/viper v1.18.2/go.mod h1:EKmWIqdnk5lOcmR72yw6hS+8OPYcwD0jteitLMVB+yk=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=
github.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=
github.com/stretchr/objx v0.5.2 h1:xuMeJ0Sdp5ZMRXx/aWO6RZxdr3beISkG5/G/aIRr3pY=
github.com/stretchr/objx v0.5.2/go.mod h1:FRsXN1f5AsAjCGJKqEizvkpNtU+EGNCLh3NxZ/8L+MA=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=
github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
github.com/stretchr/testify v1.8.2/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
github.com/stretchr/testify v1.8.4/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=
github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=
github.com/swaggo/files v1.0.1 h1:J1bVJ4XHZNq0I46UU90611i9/YzdrF7x92oX1ig5IdE=
github.com/swaggo/files v1.0.1/go.mod h1:0qXmMNH6sXNf+73t65aKeB+ApmgxdnkQzVTAj2uaMUg=
github.com/swaggo/gin-swagger v1.6.0 h1:y8sxvQ3E20/RCyrXeFfg60r6H0Z+SwpTjMYsMm+zy8M=
github.com/swaggo/gin-swagger v1.6.0/go.mod h1:BG00cCEy294xtVpyIAHG6+e2Qzj/xKlRdOqDkvq0uzo=
github.com/swaggo/swag v1.16.5 h1:nMf2fEV1TetMTJb4XzD0Lz7jFfKJmJKGTygEey8NSxM=
github.com/swaggo/swag v1.16.5/go.mod h1:ngP2etMK5a0P3QBizic5MEwpRmluJZPHjXcMoj4Xesg=
github.com/tmthrgd/go-hex v0.0.0-20190904060850-447a3041c3bc h1:9lRDQMhESg+zvGYmW5DyG0UqvY96Bu5QYsTLvCHdrgo=
github.com/tmthrgd/go-hex v0.0.0-20190904060850-447a3041c3bc/go.mod h1:bciPuU6GHm1iF1pBvUfxfsH0Wmnc2VbpgvbI9ZWuIRs=
github.com/twitchyliquid64/golang-asm v0.15.1 h1:SU5vSMR7hnwNxj24w34ZyCi/FmDZTkS4MhqMhdFk5YI=
github.com/twitchyliquid64/golang-asm v0.15.1/go.mod h1:a1lVb/DtPvCB8fslRZhAngC2+aY1QWCk3Cedj/Gdt08=
github.com/ugorji/go/codec v1.2.11 h1:BMaWp1Bb6fHwEtbplGBGJ498wD+LKlNSl25MjdZY4dU=
github.com/ugorji/go/codec v1.2.11/go.mod h1:UNopzCgEMSXjBc6AOMqYvWC1ktqTAfzJZUZgYf6w6lg=
github.com/ugorji/go/codec v1.3.0 h1:Qd2W2sQawAfG8XSvzwhBeoGq71zXOC/Q1E9y/wUcsUA=
github.com/ugorji/go/codec v1.3.0/go.mod h1:pRBVtBSKl77K30Bv8R2P+cLSGaTtex6fsA2Wjqmfxj4=
github.com/uptrace/bun v1.1.12 h1:sOjDVHxNTuM6dNGaba0wUuz7KvDE1BmNu9Gqs2gJSXQ=
github.com/uptrace/bun v1.1.12/go.mod h1:NPG6JGULBeQ9IU6yHp7YGELRa5Agmd7ATZdz4tGZ6z0=
github.com/uptrace/bun/dialect/pgdialect v1.1.12 h1:m/CM1UfOkoBTglGO5CUTKnIKKOApOYxkcP2qn0F9tJk=
github.com/uptrace/bun/dialect/pgdialect v1.1.12/go.mod h1:Ij6WIxQILxLlL2frUBxUBOZJtLElD2QQNDcu/PWDHTc=
github.com/uptrace/bun/driver/pgdriver v1.1.12 h1:3rRWB1GK0psTJrHwxzNfEij2MLibggiLdTqjTtfHc1w=
github.com/uptrace/bun/driver/pgdriver v1.1.12/go.mod h1:ssYUP+qwSEgeDDS1xm2XBip9el1y9Mi5mTAvLoiADLM=
github.com/urfave/cli/v2 v2.27.7 h1:bH59vdhbjLv3LAvIu6gd0usJHgoTTPhCFib8qqOwXYU=
github.com/urfave/cli/v2 v2.27.7/go.mod h1:CyNAG/xg+iAOg0N4MPGZqVmv2rCoP267496AOXUZjA4=
github.com/vmihailenco/bufpool v0.1.11 h1:gOq2WmBrq0i2yW5QJ16ykccQ4wH9UyEsgLm6czKAd94=
github.com/vmihailenco/bufpool v0.1.11/go.mod h1:AFf/MOy3l2CFTKbxwt0mp2MwnqjNEs5H/UxrkA5jxTQ=
github.com/vmihailenco/msgpack/v5 v5.3.5 h1:5gO0H1iULLWGhs2H5tbAHIZTV8/cYafcFOr9znI5mJU=
github.com/vmihailenco/msgpack/v5 v5.3.5/go.mod h1:7xyJ9e+0+9SaZT0Wt1RGleJXzli6Q/V5KbhBonMG9jc=
github.com/vmihailenco/tagparser v0.1.2 h1:gnjoVuB/kljJ5wICEEOpx98oXMWPLj22G67Vbd1qPqc=
github.com/vmihailenco/tagparser v0.1.2/go.mod h1:OeAg3pn3UbLjkWt+rN9oFYB6u/cQgqMEUPoW2WPyhdI=
github.com/vmihailenco/tagparser/v2 v2.0.0 h1:y09buUbR+b5aycVFQs/g70pqKVZNBmxwAhO7/IwNM9g=
github.com/vmihailenco/tagparser/v2 v2.0.0/go.mod h1:Wri+At7QHww0WTrCBeu4J6bNtoV6mEfg5OIWRZA9qds=
github.com/xdg-go/pbkdf2 v1.0.0 h1:Su7DPu48wXMwC3bs7MCNG+z4FhcyEuz5dlvchbq0B0c=
github.com/xdg-go/pbkdf2 v1.0.0/go.mod h1:jrpuAogTd400dnrH08LKmI/xc1MbPOebTwRqcT5RDeI=
github.com/xdg-go/scram v1.1.2 h1:FHX5I5B4i4hKRVRBCFRxq1iQRej7WO3hhBuJf+UUySY=
github.com/xdg-go/scram v1.1.2/go.mod h1:RT/sEzTbU5y00aCK8UOx6R7YryM0iF1N2MOmC3kKLN4=
github.com/xdg-go/stringprep v1.0.4 h1:XLI/Ng3O1Atzq0oBs3TWm+5ZVgkq2aqdlvP9JtoZ6c8=
github.com/xdg-go/stringprep v1.0.4/go.mod h1:mPGuuIYwz7CmR2bT9j4GbQqutWS1zV24gijq1dTyGkM=
github.com/xrash/smetrics v0.0.0-20250705151800-55b8f293f342 h1:FnBeRrxr7OU4VvAzt5X7s6266i6cSVkkFPS0TuXWbIg=
github.com/xrash/smetrics v0.0.0-20250705151800-55b8f293f342/go.mod h1:Ohn+xnUBiLI6FVj/9LpzZWtj1/D6lUovWYBkxHVV3aM=
github.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=
go.opentelemetry.io/otel v1.29.0 h1:PdomN/Al4q/lN6iBJEN3AwPvUiHPMlt93c8bqTG5Llw=
go.opentelemetry.io/otel v1.29.0/go.mod h1:N/WtXPs1CNCUEx+Agz5uouwCba+i+bJGFicT8SR4NP8=
go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.29.0 h1:dIIDULZJpgdiHz5tXrTgKIMLkus6jEFa7x5SOKcyR7E=
go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.29.0/go.mod h1:jlRVBe7+Z1wyxFSUs48L6OBQZ5JwH2Hg/Vbl+t9rAgI=
go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.24.0 h1:Mw5xcxMwlqoJd97vwPxA8isEaIoxsta9/Q51+TTJLGE=
go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.24.0/go.mod h1:CQNu9bj7o7mC6U7+CA/schKEYakYXWr79ucDHTMGhCM=
go.opentelemetry.io/otel/metric v1.29.0 h1:vPf/HFWTNkPu1aYeIsc98l4ktOQaL6LeSoeV2g+8YLc=
go.opentelemetry.io/otel/metric v1.29.0/go.mod h1:auu/QWieFVWx+DmQOUMgj0F8LHWdgalxXqvp7BII/W8=
go.opentelemetry.io/otel/sdk v1.29.0 h1:vkqKjk7gwhS8VaWb0POZKmIEDimRCMsopNYnriHyryo=
go.opentelemetry.io/otel/sdk v1.29.0/go.mod h1:pM8Dx5WKnvxLCb+8lG1PRNIDxu9g9b9g59Qr7hfAAok=
go.opentelemetry.io/otel/trace v1.29.0 h1:J/8ZNK4XgR7a21DZUAsbF8pZ5Jcw1VhACmnYt39JTi4=
go.opentelemetry.io/otel/trace v1.29.0/go.mod h1:eHl3w0sp3paPkYstJOmAimxhiFXPg+MMTlEh3nsQgWQ=
go.opentelemetry.io/proto/otlp v1.3.1 h1:TrMUixzpM0yuc/znrFTP9MMRh8trP93mkCiDVeXrui0=
go.opentelemetry.io/proto/otlp v1.3.1/go.mod h1:0X1WI4de4ZsLrrJNLAQbFeLCm3T7yBkR0XqQ7niQU+8=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
go.uber.org/multierr v1.10.0 h1:S0h4aNzvfcFsC3dRF1jLoaov7oRaKqRGC/pUEJ2yvPQ=
go.uber.org/multierr v1.10.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
go.uber.org/zap v1.27.0 h1:aJMhYGrd5QSmlpLMr2MftRKl7t8J8PTZPA732ud/XR8=
go.uber.org/zap v1.27.0/go.mod h1:GB2qFLM7cTU87MWRP2mPIjqfIDnGu+VIO4V/SdhGo2E=
go.yaml.in/yaml/v2 v2.4.2 h1:DzmwEr2rDGHl7lsFgAHxmNz/1NlQ7xLIrlN2h5d1eGI=
go.yaml.in/yaml/v2 v2.4.2/go.mod h1:081UH+NErpNdqlCXm3TtEran0rJZGxAYx9hb/ELlsPU=
golang.org/x/arch v0.0.0-20210923205945-b76863e36670/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=
golang.org/x/arch v0.3.0 h1:02VY4/ZcO/gBOH6PUaoiptASxtXU10jazRCP865E97k=
golang.org/x/arch v0.3.0/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=
golang.org/x/arch v0.19.0 h1:LmbDQUodHThXE+htjrnmVD73M//D9GTH6wFZjyDkjyU=
golang.org/x/arch v0.19.0/go.mod h1:bdwinDaKcfZUGpH09BB7ZmOfhalA8lQdzl62l8gGWsk=
golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
golang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=
golang.org/x/crypto v0.14.0/go.mod h1:MVFd36DqK4CsrnJYDkBA3VC4m2GkXAM0PvzMCn4JQf4=
golang.org/x/crypto v0.38.0 h1:jt+WWG8IZlBnVbomuhg2Mdq0+BBQaHbtqHEFEigjUV8=
golang.org/x/crypto v0.38.0/go.mod h1:MvrbAqul58NNYPKnOra203SB9vpuZW0e+RRZV+Ggqjw=
golang.org/x/crypto v0.40.0 h1:r4x+VvoG5Fm+eJcxMaY8CQM7Lb0l1lsmjGBQ6s8BfKM=
golang.org/x/crypto v0.40.0/go.mod h1:Qr1vMER5WyS2dfPHAlsOj01wgLbsyWtFn/aY+5+ZdxY=
golang.org/x/exp v0.0.0-20230905200255-921286631fa9 h1:GoHiUyI/Tp2nVkLI2mCxVkOjsbSXD66ic0XW0js0R9g=
golang.org/x/exp v0.0.0-20230905200255-921286631fa9/go.mod h1:S2oDrQGGwySpoQPVqRShND87VCbxmc6bL1Yd2oYrm6k=
golang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=
golang.org/x/mod v0.8.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=
golang.org/x/mod v0.26.0 h1:EGMPT//Ezu+ylkCijjPc+f4Aih7sZvaAr+O3EHBxvZg=
golang.org/x/mod v0.26.0/go.mod h1:/j6NAhSk8iQ723BGAUyoAcn7SlD7s15Dp9Nd/SfeaFQ=
golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=
golang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=
golang.org/x/net v0.6.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=
golang.org/x/net v0.7.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=
golang.org/x/net v0.10.0/go.mod h1:0qNGK6F8kojg2nk9dLZ2mShWaEBan6FAoqfSigmmuDg=
golang.org/x/net v0.17.0/go.mod h1:NxSsAGuq816PNPmqtQdLE42eU2Fs7NoRIZrHJAlaCOE=
golang.org/x/net v0.40.0 h1:79Xs7wF06Gbdcg4kdCCIQArK11Z1hr5POQ6+fIYHNuY=
golang.org/x/net v0.40.0/go.mod h1:y0hY0exeL2Pku80/zKK7tpntoX23cqL3Oa6njdgRtds=
golang.org/x/net v0.42.0 h1:jzkYrhi3YQWD6MLBJcsklgQsoAcw89EcZbJw8Z614hs=
golang.org/x/net v0.42.0/go.mod h1:FF1RA5d3u7nAYA4z2TkclSCKh68eSXtiFwcWQpPXdt8=
golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.14.0 h1:woo0S4Yywslg6hp4eUFjTVOyKt0RookbpAHG4c1HmhQ=
golang.org/x/sync v0.14.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/sync v0.16.0 h1:ycBJEhp9p4vXvUZNszeOq0kGTPghopOL8q0fq3vstxw=
golang.org/x/sync v0.16.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220704084225-05e143d24a9e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.13.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.33.0 h1:q3i8TbbEz+JRD9ywIRlyRAQbM0qF7hu24q3teo2hbuw=
golang.org/x/sys v0.33.0/go.mod h1:BJP2sWEmIv4KK5OTEluFJCKSidICx8ciO85XgH3Ak8k=
golang.org/x/sys v0.34.0 h1:H5Y5sJ2L2JRdyv7ROF1he/lPdvFsd0mJHFw2ThKHxLA=
golang.org/x/sys v0.34.0/go.mod h1:BJP2sWEmIv4KK5OTEluFJCKSidICx8ciO85XgH3Ak8k=
golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=
golang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=
golang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=
golang.org/x/term v0.8.0/go.mod h1:xPskH00ivmX89bAKVGSKKtLOWNx2+17Eiy94tnKShWo=
golang.org/x/term v0.13.0/go.mod h1:LTmsnFJwVN6bCy1rVCoS+qHT1HhALEFxKncY3WNNh4U=
golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=
golang.org/x/text v0.3.8/go.mod h1:E6s5w1FMmriuDzIBO73fBruAKo1PCIq6d2Q6DHfQ8WQ=
golang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=
golang.org/x/text v0.9.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=
golang.org/x/text v0.13.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=
golang.org/x/text v0.25.0 h1:qVyWApTSYLk/drJRO5mDlNYskwQznZmkpV2c8q9zls4=
golang.org/x/text v0.25.0/go.mod h1:WEdwpYrmk1qmdHvhkSTNPm3app7v4rsT8F2UD6+VHIA=
golang.org/x/text v0.27.0 h1:4fGWRpyh641NLlecmyl4LOe6yDdfaYNrGb2zdfo4JV4=
golang.org/x/text v0.27.0/go.mod h1:1D28KMCvyooCX9hBiosv5Tz/+YLxj0j7XhWjpSUF7CU=
golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
golang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=
golang.org/x/tools v0.6.0/go.mod h1:Xwgl3UAJ/d3gWutnCtw505GrjyAbvKui8lOU390QaIU=
golang.org/x/tools v0.35.0 h1:mBffYraMEf7aa0sB+NuKnuCy8qI/9Bughn8dC2Gu5r0=
golang.org/x/tools v0.35.0/go.mod h1:NKdj5HkL/73byiZSJjqJgKn3ep7KjFkBOkR/Hps3VPw=
golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
google.golang.org/genproto/googleapis/api v0.0.0-20240513163218-0867130af1f8 h1:W5Xj/70xIA4x60O/IFyXivR5MGqblAb8R3w26pnD6No=
google.golang.org/genproto/googleapis/api v0.0.0-20240513163218-0867130af1f8/go.mod h1:vPrPUTsDCYxXWjP7clS81mZ6/803D8K4iM9Ma27VKas=
google.golang.org/genproto/googleapis/rpc v0.0.0-20240513163218-0867130af1f8 h1:mxSlqyb8ZAHsYDCfiXN1EDdNTdvjUJSLY+OnAUtYNYA=
google.golang.org/genproto/googleapis/rpc v0.0.0-20240513163218-0867130af1f8/go.mod h1:I7Y+G38R2bu5j1aLzfFmQfTcU/WnFuqDwLZAbvKTKpM=
google.golang.org/grpc v1.64.1 h1:LKtvyfbX3UGVPFcGqJ9ItpVWW6oN/2XqTxfAnwRRXiA=
google.golang.org/grpc v1.64.1/go.mod h1:hiQF4LFZelK2WKaP6W0L92zGHtiQdZxk8CrSdvyjeP0=
google.golang.org/protobuf v1.36.5 h1:tPhr+woSbjfYvY6/GPufUoYizxw1cF/yFoxJ2fmpwlM=
google.golang.org/protobuf v1.36.5/go.mod h1:9fA7Ob0pmnwhb644+1+CVWFRbNajQ6iRojtC/QF5bRE=
google.golang.org/protobuf v1.36.6 h1:z1NpPI8ku2WgiWnf+t9wTPsn6eP1L7ksHUlkfLvd9xY=
google.golang.org/protobuf v1.36.6/go.mod h1:jduwjTPXsFjZGTmRluh+L6NjiWu7pchiJ2/5YcXBHnY=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/ini.v1 v1.67.0 h1:Dgnx+6+nfE+IfzjUEISNeydPJh9AXNNsWbGP9KzCsOA=
gopkg.in/ini.v1 v1.67.0/go.mod h1:pNLf8WUiyNEtQjuu5G5vTm06TEv9tsIgeAvK8hOrP4k=
gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
mellium.im/sasl v0.3.1 h1:wE0LW6g7U83vhvxjC1IY8DnXM+EU095yeo8XClvCdfo=
mellium.im/sasl v0.3.1/go.mod h1:xm59PUYpZHhgQ9ZqoJ5QaCqzWMi8IeS49dhp6plPCzw=
nullprogram.com/x/optparse v1.0.0/go.mod h1:KdyPE+Igbe0jQUrVfMqDMeJQIJZEuyV7pjYmp6pbG50=
rsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4=
sigs.k8s.io/yaml v1.5.0 h1:M10b2U7aEUY6hRtU870n2VTPgR5RZiL/I6Lcc2F4NUQ=
sigs.k8s.io/yaml v1.5.0/go.mod h1:wZs27Rbxoai4C0f8/9urLZtZtF3avA3gKvGyPdDqTO4=
-------------------------------------------------
