filepath = ./deployments/terraform/modules/postgres-instance/variables.tf
variable "instance_name" {
  description = "The unique name for the PostgreSQL StatefulSet and related resources."
  type        = string
}

variable "namespace" {
  description = "The Kubernetes namespace to deploy the resources into."
  type        = string
}

variable "database_name" {
  description = "The name of the database to create."
  type        = string
}

variable "database_user" {
  description = "The username for the database."
  type        = string
}

variable "database_pass" {
  description = "The password for the database user."
  type        = string
  sensitive   = true
}

variable "storage_class_name" {
  description = "The name of the StorageClass to use for the PersistentVolumeClaim."
  type        = string
}

variable "storage_size" {
  description = "The size of the persistent volume (e.g., '10Gi')."
  type        = string
}-------------------------------------------------
filepath = ./deployments/terraform/modules/postgres-instance/main.tf
terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.20"
    }
  }
}

resource "kubernetes_secret" "postgres_secret" {
  metadata {
    name      = "${var.instance_name}-secret"
    namespace = var.namespace
    labels = {
      app = var.instance_name
    }
  }
  data = {
    "POSTGRES_USER"     = var.database_user
    "POSTGRES_PASSWORD" = var.database_pass
    "POSTGRES_DB"       = var.database_name
  }
  type = "Opaque"
}

resource "kubernetes_stateful_set" "postgres_sts" {
  metadata {
    name      = var.instance_name
    namespace = var.namespace
  }
  spec {
    service_name = "${var.instance_name}-headless"
    replicas     = 1

    selector {
      match_labels = {
        app = var.instance_name
      }
    }

    template {
      metadata {
        labels = {
          app = var.instance_name
        }
      }
      spec {
        termination_grace_period_seconds = 10
        containers {
          name  = "postgres"
          image = "postgres:15-alpine"
          ports {
            container_port = 5432
            name           = "postgres"
          }
          env_from {
            secret_ref {
              name = kubernetes_secret.postgres_secret.metadata[0].name
            }
          }
          volume_mounts {
            name       = "postgres-storage"
            mount_path = "/var/lib/postgresql/data"
          }
          liveness_probe {
            exec {
              command = ["pg_isready", "-U", var.database_user, "-d", var.database_name]
            }
            initial_delay_seconds = 30
            period_seconds        = 10
          }
          readiness_probe {
            exec {
              command = ["pg_isready", "-U", var.database_user, "-d", var.database_name]
            }
            initial_delay_seconds = 5
            period_seconds        = 5
          }
        }
      }
    }
    volume_claim_template {
      metadata {
        name = "postgres-storage"
      }
      spec {
        access_modes       = ["ReadWriteOnce"]
        storage_class_name = var.storage_class_name
        resources {
          requests = {
            storage = var.storage_size
          }
        }
      }
    }
  }
  depends_on = [kubernetes_secret.postgres_secret]
}

resource "kubernetes_service" "postgres_service" {
  metadata {
    name      = var.instance_name
    namespace = var.namespace
  }
  spec {
    selector = {
      app = var.instance_name
    }
    ports {
      port        = 5432
      target_port = 5432
    }
    type = "ClusterIP"
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/postgres-instance/outputs.tf
output "service_name" {
  description = "The name of the PostgreSQL Kubernetes service."
  value       = kubernetes_service.postgres_service.metadata[0].name
}

output "service_endpoint" {
  description = "The internal DNS endpoint for the service."
  value       = "${kubernetes_service.postgres_service.metadata[0].name}.${var.namespace}.svc.cluster.local"
}

output "secret_name" {
  description = "The name of the secret containing the database credentials."
  value       = kubernetes_secret.postgres_secret.metadata[0].name
}-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/variables.tf
variable "instance_name" {
  description = "A logical name for this database instance (used for naming the secret)."
  type        = string
}

variable "namespace" {
  description = "The Kubernetes namespace to create the secret in."
  type        = string
}

variable "db_host" {
  description = "The hostname or IP address of the external MySQL database."
  type        = string
}

variable "db_port" {
  description = "The port number of the external MySQL database."
  type        = string
  default     = "3306"
}

variable "database_name" {
  description = "The name of the database to connect to."
  type        = string
}

variable "database_user" {
  description = "The username for the external database."
  type        = string
}

variable "database_pass" {
  description = "The password for the external database user."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/main.tf
terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.20"
    }
  }
}

resource "kubernetes_secret" "external_mysql_secret" {
  metadata {
    name      = "${var.instance_name}-secret"
    namespace = var.namespace
    labels = {
      app  = var.instance_name
      type = "external-db"
    }
  }

  # Note: The keys here (e.g., DB_HOST) must match what your application expects
  # to read from the environment.
  data = {
    "DB_HOST"     = var.db_host
    "DB_PORT"     = var.db_port
    "DB_USER"     = var.database_user
    "DB_PASSWORD" = var.database_pass
    "DB_NAME"     = var.database_name
  }

  type = "Opaque"
}-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/outputs.tf
output "secret_name" {
  description = "The name of the secret containing the external database credentials."
  value       = kubernetes_secret.external_mysql_secret.metadata[0].name
}-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/variables.tf
variable "bucket_names" {
  description = "A list of bucket names to create in Backblaze B2."
  type        = list(string)
  default     = []
}

variable "tags" {
  description = "A map of tags to assign to the resources."
  type        = map(string)
  default     = {}
}-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/main.tf
# main.tf for the s3-buckets module

terraform {
  required_providers {
    b2 = {
      source  = "Backblaze/b2"
      version = "~> 0.6" # Use a recent version of the Backblaze provider
    }
  }
}

# Create a Backblaze B2 bucket for each name in the list
resource "b2_bucket" "storage_buckets" {
  for_each = toset(var.bucket_names)

  bucket_name = each.key
  bucket_type = "allPrivate" # Default to private, can be overridden

  lifecycle {
    prevent_destroy = false # Set to true in production for safety
  }

  cors_rules {
    cors_rule_name = "allowAll"
    allowed_origins = ["*"]
    allowed_operations = [
      "s3_delete",
      "s3_get",
      "s3_head",
      "s3_post",
      "s3_put",
    ]
    allowed_headers = ["*"]
    expose_headers = ["x-bz-content-sha1"]
    max_age_seconds = 3600
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/outputs.tf
output "bucket_ids" {
  description = "A map of bucket names to their Backblaze B2 IDs."
  value = {
    for bucket in b2_bucket.storage_buckets : bucket.bucket_name => bucket.bucket_id
  }
}

output "bucket_names" {
  description = "A list of the names of the created buckets."
  value = [for bucket in b2_bucket.storage_buckets : bucket.bucket_name]
}

variable "b2_application_key_id" {
  description = "The application key ID for Backblaze B2."
  type        = string
  sensitive   = true
}

variable "b2_application_key" {
  description = "The application key for Backblaze B2."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/variables.tf
variable "kustomize_path" {
  description = "The path to the Kustomize overlay to apply."
  type        = string
}

variable "service_name" {
  description = "The name of the Kubernetes deployment resource."
  type        = string
}

variable "namespace" {
  description = "The Kubernetes namespace to deploy into."
  type        = string
}

variable "image_tag" {
  description = "The Docker image tag to apply to the deployment."
  type        = string
  default     = "latest"
}

variable "image_repository" {
  description = "The Docker image repository (e.g., 'aqls/personae-auth-service')."
  type        = string
}

variable "config_sha" {
  description = "A hash of the service's config file to trigger updates."
  type        = string
  default     = ""
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/main.tf
resource "null_resource" "apply_kustomization" {
  triggers = {
    image_tag_trigger = var.image_tag
    config_sha_trigger = var.config_sha
  }

  provisioner "local-exec" {
    command = <<-EOT
      set -e
      echo "Applying Kustomize overlay at ${var.kustomize_path}"
      kubectl apply -k ${var.kustomize_path}

      echo "Setting image for deployment/${var.service_name} to ${var.image_repository}:${var.image_tag}"
      kubectl set image deployment/${var.service_name} ${var.service_name}=${var.image_repository}:${var.image_tag} -n ${var.namespace}

      echo "Waiting for rollout of deployment/${var.service_name}..."
      kubectl rollout status deployment/${var.service_name} -n ${var.namespace} --timeout=5m
    EOT
  }
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/terraform.tfvars
chart_version = "x.y.z"-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/variables.tf
# terraform/modules/nginx_ingress/variables.tf

variable "ingress_namespace" {
  description = "Namespace to deploy the NGINX Ingress controller into."
  type        = string
  default     = "ingress-nginx"
}

variable "helm_chart_version" {
  description = "Version of the ingress-nginx Helm chart to deploy."
  type        = string
  default     = "4.10.1" # Example, use a known good/recent version
}

variable "helm_values_content" {
  description = "YAML content string for Helm values. Pass using file() function from root module."
  type        = string
  default     = "" # Empty by default, meaning chart defaults unless provided
}

variable "create_namespace" {
  description = "Whether the module should create the namespace for the ingress controller."
  type        = bool
  default     = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/providers.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/main.tf
# terraform/modules/nginx_ingress/main.tf

resource "kubernetes_namespace" "ns" {
  count = var.create_namespace ? 1 : 0 # Create namespace only if variable is true
  metadata {
    name = var.ingress_namespace
    labels = {
      name = var.ingress_namespace
    }
  }
}

resource "helm_release" "ingress_nginx" {
  name       = "ingress-nginx"
  repository = "https://kubernetes.github.io/ingress-nginx"
  chart      = "ingress-nginx"
  namespace  = var.create_namespace ? kubernetes_namespace.ns[0].metadata[0].name : var.ingress_namespace
  version    = var.helm_chart_version

  values = var.helm_values_content != "" ? [var.helm_values_content] : []

  # Common overrides if not in values file, especially LoadBalancer type
  set {
    name  = "controller.service.type"
    value = "NodePort"
  }
/*  set {
    name  = "controller.replicaCount"
    value = "2" # Default to 2 replicas
  }*/

  depends_on = [kubernetes_namespace.ns] # Depends on namespace if created by module
}

data "kubernetes_service" "ingress_controller_service" {
  # This data source might fail if the service is not immediately available.
  # Consider making it optional or using a more robust way to get the IP if needed for immediate output.
  # For now, it assumes the service name convention from the chart.
  # The actual service name might vary based on the release name and chart.
  # Usually it's <release-name>-controller, so "ingress-nginx-controller".
  metadata {
    name      = "${helm_release.ingress_nginx.name}-controller"
    namespace = var.create_namespace ? kubernetes_namespace.ns[0].metadata[0].name : var.ingress_namespace
  }
  depends_on = [helm_release.ingress_nginx]
}-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/config/ingress-nginx-values.yaml
# terraform/modules/nginx_ingress/config/ingress-nginx-values.yaml

controller:
  # Configure the kind of Deployment/DaemonSet to have it across all nodes
  kind: DaemonSet
  # replicaCount: 2

  # Set up node affinity to use spot instances if available
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
              - key: "role"
                operator: In
                values:
                  - "spot-instance"

  # Set up resource limits
  resources:
    requests:
      cpu: 150m
      memory: 192Mi
    limits:
      cpu: 500m
      memory: 512Mi

  # Allow for graceful termination
  terminationGracePeriodSeconds: 300

  # Configure metrics for Prometheus
  metrics:
    enabled: true
    service:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "10254"

  # Configure the service
  service:
    type: NodePort
    externalTrafficPolicy: Local
    nodePorts:
      http: 30080
      https: 30443

  # Configure admission webhooks
  admissionWebhooks:
    enabled: true
    failurePolicy: Fail
    timeoutSeconds: 10

  # Configure configmaps
  config:
    # Enable gzip compression
    use-gzip: "true"
    gzip-types: "application/json application/javascript text/css text/javascript text/plain text/xml application/xml+rss"
    # Configure log format
    log-format-upstream: '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" $request_length $request_time [$proxy_upstream_name] [$proxy_alternative_upstream_name] $upstream_addr $upstream_response_length $upstream_response_time $upstream_status $req_id'
    # Configure rate limiting
    limit-req-status-code: "429"
    # Configure proxy timeout
    proxy-read-timeout: "180"
    proxy-send-timeout: "180"
    proxy-body-size: "50m"
    proxy-next-upstream-timeout: "20"
    client-header-timeout: "60"
    client-body-timeout: "60"
    http-snippet: |
      map $http_upgrade $connection_upgrade {
        default upgrade;
        '' close;
      }
    # Configure SSL
    ssl-protocols: "TLSv1.2 TLSv1.3"
    ssl-ciphers: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384"
    # Configure HSTS
    hsts: "true"
    hsts-include-subdomains: "true"
    hsts-max-age: "31536000"

# Default backend for 404 pages
defaultBackend:
  enabled: true
  image:
    registry: k8s.gcr.io
    image: defaultbackend-amd64
    tag: "1.5"
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 10m
      memory: 20Mi
  service:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/outputs.tf
# terraform/modules/nginx_ingress/outputs.tf
# ~/projects/terraform/rackspace_generic/terraform/modules/nginx_ingress/outputs.tf

output "namespace" {
  description = "Namespace where the ingress controller is deployed."
  value       = var.create_namespace ? kubernetes_namespace.ns[0].metadata[0].name : var.ingress_namespace
}

output "release_name" {
  description = "Helm release name for the ingress controller."
  value       = helm_release.ingress_nginx.name
}

output "loadbalancer_ip" {
  description = "External IP or Hostname of the NGINX Ingress controller LoadBalancer."
  value = try(
    # Attempt to get IP from the first ingress of the first load_balancer
    data.kubernetes_service.ingress_controller_service.status[0].load_balancer[0].ingress[0].ip,
    # Fallback to hostname if IP isn't present
    data.kubernetes_service.ingress_controller_service.status[0].load_balancer[0].ingress[0].hostname,
    "IP/Hostname pending or not a LoadBalancer" # Generic fallback
  )
}

# For debugging the structure, you can add:
output "debug_ingress_load_balancer_status_block" {
  description = "The raw load_balancer status block for debugging."
  value       = try(data.kubernetes_service.ingress_controller_service.status[0].load_balancer, null)
}-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/versions.tf
# terraform/modules/nginx_ingress/versions.tf

terraform {
  required_version = ">= 1.0"
  required_providers {
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.17.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.36.0"
    }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/variables.tf
variable "operator_namespace" {
  description = "Namespace to deploy the Strimzi Kafka operator into."
  type        = string
}

variable "watched_namespaces_list" {
  description = "List of namespaces for the Strimzi operator to watch."
  type        = list(string)
}

variable "strimzi_yaml_source_path" {
  description = "Path to the directory containing the Strimzi YAML files to apply (e.g., ./strimzi-yaml-0.45.0/install/cluster-operator/)."
  type        = string
}

variable "operator_deployment_yaml_filename" {
  description = "Filename of the main operator deployment YAML within the strimzi_yaml_source_path (used for trigger)."
  type        = string
  default     = "060-Deployment-strimzi-cluster-operator.yaml"
}

variable "cluster_kubeconfig_path" {
  description = "Path to the kubeconfig file for the target Kubernetes cluster."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/048-Crd-kafkamirrormaker2.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/023-amended-ClusterRole-strimzi-cluster-operator-role.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/045-Crd-kafkamirrormaker.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/042-Crd-strimzipodset.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/047-Crd-kafkaconnector.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/020-ClusterRole-strimzi-cluster-operator-role.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/049-Crd-kafkarebalance.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/021-ClusterRole-strimzi-cluster-operator-role.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/023-RoleBinding-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/031-ClusterRole-strimzi-entity-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/030-ClusterRole-strimzi-kafka-broker.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/022-ClusterRole-strimzi-cluster-operator-role.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/022-RoleBinding-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/041-Crd-kafkaconnect.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/046-Crd-kafkabridge.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/010-ServiceAccount-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/043-Crd-kafkatopic.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/020-RoleBinding-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/04A-Crd-kafkanodepool.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/021-ClusterRoleBinding-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/050-ConfigMap-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/044-Crd-kafkauser.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/033-ClusterRole-strimzi-kafka-client.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/040-Crd-kafka.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/060-Deployment-strimzi-cluster-operator.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/added-clusterrolebinding-operator-watched.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/strimzi-yaml-0.45.0/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
[File listed only - content not included]
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/providers.tf
/*
provider "kubernetes" {
  config_path = "~/.kube/config_production_sydney"
}

provider "helm" {
  kubernetes {
    config_path = "~/.kube/config_production_sydney"
  }
}*/-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/main.tf
# This module assumes the namespaces (operator_namespace and watched_namespaces_list)
# are created by a separate configuration or exist.
# The main.tf in the instance directory (e.g., 030-strimzi-operator) will create them.

resource "null_resource" "apply_strimzi_operator_yaml" {
  triggers = {
    operator_deployment_sha1 = fileexists("${var.strimzi_yaml_source_path}/${var.operator_deployment_yaml_filename}") ? filesha1("${var.strimzi_yaml_source_path}/${var.operator_deployment_yaml_filename}") : ""
    watched_namespaces_trigger = join(",", var.watched_namespaces_list)
  }

  provisioner "local-exec" {
    command = "kubectl apply --namespace ${var.operator_namespace} --filename ${var.strimzi_yaml_source_path}/"
    environment = {
      KUBECONFIG = var.cluster_kubeconfig_path
    }
  }
}

-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/README.md
The file [060-Deployment-strimzi-cluster-operator.yaml](strimzi-yaml-0.45.0/060-Deployment-strimzi-cluster-operator.yaml)
was altered to add the namespaces that we want strimzi kafka to watch
s/b value: "kafka,personae,strimzi"
(not valueFrom: fieldRef: fieldPath: metadata.namespace)

all myproject namespaces in yamls have to be sed replaced or find/replaced to strimzi

added the clusterrolebinding added-clusterrolebinding-operator-watched.yaml in config dir

github of strimzi files is:
https://github.com/strimzi/strimzi-kafka-operator-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/outputs.tf
output "operator_namespace_used" {
  description = "Namespace where the Strimzi operator was deployed."
  value       = var.operator_namespace
}

output "watched_namespaces_configured" {
  description = "Namespaces the Strimzi operator is configured to watch."
  value       = var.watched_namespaces_list
}-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/versions.tf

terraform {
  required_version = ">=1.0"
  required_providers {
    helm = { source = "hashicorp/helm", version = "~> 2.17.0" }
    kubernetes = { source = "hashicorp/kubernetes", version = "~> 2.36.0"}
    null = { source = "hashicorp/null", version = "~> 3.2.4" }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/variables.tf
variable "kafka_cr_namespace" {
  description = "Namespace where the Kafka CR will be applied (must be watched by Strimzi operator)."
  type        = string
}

variable "kafka_cr_yaml_file_path" {
  description = "Path to the Kafka Custom Resource YAML file."
  type        = string
}

variable "kubeconfig_path" {
  description = "Path to the kubeconfig file for the target Kubernetes cluster."
  type        = string
  sensitive   = true
}

variable "kube_context_name" {
  description = "The kubectl context to use for applying resources. Must be valid for the provided kubeconfig_path."
  type        = string
  # This will be provided by the calling component
}

# Variables to construct output values, assuming fixed naming conventions from Strimzi
variable "kafka_cr_cluster_name" {
  description = "The metadata.name of the Kafka cluster defined in the CR YAML."
  type        = string
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/providers.tf
# terraform/environments/production/services-sydney/030-kafka-cluster/providers.tf
provider "kubernetes" {
  config_path = var.kubeconfig_path
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/main.tf
resource "null_resource" "apply_kafka_cluster_cr" {
  triggers = {
    yaml_file_sha1 = fileexists(var.kafka_cr_yaml_file_path) ? filesha1(var.kafka_cr_yaml_file_path) : ""
    # Adding context and namespace to triggers to ensure re-apply if they change for some reason
    context_trigger   = var.kube_context_name
    namespace_trigger = var.kafka_cr_namespace
  }

  provisioner "local-exec" {
    command = "kubectl --kubeconfig=${var.kubeconfig_path} --context=${var.kube_context_name} apply --namespace ${var.kafka_cr_namespace} --filename ${var.kafka_cr_yaml_file_path}"
    # The KUBECONFIG env var is redundant if --kubeconfig is used in the command, but harmless.
    environment = {
      KUBECONFIG = var.kubeconfig_path
    }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafkauser-permissive-test.yaml
# test-anonymous-broader-corrected.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: test-anonymous-broader
  namespace: kafka
  labels:
    strimzi.io/cluster: personae-kafka-cluster
spec:
  authorization:
    type: simple
    acls:
      # Allow Describe on ALL topics
      - resource:
          type: topic
          name: "*"
          patternType: literal # For all topics, name should be "*" and patternType literal
        operations:
          - Describe
        host: "*"
      # Allow Describe on the cluster
      - resource:
          type: cluster # For 'cluster' type, no 'name' or 'patternType' needed in the resource block.
          # Strimzi implicitly uses 'kafka-cluster' as the resource name.
        operations:
          - Describe
        host: "*"-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafka-temp-fix.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: personae-kafka-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.9.0
    replicas: 1
    listeners:
      # We are defining ONLY the plaintext listener.
      - name: plain
        port: 9092
        type: internal
        tls: false
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
      default.replication.factor: 1
      min.insync.replicas: 1
      inter.broker.protocol.version: "3.9"
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  zookeeper:
    replicas: 1
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafkauser-test.yaml
# modules/kafka_cluster/config/kafkauser-test.yaml
# test-anonymous-describe-user.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: test-anonymous-describe
  namespace: kafka
  labels:
    strimzi.io/cluster: personae-kafka-cluster
spec:
  authorization:
    type: simple
    acls:
      - resource:
          type: topic
          name: personae-
          patternType: prefix
        host: "*"
        operations:
          - Describe
-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafka-cluster-cr-dev.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: personae-kafka-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.9.0
    replicas: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false # Match working config
    config:
      default.replication.factor: 1
      min.insync.replicas: 1
      inter.broker.protocol.version: "3.9"
      log.message.format.version: "3.9"
      log.retention.hours: "160"
      log4j.logger.kafka.authorizer.logger: INFO
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
    resources:
      requests:
        memory: "512Mi"
        cpu: "300m"
      limits:
        memory: "1000Mi"
        cpu: "500"
  zookeeper:
    replicas: 1
    resources:
      requests:
        memory: "250Mi"
        cpu: "250m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false # Match working config
  entityOperator:
    topicOperator:
      reconciliationIntervalMs: 90000
    userOperator: {}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/personae-app-anonymous-v2.yaml
# personae-app-anonymous-V2.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: personae-app-anonymous
  namespace: kafka
  labels:
    strimzi.io/cluster: personae-kafka-cluster
spec:
  authorization:
    type: simple
    acls:
      # For init containers & tools to describe specific topics
      - resource:
          type: topic
          name: personae-core-requests # Be very specific for the test topic
          patternType: literal
        operations: [ Describe ]
        host: "*"
      # For init containers & tools to list/describe all topics (if needed)
      - resource:
          type: topic
          name: "*" # All topics
          patternType: literal
        operations: [ Describe ]
        host: "*"
      # For applications to Read/Write their topics
      - resource:
          type: topic
          name: personae- # Topics starting with "personae-"
          patternType: prefix
        operations: [ Read, Write, Create, Describe ] # Describe is good for apps too
        host: "*"

      # For applications and tools to describe the cluster (often needed for metadata)
      - resource:
          type: cluster
        operations: [ Describe ]
        host: "*"

      # For personae-core-manager consumer group (explicit literal match)
      - resource:
          type: group
          name: personae-core-manager # Explicit name
          patternType: literal
        operations: [ Read, Describe ] # Read is for consuming offsets, Describe for FindCoordinator etc.
        host: "*"
      # General describe for any other group (e.g., if other agents use different group names)
      - resource:
          type: group
          name: "*"
          patternType: literal
        operations: [ Describe ]
        host: "*"-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/personae-app-anonymous.yaml
# personae-app-anonymous-access.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: personae-app-anonymous
  namespace: kafka # Users must be in the same namespace as the Kafka cluster
  labels:
    strimzi.io/cluster: personae-kafka-cluster
spec:
  # No authentication block: applies to User:ANONYMOUS
  authorization:
    type: simple
    acls:
      # For init containers and apps to find/describe topics
      - resource:
          type: topic
          name: personae- # Topics starting with "personae-"
          patternType: prefix
        operations:
          - Describe
          - Read    # For consumers
          - Write   # For producers (if any agents produce to these)
          - Create  # If apps/producers might create them (less likely if Strimzi manages topics)
        host: "*"

      # For consumers to operate on their groups
      # Assuming consumer group names might also start with "personae-" or are related
      - resource:
          type: group
          name: personae- # Consumer groups starting with "personae-"
          patternType: prefix # Adjust if group names are different
        operations:
          - Read
          - Describe
          # - Delete # If consumers manage their own offset commits and group membership actively
        host: "*"

      # For consumers to find their coordinator (often needs Describe on a group)
      # The previous rule for group 'personae-' prefix should cover FindCoordinator for those groups.
      # If using arbitrary group names, you'd need a broader group rule like:
      - resource:
          type: group
          name: "*" # Or a more specific group prefix/name if known
          patternType: literal
        operations:
          - Describe # For FindCoordinator for any group
          # - Read # Be cautious with Read on group "*"
        host: "*"

      # General cluster describe, often needed by tools and sometimes clients
      - resource:
          type: cluster
        operations:
          - Describe
        host: "*"-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafka-cluster-cr.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: personae-kafka-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.9.0 # Match operator compatibility
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    storage:
      type: persistent-claim
      size: 100Gi
      class: ssd-large
      deleteClaim: false # Match working config
    config:
      default.replication.factor: 3
      min.insync.replicas: 2
      inter.broker.protocol.version: "3.9"
      log.message.format.version: "3.9"
      log.retention.hours: "160"
      log4j.logger.kafka.authorizer.logger: INFO
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "1500Mi"
        cpu: "1"
  zookeeper:
    replicas: 3
    resources:
      requests:
        memory: "512Mi" # Adjust
        cpu: "250m"
      limits:
        memory: "768Mi" # Adjust
        cpu: "500m"
    storage:
      type: persistent-claim
      size: 10Gi
      class: ssd
      deleteClaim: false # Match working config
  entityOperator:
    topicOperator:
      reconciliationIntervalMs: 90000
    userOperator: {}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/config/kafka-kraft-cluster.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: personae-kafka-cluster
  namespace: kafka
  annotations:
    strimzi.io/kraft: "enabled"
spec:
  kafka:
    version: 3.9.0
    replicas: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    config:
      # Single-node compatible settings
      default.replication.factor: 1
      min.insync.replicas: 1
      inter.broker.protocol.version: "3.9"
      log.retention.hours: 160
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
    resources:
      requests:
        memory: 512Mi
        cpu: 300m
      limits:
        memory: 1000Mi
        cpu: 500m
    # KRaft mode - no separate ZooKeeper needed
    metadataVersion: 3.9-IV4
  # No zookeeper section needed for KRaft
  entityOperator:
    topicOperator:
      reconciliationIntervalMs: 90000
    userOperator: {}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/README.md
when trying to deploy using a kubernetes_manifest for this kafka CRD it continually bugged out when setting config variables, so we are doing it using null_resource kubectl apply ...

this didn't work:
resource "kubernetes_manifest" "kafka_cluster" {
manifest = {
"apiVersion" = "kafka.strimzi.io/v1beta2"
"kind"       = "Kafka"
"metadata" = {
"name"      = var.kafka_cluster_name
"namespace" = var.kafka_cluster_namespace
}
"spec" = {
"kafka" = {
"version"  = var.kafka_version
"replicas" = var.kafka_replicas
"listeners" = [ # Minimal listeners
{ "name": "plain", "port": 9092, "type": "internal", "tls": false },
{ "name" = "tls", "port" = 9093, "type" = "internal", "tls"  = true }
]
"storage" = merge( # Assuming you kept the merge logic for class
{
"type" = "persistent-claim"
"size" = var.kafka_persistent_claim_size
},
var.kafka_persistent_claim_storage_class == null ? {} : { "class" = var.kafka_persistent_claim_storage_class
}
),
"config" = var.kafka_config
}
"entityOperator" = {
"topicOperator" = var.enable_topic_operator ? {} : null
"userOperator"  = var.enable_user_operator ? {} : null
}
}
}
computed_fields = [
"spec.kafka.config"
]
}

because of the "config" = va.kafka_config didn't read the map properly:
variable "kafka_config" {
description = "List of Kafka broker configuration overrides."
type        = map(string)
default = {
"log.message.format.version" = "4.0",
"log.retention.hours"        = "168"
}
}

I got:
terraform apply -auto-approve
kubernetes_manifest.kafka_cluster: Refreshing state...
╷
│ Error: Failed to update proposed state from prior state
│
│   with kubernetes_manifest.kafka_cluster,
│   on main.tf line 3, in resource "kubernetes_manifest" "kafka_cluster":
│    3: resource "kubernetes_manifest" "kafka_cluster" {
│
│ AttributeName("config"): can't use tftypes.Object["log.message.format.version":tftypes.String, "log.retention.hours":tftypes.String] as
│ tftypes.Map[tftypes.String]
╵
-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/outputs.tf
# terraform/modules/kafka_cluster/outputs.tf
output "cluster_name_applied" {
  description = "The name of the Kafka cluster that was applied."
  value       = var.kafka_cr_cluster_name
}

output "cluster_namespace_applied" {
  description = "The namespace where the Kafka cluster CR was applied."
  value       = var.kafka_cr_namespace
}

output "bootstrap_servers_plain" {
  description = "Assumed Internal Plaintext Bootstrap Servers."
  value       = "${var.kafka_cr_cluster_name}-kafka-bootstrap.${var.kafka_cr_namespace}.svc:9092"
}

output "bootstrap_servers_tls" {
  description = "Assumed Internal TLS Bootstrap Servers."
  value       = "${var.kafka_cr_cluster_name}-kafka-bootstrap.${var.kafka_cr_namespace}.svc:9093"
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/versions.tf
terraform {
  required_providers {
    kubernetes = { # Needed if you want to add data sources for services later, but not strictly for null_resource
      source  = "hashicorp/kubernetes"
      version = "~> 2.36.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "~> 3.2.4"
    }
  }
  required_version = ">= 1.0"
}-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/variables.tf
# ~/projects/terraform/rackspace_generic/terraform/modules/kubernetes_cluster_rackspace/variables.tf

variable "cluster_name" {
  description = "Name for the Kubernetes cluster (Spot Cloudspace)."
  type        = string
}

variable "rackspace_region" {
  description = "Rackspace region for the cluster (e.g., aus-syd-1)."
  type        = string
}

variable "kubernetes_version" {
  description = "Kubernetes version for the cluster."
  type        = string
  default     = "1.31.1"
}

variable "cni" {
  description = "CNI plugin for the cluster."
  type        = string
  default     = "calico"
}

variable "hacontrol_plane" {
  description = "Enable HA control plane."
  type        = bool
  default     = false
}

variable "preemption_webhook_url" {
  description = "Preemption webhook URL (e.g., Slack webhook)."
  type        = string
  sensitive   = true
  default     = null
}

# --- REMOVED OLD ON-DEMAND VARIABLES ---
# variable "ondemand_node_count" { ... }
# variable "ondemand_node_flavor" { ... }
# variable "ondemand_node_labels" { ... }
# variable "ondemand_node_taints" { ... }

# --- REMOVED OLD SPOT VARIABLES ---
# variable "spot_min_nodes" { ... }
# variable "spot_max_nodes" { ... }
# variable "spot_node_flavor" { ... }
# variable "spot_max_price" { ... }
# variable "spot_node_labels" { ... }


# +++ ADDED NEW FLEXIBLE ON-DEMAND POOL VARIABLE +++
variable "ondemand_node_pools" {
  description = "A map of on-demand node pools to create."
  type = map(object({
    node_count = number
    flavor     = string
    labels     = map(string)
    taints = list(object({
      key    = string
      value  = string
      effect = string
    }))
  }))
  default = {
    "default_pool" = {
      node_count = 0
      flavor     = "gp.small" # Example, replace with your actual flavor
      labels = {
        "role"       = "general",
        "app.type"   = "stateful",
        "managed-by" = "terraform"
      }
      taints = []
    }
  }
}

# +++ ADDED NEW FLEXIBLE SPOT POOL VARIABLE +++
variable "spot_node_pools" {
  description = "A map of spot node pools to create."
  type = map(object({
    min_nodes = number
    max_nodes = number
    flavor    = string
    max_price = number
    labels    = map(string)
  }))
  default = {
    "spot_worker_pool" = {
      min_nodes = 3
      max_nodes = 5
      flavor    = "c.large" # Example, replace with your actual flavor
      max_price = 0.01
      labels = {
        "role"       = "spot-instance",
        "app.type"   = "stateless",
        "managed-by" = "terraform"
      }
    }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/providers.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/main.tf
data "spot_serverclasses" "available_flavors" {
  # This data source does not take a region argument.
  # It's here mostly for reference if you output it.
}

resource "spot_cloudspace" "cluster" {
  cloudspace_name      = var.cluster_name
  region               = var.rackspace_region
  hacontrol_plane      = var.hacontrol_plane
  preemption_webhook   = var.preemption_webhook_url # Using the module's input variable
  wait_until_ready     = true
  kubernetes_version   = var.kubernetes_version
  cni                  = var.cni
}

# +++ CREATES MULTIPLE, DYNAMIC SPOT POOLS +++
resource "spot_spotnodepool" "spot_pools" {
  for_each = var.spot_node_pools

  cloudspace_name = spot_cloudspace.cluster.cloudspace_name
  server_class    = each.value.flavor
  bid_price       = each.value.max_price
  autoscaling = {
    min_nodes = each.value.min_nodes
    max_nodes = each.value.max_nodes
  }
  labels     = each.value.labels
  depends_on = [spot_cloudspace.cluster]
}

# +++ CREATES MULTIPLE, DYNAMIC ON-DEMAND POOLS +++
resource "spot_ondemandnodepool" "ondemand_pools" {
  for_each = var.ondemand_node_pools

  cloudspace_name      = spot_cloudspace.cluster.cloudspace_name
  server_class         = each.value.flavor
  desired_server_count = each.value.node_count
  labels               = each.value.labels
  taints               = each.value.taints
  depends_on           = [spot_cloudspace.cluster]
}


data "spot_kubeconfig" "cluster_kubeconfig" {
  cloudspace_name = spot_cloudspace.cluster.cloudspace_name
  depends_on = [
    spot_cloudspace.cluster,
    spot_ondemandnodepool.ondemand_pools, # Depends on the collection of pools
    spot_spotnodepool.spot_pools          # Depends on the collection of pools
  ]
}-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/README.md
-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/outputs.tf
# terraform/modules/kubernetes_cluster_rackspace/outputs.tf

output "kubeconfig_raw" {
  description = "Raw kubeconfig content for the cluster."
  value       = data.spot_kubeconfig.cluster_kubeconfig.raw
  sensitive   = true
}
output "cluster_name_actual" {
  description = "Name of the created Kubernetes cluster."
  value       = spot_cloudspace.cluster.cloudspace_name
}
output "cluster_endpoint_actual" {
  description = "API endpoint for the Kubernetes cluster."
  value       = data.spot_kubeconfig.cluster_kubeconfig.kubeconfigs[0].host
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/versions.tf
terraform {
  required_providers {
    spot = {
      source  = "rackerlabs/spot"
      version = "~> 0.1.4"
    }
  }
  required_version = ">= 1.0"
}
-------------------------------------------------
filepath = ./deployments/terraform/filestructureterraform.txt
.:
total 16
drwxrwxr-x  4 ant ant 4096 Jul 16 19:17 .
drwxrwxr-x  5 ant ant 4096 Jul 15 17:09 ..
drwxrwxr-x  4 ant ant 4096 Jul 15 17:34 environments
-rw-rw-r--  1 ant ant    0 Jul 16 19:17 filestructureterraform.txt
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 modules

./environments:
total 16
drwxrwxr-x 4 ant ant 4096 Jul 15 17:34 .
drwxrwxr-x 4 ant ant 4096 Jul 16 19:17 ..
drwxrwxr-x 4 ant ant 4096 Jul 16 16:35 development
drwxrwxr-x 3 ant ant 4096 Jul 15 17:24 production

./environments/development:
total 16
drwxrwxr-x 4 ant ant 4096 Jul 16 16:35 .
drwxrwxr-x 4 ant ant 4096 Jul 15 17:34 ..
drwxrwxr-x 2 ant ant 4096 Jul 15 17:34 local
drwxrwxr-x 6 ant ant 4096 Jul 16 16:35 uk_dev

./environments/development/local:
total 8
drwxrwxr-x 2 ant ant 4096 Jul 15 17:34 .
drwxrwxr-x 4 ant ant 4096 Jul 16 16:35 ..
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars

./environments/development/uk_dev:
total 24
drwxrwxr-x 6 ant ant 4096 Jul 16 16:35 .
drwxrwxr-x 4 ant ant 4096 Jul 16 16:35 ..
drwxrwxr-x 2 ant ant 4096 Jul 16 16:35 010-infrastructure
drwxrwxr-x 2 ant ant 4096 Jul 16 16:35 020-ingress-nginx
drwxrwxr-x 2 ant ant 4096 Jul 16 16:36 030-strimzi-operator
drwxrwxr-x 2 ant ant 4096 Jul 16 16:36 040-kafka-cluster

./environments/development/uk_dev/010-infrastructure:
total 24
drwxrwxr-x 2 ant ant 4096 Jul 16 16:35 .
drwxrwxr-x 6 ant ant 4096 Jul 16 16:35 ..
-rw-rw-r-- 1 ant ant 1703 Jun 18 15:53 main.tf
-rw-rw-r-- 1 ant ant  432 Jun  2 14:51 outputs.tf
-rw-rw-r-- 1 ant ant  119 Jun  2 14:52 providers.tf
-rw-rw-r-- 1 ant ant    0 Jun  2 14:52 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jun  2 16:35 terraform.tfvars.secret
-rw-rw-r-- 1 ant ant 1044 Jun  9 16:07 variables.tf

./environments/development/uk_dev/020-ingress-nginx:
total 24
drwxrwxr-x 2 ant ant 4096 Jul 16 16:35 .
drwxrwxr-x 6 ant ant 4096 Jul 16 16:35 ..
-rw-rw-r-- 1 ant ant 1073 Jun  2 15:20 main.tf
-rw-rw-r-- 1 ant ant  490 Jun  2 15:20 outputs.tf
-rw-rw-r-- 1 ant ant  223 Jun  2 15:23 providers.tf
-rw-rw-r-- 1 ant ant    0 Jun  2 14:52 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jun  2 16:35 terraform.tfvars.secret
-rw-rw-r-- 1 ant ant 1011 Jun  9 16:09 variables.tf

./environments/development/uk_dev/030-strimzi-operator:
total 44
drwxrwxr-x 2 ant ant  4096 Jul 16 16:36 .
drwxrwxr-x 6 ant ant  4096 Jul 16 16:35 ..
-rw-rw-r-- 1 ant ant  1292 Jun  2 16:58 main.tf
-rw-rw-r-- 1 ant ant   373 Jun  2 17:06 outputs.tf
-rw-rw-r-- 1 ant ant   159 Jun  2 15:54 providers.tf
-rw-rw-r-- 1 ant ant  2645 Jun 17 20:36 strimzi-rbac-terraform.tf
-rw-rw-r-- 1 ant ant 13018 Jun 24 21:42 terraform.tfstate
-rw-rw-r-- 1 ant ant     0 Jun  2 14:52 terraform.tfvars
-rw-rw-r-- 1 ant ant     0 Jun  2 16:35 terraform.tfvars.secret
-rw-rw-r-- 1 ant ant  1290 Jun  9 16:09 variables.tf

./environments/development/uk_dev/040-kafka-cluster:
total 28
drwxrwxr-x 2 ant ant 4096 Jul 16 16:36 .
drwxrwxr-x 6 ant ant 4096 Jul 16 16:35 ..
-rw-rw-r-- 1 ant ant  399 Jun  9 16:09 main.tf
-rw-rw-r-- 1 ant ant  680 Jun  2 15:59 outputs.tf
-rw-rw-r-- 1 ant ant  306 Jun  2 17:28 providers.tf
-rw-rw-r-- 1 ant ant 1342 Jun 24 21:42 terraform.tfstate
-rw-rw-r-- 1 ant ant    0 Jun  2 14:52 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jun  2 16:35 terraform.tfvars.secret
-rw-rw-r-- 1 ant ant 1075 Jun 18 20:37 variables.tf

./environments/production:
total 12
drwxrwxr-x  3 ant ant 4096 Jul 15 17:24 .
drwxrwxr-x  4 ant ant 4096 Jul 15 17:34 ..
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 uk001

./environments/production/uk001:
total 52
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 .
drwxrwxr-x  3 ant ant 4096 Jul 15 17:24 ..
drwxrwxr-x  2 ant ant 4096 Jul 16 16:31 010-infrastructure
drwxrwxr-x  2 ant ant 4096 Jul 16 16:32 020-ingress-nginx
drwxrwxr-x  2 ant ant 4096 Jul 16 16:33 030-strimzi-operator
drwxrwxr-x  2 ant ant 4096 Jul 16 16:34 040-kafka-cluster
drwxrwxr-x  2 ant ant 4096 Jul 15 17:26 050-storage
drwxrwxr-x  2 ant ant 4096 Jul 15 17:26 060-databases
drwxrwxr-x  2 ant ant 4096 Jul 15 17:27 070-database-schemas
drwxrwxr-x  2 ant ant 4096 Jul 15 17:27 080-kafka-topics
drwxrwxr-x  2 ant ant 4096 Jul 15 17:28 090-monitoring
-rw-rw-r--  1 ant ant   20 Jul 15 17:34 README.md
drwxrwxr-x  5 ant ant 4096 Jul 15 17:32 services

./environments/production/uk001/010-infrastructure:
total 56
drwxrwxr-x  2 ant ant 4096 Jul 16 16:31 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant  290 May 22 17:39 backend.tf
-rw-rw-r--  1 ant ant 1512 May 23 09:10 main.tf
-rw-rw-r--  1 ant ant 2563 May 22 17:39 outputs_nodes.tf
-rw-rw-r--  1 ant ant  639 May 22 17:39 outputs.tf
-rw-rw-r--  1 ant ant  167 May 22 17:39 providers.tf
-rw-rw-r--  1 ant ant 9300 Jun  2 12:56 terraform.tfstate.uk001-prod-cluster
-rw-rw-r--  1 ant ant  860 Jun  2 12:55 terraform.tfvars
-rw-rw-r--  1 ant ant  176 May 22 17:39 terraform.tfvars.secret
-rw-rw-r--  1 ant ant 1996 May 28 17:22 variables.tf
-rw-rw-r--  1 ant ant  518 May 22 17:39 versions.tf

./environments/production/uk001/020-ingress-nginx:
total 36
drwxrwxr-x  2 ant ant 4096 Jul 16 16:32 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant  823 May 22 17:40 main.tf
-rw-rw-r--  1 ant ant  223 May 22 17:41 outputs.tf
-rw-rw-r--  1 ant ant  251 May 22 17:39 providers.tf
-rw-rw-r--  1 ant ant  178 May 22 17:39 terraform.tfvars
-rw-rw-r--  1 ant ant   84 May 22 17:39 terraform.tfvars.secret
-rw-rw-r--  1 ant ant 1087 May 22 17:39 variables.tf
-rw-rw-r--  1 ant ant  289 May 22 17:39 versions.tf

./environments/production/uk001/030-strimzi-operator:
total 48
drwxrwxr-x  2 ant ant  4096 Jul 16 16:33 .
drwxrwxr-x 12 ant ant  4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant  4318 May 22 17:59 main.tf
-rw-rw-r--  1 ant ant     0 Jul 15 17:23 outputs.tf
-rw-rw-r--  1 ant ant   289 May 22 17:58 providers.tf
-rw-rw-r--  1 ant ant 10305 May 29 12:09 terraform.tfstate
-rw-rw-r--  1 ant ant   358 May 22 17:58 terraform.tfvars
-rw-rw-r--  1 ant ant    88 May 22 17:58 terraform.tfvars.secret
-rw-rw-r--  1 ant ant   989 May 22 18:02 variables.tf
-rw-rw-r--  1 ant ant   355 May 22 17:58 versions.tf

./environments/production/uk001/040-kafka-cluster:
total 36
drwxrwxr-x  2 ant ant 4096 Jul 16 16:34 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant  440 May 22 18:06 main.tf
-rw-rw-r--  1 ant ant  918 Jun  2 17:29 outputs.tf
-rw-rw-r--  1 ant ant  212 May 10 16:51 providers.tf
-rw-rw-r--  1 ant ant  339 Jun 18 20:37 terraform.tfvars
-rw-rw-r--  1 ant ant   86 May 10 19:16 terraform.tfvars.secret
-rw-rw-r--  1 ant ant 1012 Jun 18 20:37 variables.tf
-rw-rw-r--  1 ant ant  470 May 10 16:35 versions.tf

./environments/production/uk001/050-storage:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:26 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/060-databases:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:26 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/070-database-schemas:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:27 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/080-kafka-topics:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:27 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/090-monitoring:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:28 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services:
total 20
drwxrwxr-x  5 ant ant 4096 Jul 15 17:32 .
drwxrwxr-x 12 ant ant 4096 Jul 15 17:34 ..
drwxrwxr-x  6 ant ant 4096 Jul 15 17:32 agents
drwxrwxr-x  4 ant ant 4096 Jul 15 17:31 core-platform
drwxrwxr-x  5 ant ant 4096 Jul 15 17:33 frontends

./environments/production/uk001/services/agents:
total 24
drwxrwxr-x 6 ant ant 4096 Jul 15 17:32 .
drwxrwxr-x 5 ant ant 4096 Jul 15 17:32 ..
drwxrwxr-x 2 ant ant 4096 Jul 15 17:30 2210-agent-chassis
drwxrwxr-x 2 ant ant 4096 Jul 15 17:30 2220-reasoning-agent
drwxrwxr-x 2 ant ant 4096 Jul 15 17:30 2230-web-search-adapter
drwxrwxr-x 2 ant ant 4096 Jul 15 17:31 2240-image-generator-adapter

./environments/production/uk001/services/agents/2210-agent-chassis:
total 8
drwxrwxr-x 2 ant ant 4096 Jul 15 17:30 .
drwxrwxr-x 6 ant ant 4096 Jul 15 17:32 ..
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services/agents/2220-reasoning-agent:
total 8
drwxrwxr-x 2 ant ant 4096 Jul 15 17:30 .
drwxrwxr-x 6 ant ant 4096 Jul 15 17:32 ..
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services/agents/2230-web-search-adapter:
total 8
drwxrwxr-x 2 ant ant 4096 Jul 15 17:30 .
drwxrwxr-x 6 ant ant 4096 Jul 15 17:32 ..
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services/agents/2240-image-generator-adapter:
total 8
drwxrwxr-x 2 ant ant 4096 Jul 15 17:31 .
drwxrwxr-x 6 ant ant 4096 Jul 15 17:32 ..
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services/core-platform:
total 16
drwxrwxr-x 4 ant ant 4096 Jul 15 17:31 .
drwxrwxr-x 5 ant ant 4096 Jul 15 17:32 ..
drwxrwxr-x 2 ant ant 4096 Jul 16 13:10 1110-auth-service
drwxrwxr-x 2 ant ant 4096 Jul 15 17:29 1120-core-manager

./environments/production/uk001/services/core-platform/1110-auth-service:
total 12
drwxrwxr-x 2 ant ant 4096 Jul 16 13:10 .
drwxrwxr-x 4 ant ant 4096 Jul 15 17:31 ..
-rw-rw-r-- 1 ant ant 1370 Jul 16 13:10 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services/core-platform/1120-core-manager:
total 8
drwxrwxr-x 2 ant ant 4096 Jul 15 17:29 .
drwxrwxr-x 4 ant ant 4096 Jul 15 17:31 ..
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services/frontends:
total 20
drwxrwxr-x 5 ant ant 4096 Jul 15 17:33 .
drwxrwxr-x 5 ant ant 4096 Jul 15 17:32 ..
drwxrwxr-x 2 ant ant 4096 Jul 15 17:33 3310-admin-dashboard
drwxrwxr-x 2 ant ant 4096 Jul 16 13:12 3320-user-portal
drwxrwxr-x 2 ant ant 4096 Jul 15 17:33 3330-agent-playground

./environments/production/uk001/services/frontends/3310-admin-dashboard:
total 8
drwxrwxr-x 2 ant ant 4096 Jul 15 17:33 .
drwxrwxr-x 5 ant ant 4096 Jul 15 17:33 ..
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services/frontends/3320-user-portal:
total 12
drwxrwxr-x 2 ant ant 4096 Jul 16 13:12 .
drwxrwxr-x 5 ant ant 4096 Jul 15 17:33 ..
-rw-rw-r-- 1 ant ant  595 Jul 16 13:12 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./environments/production/uk001/services/frontends/3330-agent-playground:
total 8
drwxrwxr-x 2 ant ant 4096 Jul 15 17:33 .
drwxrwxr-x 5 ant ant 4096 Jul 15 17:33 ..
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r-- 1 ant ant    0 Jul 15 17:24 terraform.tfvars
-rw-rw-r-- 1 ant ant    0 Jul 15 17:23 variables.tf

./modules:
total 44
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 .
drwxrwxr-x  4 ant ant 4096 Jul 16 19:17 ..
drwxrwxr-x  2 ant ant 4096 Jul 15 17:23 k8s-job-runner
drwxrwxr-x  3 ant ant 4096 Jul 16 16:29 kafka-cluster
drwxrwxr-x  2 ant ant 4096 Jul 16 13:26 kustomize-apply
drwxrwxr-x  2 ant ant 4096 Jul 15 17:23 mysql-instance
drwxrwxr-x  3 ant ant 4096 Jul 16 16:28 nginx-ingress
drwxrwxr-x  2 ant ant 4096 Jul 15 17:23 postgres-instance
drwxrwxr-x  2 ant ant 4096 Jul 16 16:58 rackspace-kubernetes
drwxrwxr-x  2 ant ant 4096 Jul 15 17:24 s3-buckets
drwxrwxr-x  3 ant ant 4096 Jul 16 16:25 strimzi-operator

./modules/k8s-job-runner:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:23 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./modules/kafka-cluster:
total 36
drwxrwxr-x  3 ant ant 4096 Jul 16 16:29 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
drwxrwxr-x  2 ant ant 4096 Jul 16 16:29 config
-rw-rw-r--  1 ant ant  744 Jun  2 17:26 main.tf
-rw-rw-r--  1 ant ant  729 May  9 20:03 outputs.tf
-rw-rw-r--  1 ant ant  144 May 10 16:38 providers.tf
-rw-rw-r--  1 ant ant 1953 May  9 12:42 README.md
-rw-rw-r--  1 ant ant    0 May  9 20:04 terraform.tfvars
-rw-rw-r--  1 ant ant  895 Jun  2 17:25 variables.tf
-rw-rw-r--  1 ant ant  337 May 10 16:35 versions.tf

./modules/kafka-cluster/config:
total 40
drwxrwxr-x 2 ant ant 4096 Jul 16 16:29 .
drwxrwxr-x 3 ant ant 4096 Jul 16 16:29 ..
-rw-rw-r-- 1 ant ant 1230 Jun 23 17:42 kafka-cluster-cr-dev.yaml
-rw-rw-r-- 1 ant ant 1265 Jun 18 09:48 kafka-cluster-cr.yaml
-rw-rw-r-- 1 ant ant 1125 Jun 18 09:48 kafka-kraft-cluster.yaml
-rw-rw-r-- 1 ant ant  814 Jun 23 17:39 kafka-temp-fix.yaml
-rw-rw-r-- 1 ant ant  799 Jun 18 09:48 kafkauser-permissive-test.yaml
-rw-rw-r-- 1 ant ant  463 Jun 18 09:48 kafkauser-test.yaml
-rw-rw-r-- 1 ant ant 1775 Jun 18 09:48 personae-app-anonymous-v2.yaml
-rw-rw-r-- 1 ant ant 2046 Jun 18 09:48 personae-app-anonymous.yaml

./modules/kustomize-apply:
total 16
drwxrwxr-x  2 ant ant 4096 Jul 16 13:26 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
-rw-rw-r--  1 ant ant  741 Jul 16 13:07 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r--  1 ant ant  764 Jul 16 13:26 variables.tf

./modules/mysql-instance:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:23 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./modules/nginx-ingress:
total 32
drwxrwxr-x  3 ant ant 4096 Jul 16 16:28 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
drwxrwxr-x  2 ant ant 4096 Jul 16 16:28 config
-rw-rw-r--  1 ant ant 1678 May 20 15:41 main.tf
-rw-rw-r--  1 ant ant 1262 May 10 19:01 outputs.tf
-rw-rw-r--  1 ant ant    0 May  9 19:28 providers.tf
-rw-rw-r--  1 ant ant   23 May  9 18:23 terraform.tfvars
-rw-rw-r--  1 ant ant  808 May  9 19:30 variables.tf
-rw-rw-r--  1 ant ant  289 May  9 19:30 versions.tf

./modules/nginx-ingress/config:
total 12
drwxrwxr-x 2 ant ant 4096 Jul 16 16:28 .
drwxrwxr-x 3 ant ant 4096 Jul 16 16:28 ..
-rw-rw-r-- 1 ant ant 2881 May 20 15:36 ingress-nginx-values.yaml

./modules/postgres-instance:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:23 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./modules/rackspace-kubernetes:
total 32
drwxrwxr-x  2 ant ant 4096 Jul 16 16:58 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
-rw-rw-r--  1 ant ant 1722 Jul 16 16:58 main.tf
-rw-rw-r--  1 ant ant  567 May  9 19:22 outputs.tf
-rw-rw-r--  1 ant ant    0 May  9 19:25 providers.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:24 README.md
-rw-rw-r--  1 ant ant 4874 May  9 12:42 terraform.tfvars.secret
-rw-rw-r--  1 ant ant 2562 Jul 16 16:55 variables.tf
-rw-rw-r--  1 ant ant  151 May  9 18:23 versions.tf

./modules/s3-buckets:
total 8
drwxrwxr-x  2 ant ant 4096 Jul 15 17:24 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 main.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 outputs.tf
-rw-rw-r--  1 ant ant    0 Jul 15 17:23 variables.tf

./modules/strimzi-operator:
total 36
drwxrwxr-x  3 ant ant 4096 Jul 16 16:25 .
drwxrwxr-x 11 ant ant 4096 Jul 15 17:22 ..
-rw-rw-r--  1 ant ant  806 Jun 17 19:48 main.tf
-rw-rw-r--  1 ant ant  306 May  9 19:54 outputs.tf
-rw-rw-r--  1 ant ant  174 May 23 08:45 providers.tf
-rw-rw-r--  1 ant ant  541 May  9 12:42 README.md
drwxrwxr-x  2 ant ant 4096 Jul 16 16:25 strimzi-yaml-0.45.0
-rw-rw-r--  1 ant ant  893 May  9 19:53 variables.tf
-rw-rw-r--  1 ant ant  272 May 10 16:35 versions.tf

./modules/strimzi-operator/strimzi-yaml-0.45.0:
total 1164
drwxrwxr-x 2 ant ant   4096 Jul 16 16:25 .
drwxrwxr-x 3 ant ant   4096 Jul 16 16:25 ..
-rw-r--r-- 1 ant ant    106 May  9 12:42 010-ServiceAccount-strimzi-cluster-operator.yaml
-rw-r--r-- 1 ant ant   4833 May  9 12:42 020-ClusterRole-strimzi-cluster-operator-role.yaml
-rw-r--r-- 1 ant ant    333 May  9 12:42 020-RoleBinding-strimzi-cluster-operator.yaml
-rw-r--r-- 1 ant ant    336 May  9 12:42 021-ClusterRoleBinding-strimzi-cluster-operator.yaml
-rw-r--r-- 1 ant ant   1149 May  9 12:42 021-ClusterRole-strimzi-cluster-operator-role.yaml
-rw-r--r-- 1 ant ant    937 May  9 12:42 022-ClusterRole-strimzi-cluster-operator-role.yaml
-rw-r--r-- 1 ant ant    354 May  9 12:42 022-RoleBinding-strimzi-cluster-operator.yaml
-rw-r--r-- 1 ant ant  10258 May 13 15:23 023-amended-ClusterRole-strimzi-cluster-operator-role.yaml
-rw-r--r-- 1 ant ant    338 May  9 12:42 023-RoleBinding-strimzi-cluster-operator.yaml
-rw-r--r-- 1 ant ant    587 May  9 12:42 030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml
-rw-r--r-- 1 ant ant    396 May  9 12:42 030-ClusterRole-strimzi-kafka-broker.yaml
-rw-r--r-- 1 ant ant   1363 May  9 12:42 031-ClusterRole-strimzi-entity-operator.yaml
-rw-r--r-- 1 ant ant    591 May  9 12:42 031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
-rw-r--r-- 1 ant ant    634 May  9 12:42 033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml
-rw-r--r-- 1 ant ant    471 May  9 12:42 033-ClusterRole-strimzi-kafka-client.yaml
-rw-r--r-- 1 ant ant 480170 May  9 12:42 040-Crd-kafka.yaml
-rw-r--r-- 1 ant ant 136179 May  9 12:42 041-Crd-kafkaconnect.yaml
-rw-r--r-- 1 ant ant   5170 May  9 12:42 042-Crd-strimzipodset.yaml
-rw-r--r-- 1 ant ant  18034 May  9 12:42 043-Crd-kafkatopic.yaml
-rw-r--r-- 1 ant ant  38528 May  9 12:42 044-Crd-kafkauser.yaml
-rw-r--r-- 1 ant ant  84313 May  9 12:42 045-Crd-kafkamirrormaker.yaml
-rw-r--r-- 1 ant ant  79372 May  9 12:42 046-Crd-kafkabridge.yaml
-rw-r--r-- 1 ant ant   7373 May  9 12:42 047-Crd-kafkaconnector.yaml
-rw-r--r-- 1 ant ant 143431 May  9 12:42 048-Crd-kafkamirrormaker2.yaml
-rw-r--r-- 1 ant ant   8672 May  9 12:42 049-Crd-kafkarebalance.yaml
-rw-r--r-- 1 ant ant  63056 May  9 12:42 04A-Crd-kafkanodepool.yaml
-rw-r--r-- 1 ant ant    992 May  9 12:42 050-ConfigMap-strimzi-cluster-operator.yaml
-rw-r--r-- 1 ant ant   4424 May  9 12:42 060-Deployment-strimzi-cluster-operator.yaml
-rw-rw-r-- 1 ant ant    344 May  9 12:42 added-clusterrolebinding-operator-watched.yaml
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/020-ingress-nginx/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/020-ingress-nginx/variables.tf

variable "kube_context_name" {
  description = "The Kubernetes context name for Kind."
  type        = string
  default     = "kind-personae-dev"
}

variable "kubeconfig_path" {
  description = "Optional path to kubeconfig YAML file."
  type        = string
  default     = null # If null, a default single-node cluster is created
}

variable "ingress_nginx_dev_namespace" { // Renamed for clarity and consistency
  description = "Namespace for Nginx Ingress in dev."
  type        = string
  default     = "ingress-nginx"
}

variable "ingress_nginx_dev_chart_version" { // Renamed
  description = "Helm chart version for Nginx Ingress in dev."
  type        = string
  default     = "4.10.1"
}

variable "ingress_nginx_dev_http_node_port" {
  description = "NodePort for HTTP for Nginx Ingress in dev."
  type        = number
  default     = 30080
}

variable "ingress_nginx_dev_https_node_port" {
  description = "NodePort for HTTPS for Nginx Ingress in dev."
  type        = number
  default     = 30443
}

-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/020-ingress-nginx/providers.tf
provider "kubernetes" {
  config_path    = "~/.kube/config"
  config_context = var.kube_context_name
}

provider "helm" {
  kubernetes {
    config_path    = "~/.kube/config"
    config_context = var.kube_context_name
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/020-ingress-nginx/main.tf

module "nginx_ingress_dev" {
  source = "../../../../modules/nginx_ingress"

  ingress_namespace    = var.ingress_nginx_dev_namespace
  helm_chart_version   = var.ingress_nginx_dev_chart_version
  create_namespace     = true # Let the module create the namespace

  # For Kind, you usually don't need custom values unless you want specific NodePorts
  # or to disable LoadBalancer service type (which isn't typically used with Kind directly).
  # The module default of NodePort service type for the controller is fine for Kind.
  helm_values_content = yamlencode({
    controller = {
      kind = "Deployment" # DaemonSet is fine too, Deployment is often simpler for local Kind
      replicaCount = 1
      service = {
        type = "NodePort" # Exposes on NodePorts
        nodePorts = {
          http = var.ingress_nginx_dev_http_node_port
          https = var.ingress_nginx_dev_https_node_port
        }
      }
      # Disable admission webhooks for simpler Kind setup if they cause issues.
      admissionWebhooks = {
         enabled = false
      }
    }
  })
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/020-ingress-nginx/outputs.tf

output "http_node_port" {
  description = "HTTP NodePort for the Nginx Ingress controller in dev."
  value       = var.ingress_nginx_dev_http_node_port
}
output "https_node_port" {
  description = "HTTPS NodePort for the Nginx Ingress controller in dev."
  value       = var.ingress_nginx_dev_https_node_port
}
output "namespace" {
  description = "Namespace of the Nginx Ingress controller in dev."
  value       = module.nginx_ingress_dev.namespace // Assuming your module outputs this
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/070-database-schemas/variables.tf
# No variables needed for this layer as all values are derived from remote state.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/070-database-schemas/providers.tf
terraform {
  required_providers {
    postgresql = {
      source  = "cyrilgdn/postgresql"
      version = "~> 1.20.0"
    }
    mysql = {
      source  = "drarko/mysql"
      version = "2.0.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "~> 3.2.1"
    }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/070-database-schemas/main.tf
terraform {
  required_providers {
    postgresql = {
      source  = "cyrilgdn/postgresql"
      version = "~> 1.20.0"
    }
    mysql = {
      source  = "drarko/mysql"
      version = "2.0.0"
    }
    null = {
      source = "hashicorp/null"
      version = "~> 3.2.1"
    }
  }

  backend "kubernetes" {
    secret_suffix = "tfstate-db-schemas-dev"
    config_path   = "~/.kube/config"
  }
}

# Read the outputs from the dev database creation layer
data "terraform_remote_state" "databases_dev" {
  backend = "kubernetes"
  config = {
    secret_suffix = "tfstate-databases-dev"
    config_path   = "~/.kube/config"
  }
}

# --- MySQL Provider ---
provider "mysql" {
  endpoint = "${data.terraform_remote_state.databases_dev.outputs.external_mysql_host}:3306"
  username = "auth_user_dev"
  password = data.terraform_remote_state.databases_dev.outputs.external_mysql_password
}

# --- Apply PostgreSQL Schemas ---
# Read the SQL file for the pgvector extension
data "local_file" "pgvector_sql" {
  filename = "${path.module}/../../../../sql/001_enable_pgvector.sql"
}

# Apply the pgvector schema using a local psql command
resource "null_resource" "pgvector_extension_dev" {
  # Trigger a re-run if the SQL file content changes
  triggers = {
    content_sha1 = sha1(data.local_file.pgvector_sql.content)
  }

  provisioner "local-exec" {
    # This command uses the psql client to run the schema.
    # It securely passes the password via the PGPASSWORD environment variable.
    command = "psql -h ${data.terraform_remote_state.databases_dev.outputs.postgres_clients_db_dev_service_endpoint} -U clients_user_dev -d clientsdb_dev -f ${data.local_file.pgvector_sql.filename}"

    environment = {
      PGPASSWORD = data.terraform_remote_state.databases_dev.outputs.clients_db_password
    }
  }
}

# --- Apply MySQL Schema ---
# Read the SQL file for the auth service database schema
data "local_file" "auth_db_schema" {
  filename = "${path.module}/../../../../sql/auth_schema.sql"
}

# Apply the schema to the external dev database using the mysql_script resource
resource "mysql_script" "auth_db_schema_apply" {
  database = "authservicedb_dev"
  script_path = data.local_file.auth_db_schema.filename

  # This ensures the database is created if it doesn't exist before running the script
  depends_on = [
    resource.mysql_database.auth_db_from_schema
  ]
}

resource "mysql_database" "auth_db_from_schema" {
  name = "authservicedb_dev"
  default_character_set = "utf8mb4"
  default_collation     = "utf8mb4_unicode_ci"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/070-database-schemas/outputs.tf
output "pgvector_extension_status" {
  description = "Status of the pgvector extension application on the dev clients database."
  value       = "Applied"
  depends_on  = [postgresql_query.pgvector_extension_dev]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/080-kafka-topics/variables.tf
variable "platform_topics" {
  description = "A list of Kafka topic names to be created for the application."
  type        = list(string)
  default = [
    # System & Orchestration Topics
    "system.commands.workflow.resume",
    "system.events.workflow.paused",
    "system.events.workflow.completed",

    # Core Service Topics
    "requests.auth.user.create",
    "events.auth.user.created",

    # Agent Communication Topics
    "requests.agent.task.execute",
    "events.agent.task.completed",
    "events.agent.task.failed",
    "events.agent.task.progress",

    # Specialized Agent Topics
    "requests.agent.reasoning",
    "requests.agent.web-search",
    "requests.agent.image-generation",
  ]
}

variable "default_partitions" {
  description = "Default number of partitions for development topics."
  type        = number
  default     = 1
}

variable "default_replication_factor" {
  description = "Default replication factor for development topics. Should be 1 for a single-broker dev cluster."
  type        = number
  default     = 1
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/080-kafka-topics/main.tf
terraform {
  required_providers {
    kafka = {
      source  = "mongey/kafka"
      version = "~> 0.11.0"
    }
  }
  backend "kubernetes" {
    secret_suffix = "tfstate-kafka-topics-dev"
    config_path   = "~/.kube/config"
  }
}

# Read the outputs from the dev Kafka cluster layer
data "terraform_remote_state" "kafka_cluster_dev" {
  backend = "kubernetes"
  config = {
    # This suffix must match the backend config of your dev 040-kafka-cluster layer
    secret_suffix = "tfstate-kafka-cluster-dev"
    config_path   = "~/.kube/config"
  }
}

provider "kafka" {
  bootstrap_servers = data.terraform_remote_state.kafka_cluster_dev.outputs.kafka_bootstrap_servers
}

# Define all required Kafka topics for the platform
resource "kafka_topic" "topics_dev" {
  for_each = toset(var.platform_topics)

  name               = each.key
  partitions         = var.default_partitions
  replication_factor = var.default_replication_factor
  config = {
    "retention.ms" = "604800000" # Retain messages for 7 days in dev
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/080-kafka-topics/outputs.tf
output "topic_names" {
  description = "The names of the created Kafka topics for the development environment."
  value       = [for topic in kafka_topic.topics_dev : topic.name]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/010-infrastructure/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/010-infrastructure/variables.tf
variable "kind_cluster_name" {
  description = "Name for the Kind cluster for development."
  type        = string
  default     = "personae-dev"
}

variable "kind_node_image" {
  description = "Node image for Kind cluster (e.g., kindest/node:v1.27.3)."
  type        = string
  default     = "kindest/node:v1.27.3" # Choose a version
}

variable "kind_config_path" {
  description = "Optional path to a Kind configuration YAML file."
  type        = string
  default     = null # If null, a default single-node cluster is created
}

variable "kubeconfig_path" {
  description = "Optional path to kubeconfig YAML file."
  type        = string
  default     = null # If null, a default single-node cluster is created
}

# This output is not directly from a resource, but reflects the context name
# that will be used by other components.
variable "kube_context_name" {
  description = "The kubectl context name to use for this Kind cluster."
  type        = string
  default     = "kind-personae-dev" # Must match KIND_CONTEXT_DEV in Makefile
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/010-infrastructure/providers.tf
terraform {
  required_providers {
    null = {
      source  = "hashicorp/null"
      version = "~> 3.2.1"
    }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/010-infrastructure/main.tf
resource "null_resource" "kind_cluster" {
  triggers = {
    cluster_name = var.kind_cluster_name
    config_path  = var.kind_config_path
    node_image   = var.kind_node_image
  }

  provisioner "local-exec" {
    when    = create
    command = <<-EOT
      set -e
      if ! kind get clusters | grep -q "^${self.triggers.cluster_name}$$"; then
        echo "Creating Kind cluster '${self.triggers.cluster_name}'..."
        kind create cluster --name "${self.triggers.cluster_name}" --image "${self.triggers.node_image}" ${var.kind_config_path != null ? "--config \"${var.kind_config_path}\"" : ""}
        echo "Waiting for Kind cluster control plane to be ready..."
        timeout 120s bash -c 'while ! kubectl --context="kind-${self.triggers.cluster_name}" cluster-info >/dev/null 2>&1; do sleep 1; done' || \
          (echo "Timeout waiting for Kind cluster. Check 'kind get logs ${self.triggers.cluster_name}'" && exit 1)
        echo "Kind cluster '${self.triggers.cluster_name}' is ready."
      else
        echo "Kind cluster '${self.triggers.cluster_name}' already exists. Skipping creation."
      fi
    EOT
  }

  provisioner "local-exec" {
    when    = destroy
    # Use self.triggers.cluster_name which is known at destroy time based on the state
    command = "kind delete cluster --name \"${self.triggers.cluster_name}\" || true"
  }
}

resource "null_resource" "label_kind_node" {
  depends_on = [null_resource.kind_cluster]

  provisioner "local-exec" {
    command = <<-EOT
      # Wait for the node to be ready
      kubectl wait --for=condition=ready node --all --timeout=60s

      # Label the node
      kubectl label nodes --all role=spot-instance --overwrite
    EOT
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/010-infrastructure/outputs.tf
output "kind_cluster_name_output" {
  description = "Name of the Kind cluster."
  value       = var.kind_cluster_name
}

output "kind_kube_context_name_output" {
  description = "The kubectl context name for this Kind cluster."
  value       = "kind-${var.kind_cluster_name}" # Standard Kind context naming
}

# No kubeconfig_raw output here as Kind manages the default kubeconfig file.
# Other components will use the context name.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/090-monitoring/variables.tf
variable "monitoring_namespace" {
  description = "The Kubernetes namespace to deploy the dev monitoring stack into."
  type        = string
  default     = "monitoring-dev"
}

variable "grafana_admin_password" {
  description = "The admin password for the Grafana dashboard."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/090-monitoring/main.tf
terraform {
  required_providers {
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.11.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.20"
    }
  }
  backend "kubernetes" {
    secret_suffix = "tfstate-monitoring-dev"
    config_path   = "~/.kube/config"
  }
}

data "helm_repository" "prometheus_community" {
  name = "prometheus-community"
  url  = "https://prometheus-community.github.io/helm-charts"
}

resource "kubernetes_namespace" "monitoring_ns_dev" {
  metadata {
    name = var.monitoring_namespace
  }
}

resource "helm_release" "prometheus_stack_dev" {
  name       = "prometheus-stack-dev"
  repository = data.helm_repository.prometheus_community.metadata[0].name
  chart      = "kube-prometheus-stack"
  namespace  = kubernetes_namespace.monitoring_ns_dev.metadata[0].name
  version    = "51.8.0" # Pin to the same chart version as production

  values = [
    templatefile("${path.module}/values.yaml.tpl", {
      grafana_admin_password = var.grafana_admin_password
    })
  ]

  depends_on = [kubernetes_namespace.monitoring_ns_dev]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/090-monitoring/values.yaml.tpl
# values.yaml.tpl for development
# This configuration is lightweight and suitable for local clusters.

# Grafana configuration
grafana:
  adminPassword: "${grafana_admin_password}"
  # For dev, we use ClusterIP and access via `kubectl port-forward`
  service:
    type: ClusterIP

# Prometheus configuration
prometheus:
  prometheusSpec:
    # Disable persistent storage for development to keep it lightweight
    storageSpec: {}
    retention: 1d # Lower retention for dev-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/090-monitoring/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/050-storage/variables.tf
variable "region" {
  description = "The region where resources will be deployed."
  type        = string
  default     = "uk-dev"
}

variable "image_bucket_name" {
  description = "Name for the bucket to store generated images for development."
  type        = string
  default     = "personae-dev-uk-images"
}

variable "site_assets_bucket_name" {
  description = "Name for the bucket to store generated static site assets for development."
  type        = string
  default     = "personae-dev-uk-site-assets"
}

variable "b2_application_key_id" {
  description = "The application key ID for Backblaze B2."
  type        = string
  sensitive   = true
}

variable "b2_application_key" {
  description = "The application key for Backblaze B2."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/050-storage/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-storage-dev"
    config_path   = "~/.kube/config"
  }
}

provider "b2" {
  application_key_id = var.b2_application_key_id
  application_key    = var.b2_application_key
}

module "storage_buckets_dev" {
  source = "../../../../modules/s3-buckets"

  bucket_names = [
    var.image_bucket_name,
    var.site_assets_bucket_name
  ]

  tags = {
    environment = "development"
    region      = var.region
    managed_by  = "terraform"
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/060-databases/variables.tf
variable "k8s_namespace" {
  description = "The Kubernetes namespace to deploy dev database resources into."
  type        = string
  default     = "personae-dev-db"
}

variable "postgres_storage_class" {
  description = "The name of the StorageClass for dev PostgreSQL volumes (e.g., your local-path-provisioner)."
  type        = string
  default     = "standard"
}

# --- External MySQL Variables ---
variable "external_mysql_host" {
  description = "The endpoint for the external MySQL database used for development."
  type        = string
  sensitive   = true
}

variable "external_mysql_password" {
  description = "Password for the external MySQL database."
  type        = string
  sensitive   = true
}

# --- In-Cluster PostgreSQL Variables ---
variable "templates_db_password" {
  description = "Password for the dev templates PostgreSQL database."
  type        = string
  sensitive   = true
}

variable "clients_db_password" {
  description = "Password for the dev clients PostgreSQL database."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/060-databases/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-databases-dev"
    config_path   = "~/.kube/config"
  }
}

# --- External MySQL Secret ---
module "external_mysql_auth_db_dev" {
  source = "../../../../modules/mysql-instance"

  instance_name = "personae-dev-uk-auth-db"
  namespace     = var.k8s_namespace
  db_host       = var.external_mysql_host
  database_name = "authservicedb_dev"
  database_user = "auth_user_dev"
  database_pass = var.external_mysql_password
}

# --- In-Cluster PostgreSQL for Templates ---
module "postgres_templates_db_dev" {
  source = "../../../../modules/postgres-instance"

  instance_name      = "postgres-templates-dev"
  namespace          = var.k8s_namespace
  database_name      = "templatesdb_dev"
  database_user      = "templates_user_dev"
  database_pass      = var.templates_db_password
  storage_class_name = var.postgres_storage_class
  storage_size       = "2Gi" # Smaller size for dev
}

# --- In-Cluster PostgreSQL for Client Data ---
module "postgres_clients_db_dev" {
  source = "../../../../modules/postgres-instance"

  instance_name      = "postgres-clients-dev"
  namespace          = var.k8s_namespace
  database_name      = "clientsdb_dev"
  database_user      = "clients_user_dev"
  database_pass      = var.clients_db_password
  storage_class_name = var.postgres_storage_class
  storage_size       = "5Gi" # Smaller size for dev
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/030-strimzi-operator/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/030-strimzi-operator/variables.tf
variable "kube_context_name" {
  description = "The Kubernetes context name for Kind."
  type        = string
  default = "kind-personae-dev"
}

variable "kubeconfig_path" {
  description = "Optional path to kubeconfig YAML file."
  type        = string
  default     = null # If null, a default single-node cluster is created
}

variable "strimzi_operator_dev_namespace" {
  description = "Namespace for the Strimzi operator in dev."
  type        = string
  default     = "strimzi" // Operator's own namespace
}

variable "watched_namespaces_dev" {
  description = "List of namespaces for the Strimzi operator to watch in dev."
  type        = list(string)
  default     = ["kafka", "personae"] // Strimzi will watch 'kafka' for Kafka CRs and 'personae' if KafkaUsers are there
}

variable "strimzi_yaml_bundle_path_dev" {
  description = "Path to the Strimzi YAML files directory for dev."
  type        = string
  # Path relative to this file's directory, pointing to the module's shared Strimzi YAMLs
  default     = "../../../../modules/strimzi_operator/strimzi-yaml-0.45.0/"
}

variable "strimzi_operator_deployment_yaml_filename_dev" {
description = "Filename of the main operator deployment YAML."
type        = string
default     = "060-Deployment-strimzi-cluster-operator.yaml"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/030-strimzi-operator/providers.tf
provider "kubernetes" {
  config_path    = "~/.kube/config"
  config_context = var.kube_context_name
}

provider "null" {} // If your module uses null_resource-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/030-strimzi-operator/main.tf
# Ensure the namespaces Strimzi will operate in or watch exist.
# Strimzi operator's own namespace:
resource "kubernetes_namespace" "operator_ns" {
  metadata {
    name = var.strimzi_operator_dev_namespace // e.g., "strimzi"
  }
}

# Namespace for Kafka clusters (watched by Strimzi):
resource "kubernetes_namespace" "kafka_cluster_ns" {
  metadata {
    name = "kafka" // Assuming Kafka CRs will be in 'kafka' namespace
  }
}

# Namespace for Personae app (if Strimzi needs to manage KafkaUsers there):
resource "kubernetes_namespace" "personae_app_ns" {
  metadata {
    name = "personae" // Assuming Personae app and potentially KafkaUsers are in 'personae'
  }
}

module "strimzi_operator" {
  source = "../../../../modules/strimzi_operator"

  operator_namespace                = kubernetes_namespace.operator_ns.metadata[0].name
  watched_namespaces_list           = var.watched_namespaces_dev
  strimzi_yaml_source_path          = var.strimzi_yaml_bundle_path_dev
  operator_deployment_yaml_filename = var.strimzi_operator_deployment_yaml_filename_dev
  cluster_kubeconfig_path           = "" # Path to the kubeconfig Terraform should use

  depends_on = [
    kubernetes_namespace.operator_ns,
    kubernetes_namespace.kafka_cluster_ns,
    kubernetes_namespace.personae_app_ns
  ]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/030-strimzi-operator/strimzi-rbac-terraform.tf
resource "kubernetes_cluster_role" "strimzi_kafka_namespace" {
  metadata {
    name = "strimzi-cluster-operator-kafka-namespace"
  }

  rule {
    api_groups = [""]
    resources  = ["pods", "services", "endpoints", "persistentvolumeclaims", "configmaps", "secrets", "serviceaccounts"]
    verbs      = ["get", "list", "watch", "create", "update", "patch", "delete"]
  }

  rule {
    api_groups = ["apps"]
    resources  = ["deployments", "statefulsets", "replicasets"]
    verbs      = ["get", "list", "watch", "create", "update", "patch", "delete"]
  }

  rule {
    api_groups = ["networking.k8s.io"]
    resources  = ["ingresses", "networkpolicies"]
    verbs      = ["get", "list", "watch", "create", "update", "patch", "delete"]
  }

  rule {
    api_groups = ["kafka.strimzi.io"]
    resources  = ["*"]
    verbs      = ["*"]
  }

  rule {
    api_groups = ["core.strimzi.io"]
    resources  = ["*"]
    verbs      = ["*"]
  }

  rule {
    api_groups = ["rbac.authorization.k8s.io"]
    resources  = ["roles", "rolebindings"]
    verbs      = ["get", "list", "watch", "create", "update", "patch", "delete"]
  }

  rule {
    api_groups = ["policy"]
    resources  = ["poddisruptionbudgets"]
    verbs      = ["get", "list", "watch", "create", "update", "patch", "delete"]
  }
}

resource "kubernetes_cluster_role_binding" "strimzi_kafka_namespace" {
  metadata {
    name = "strimzi-cluster-operator-kafka-namespace"
  }

  role_ref {
    api_group = "rbac.authorization.k8s.io"
    kind      = "ClusterRole"
    name      = kubernetes_cluster_role.strimzi_kafka_namespace.metadata[0].name
  }

  subject {
    kind      = "ServiceAccount"
    name      = "strimzi-cluster-operator"
    namespace = "strimzi"
  }
}

resource "kubernetes_role_binding" "strimzi_kafka_namespace" {
  metadata {
    name      = "strimzi-cluster-operator-kafka-namespace"
    namespace = "kafka"
  }

  role_ref {
    api_group = "rbac.authorization.k8s.io"
    kind      = "ClusterRole"
    name      = kubernetes_cluster_role.strimzi_kafka_namespace.metadata[0].name
  }

  subject {
    kind      = "ServiceAccount"
    name      = "strimzi-cluster-operator"
    namespace = "strimzi"
  }
}

resource "kubernetes_cluster_role_binding" "strimzi_entity_operator_delegation" {
  metadata {
    name = "strimzi-cluster-operator-entity-operator-delegation"
    labels = {
      app = "strimzi"
    }
  }

  role_ref {
    api_group = "rbac.authorization.k8s.io"
    kind      = "ClusterRole"
    name      = "strimzi-entity-operator"
  }

  subject {
    kind      = "ServiceAccount"
    name      = "strimzi-cluster-operator"
    namespace = "strimzi"
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/030-strimzi-operator/outputs.tf
output "operator_namespace_used" {
  description = "Namespace where the Strimzi operator was deployed for dev."
  value       = module.strimzi_operator.operator_namespace_used
}

output "watched_namespaces_configured" {
  description = "Namespaces the Strimzi operator is configured to watch for dev."
  value       = module.strimzi_operator.watched_namespaces_configured
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/040-kafka-cluster/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/040-kafka-cluster/variables.tf
variable "kube_context_name" {
  description = "The Kubernetes context name for Kind (e.g., kind-personae-dev)."
  type        = string
  default     = "kind-personae-dev"
}

variable "kubeconfig_path" { // Specific name for this component's var
  description = "Path to the kubeconfig file to be used for this dev component."
  type        = string
  default     = "~/.kube/config" // Default for Kind, overridden by Makefile if necessary
}

variable "kafka_namespace_dev" {
  description = "Namespace where the Kafka CR for dev will be deployed."
  type        = string
  default     = "kafka"
}

variable "kafka_cluster_cr_yaml_path_dev" {
  description = "Path to the Kafka CR YAML file for the dev instance."
  type        = string
  default     = "../../../../modules/kafka_cluster/config/kafka-cluster-cr-dev.yaml" // Point to your DEV version
}

variable "kafka_cluster_name_dev" {
  description = "The metadata.name of the Kafka cluster for dev."
  type        = string
  default     = "personae-kafka-cluster" // Should match the name in kafka-cluster-cr-dev.yaml
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/040-kafka-cluster/providers.tf
provider "kubernetes" {
  config_path    = abspath(pathexpand(var.kubeconfig_path_for_dev))
  config_context = var.kube_context_name
}

provider "helm" {
  kubernetes {
    config_path    = abspath(pathexpand(var.kubeconfig_path_for_dev))
    config_context = var.kube_context_name
  }
}
provider "null" {}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/040-kafka-cluster/main.tf
module "kafka_cluster_dev" {
  source = "../../../../modules/kafka_cluster"

  kubeconfig_path         = abspath(pathexpand(var.kubeconfig_path))
  kube_context_name       = var.kube_context_name    // Pass the dev context name
  kafka_cr_namespace      = var.kafka_namespace_dev
  kafka_cr_yaml_file_path = var.kafka_cluster_cr_yaml_path_dev
  kafka_cr_cluster_name   = var.kafka_cluster_name_dev
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/040-kafka-cluster/outputs.tf
output "dev_kafka_cluster_name" {
  description = "Name of the Kafka cluster deployed in dev."
  value       = module.kafka_cluster_dev.cluster_name_applied
}

output "dev_kafka_cluster_namespace" {
  description = "Namespace of the Kafka cluster in dev."
  value       = module.kafka_cluster_dev.cluster_namespace_applied
}

output "dev_kafka_bootstrap_servers_plain" {
  description = "Internal Plaintext Bootstrap Servers for dev Kafka."
  value       = module.kafka_cluster_dev.bootstrap_servers_plain
}

output "dev_kafka_bootstrap_servers_tls" {
  description = "Internal TLS Bootstrap Servers for dev Kafka."
  value       = module.kafka_cluster_dev.bootstrap_servers_tls
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2230-web-search-adapter/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2230-web-search-adapter/variables.tf
# No vari# No variables needed as the path is static for this service definition.ables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2230-web-search-adapter/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-web-search-adapter-dev"
    config_path   = "~/.kube/config"
  }
}

module "web_search_adapter_deployment_dev" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the DEVELOPMENT overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/web-search-adapter/overlays/development"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2230-web-search-adapter/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the development web-search-adapter."
  value       = module.web_search_adapter_deployment_dev.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2210-agent-chassis/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2210-agent-chassis/variables.tf
# No# No variables needed as the path is static for this service definition. variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2210-agent-chassis/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-agent-chassis-dev"
    config_path   = "~/.kube/config"
  }
}

module "agent_chassis_deployment_dev" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the DEVELOPMENT overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/agent-chassis/overlays/development"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2210-agent-chassis/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the development agent-chassis."
  value       = module.agent_chassis_deployment_dev.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2240-image-generator-adapter/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2240-image-generator-adapter/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2240-image-generator-adapter/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-image-generator-adapter-dev"
    config_path   = "~/.kube/config"
  }
}

module "image_generator_adapter_deployment_dev" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the DEVELOPMENT overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/image-generator-adapter/overlays/development"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2240-image-generator-adapter/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the development image-generator-adapter."
  value       = module.image_generator_adapter_deployment_dev.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2220-reasoning-agent/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2220-reasoning-agent/variables.tf
# No variables nee# No variables needed as the path is static for this service definition.ded as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2220-reasoning-agent/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-reasoning-agent-dev"
    config_path   = "~/.kube/config"
  }
}

module "reasoning_agent_deployment_dev" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the DEVELOPMENT overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/reasoning-agent/overlays/development"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/agents/2220-reasoning-agent/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the development reasoning-agent."
  value       = module.reasoning_agent_deployment_dev.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3330-agent-playground/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3330-agent-playground/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3330-agent-playground/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3330-agent-playground/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3320-user-portal/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3320-user-portal/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3320-user-portal/main.tf
# This instance deploys the main user-facing React application.
module "user_frontend_app" {
  source = "../../../../../../modules/kustomize-apply"

  service_name     = "user-frontend"
  namespace        = "personae-system"
  image_repository = "aqls/personae-web-interface" # Your frontend image
  image_tag        = var.image_tag

  # Point to the production Kustomize overlay for the frontend.
  # This directory would contain the deployment.yaml, service.yaml, ingress.yaml, etc.
  kustomize_path = "../../../../../../../deployments/kustomize/frontends/user-frontend/overlays/production"
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3320-user-portal/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3310-admin-dashboard/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3310-admin-dashboard/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3310-admin-dashboard/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/frontends/3310-admin-dashboard/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/core-platform/1120-core-manager/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/core-platform/1120-core-manager/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/core-platform/1120-core-manager/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-core-manager-dev"
    config_path   = "~/.kube/config"
  }
}

module "core_manager_deployment_dev" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the DEVELOPMENT overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/core-manager/overlays/development"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/core-platform/1120-core-manager/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the development core-manager."
  value       = module.core_manager_deployment_dev.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/core-platform/1110-auth-service/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/core-platform/1110-auth-service/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/core-platform/1110-auth-service/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-auth-dev"
    config_path   = "~/.kube/config"
  }
}

module "auth_service_deployment_dev" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the DEVELOPMENT overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/auth-service/overlays/development"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/uk_dev/services/core-platform/1110-auth-service/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the development auth-service."
  value       = module.auth_service_deployment_dev.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/development/local/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/local/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/terraform.tfvars
# terraform/environments/production/uk001/020-ingress-nginx/terraform.tfvars

// ingress_namespace = "custom-ingress" // Optional override
// ingress_helm_chart_version = "x.y.z"-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/variables.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/020-ingress-nginx/variables.tf
variable "ingress_helm_chart_version_override" {
  description = "Specific NGINX Ingress Helm chart version for Sydney (optional)."
  type        = string
  default     = null # Module will use its default if this is null
}

variable "ingress_custom_values_yaml_path" {
  description = "Path to a custom Helm values YAML file for NGINX Ingress for Sydney."
  type        = string
  # Path from this TF config to the actual YAML file in your modules directory
  default     = "../../../../modules/nginx_ingress/config/ingress-nginx-values.yaml" # ADJUST THIS PATH
}

variable "ingress_target_namespace" {
  description = "Target namespace for NGINX Ingress in Sydney."
  type        = string
  default     = "ingress-nginx"
}

variable "kubeconfig_path" {
  description = "Path to the kubeconfig file for the target Kubernetes cluster. This is typically set by the Makefile."
  type        = string
  sensitive   = true
  # No default is needed as the Makefile will pass it.
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/providers.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/020-ingress-nginx/providers.tf
provider "kubernetes" {
  config_path = var.kubeconfig_path
}

provider "helm" {
  kubernetes {
    config_path = var.kubeconfig_path
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/main.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/020-ingress-nginx/main.tf
module "nginx_ingress" {
  source = "../../../../modules/nginx_ingress" # Relative path to your new reusable module

  ingress_namespace    = var.ingress_target_namespace
  helm_chart_version = var.ingress_helm_chart_version_override == null ? null : var.ingress_helm_chart_version_override # Pass override or let module use default
  helm_values_content  = fileexists(var.ingress_custom_values_yaml_path) ? file(var.ingress_custom_values_yaml_path) : ""
  # create_namespace     = true # Or false if namespace is created elsewhere (e.g., by operator config)
  # If the namespace is also defined in 030-strimzi-operator for watched ns,
  # set create_namespace = false here and add depends_on to ensure it exists.
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/outputs.tf
# terraform/environments/production/uk001/020-ingress-nginx/outputs.tf

output "ingress_loadbalancer_ip" {
  description = "External IP of the NGINX Ingress for uk001."
  value       = module.nginx_ingress.loadbalancer_ip
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/versions.tf
# terraform/environments/production/uk001/020-ingress-nginx/versions.tf

terraform {
  required_providers {
    kubernetes = { source = "hashicorp/kubernetes", version = "~> 2.36.0" }
    helm       = { source = "hashicorp/helm", version = "~> 2.17.0" }
  }
  required_version = ">= 1.0"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/070-database-schemas/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/070-database-schemas/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/070-database-schemas/main.tf
terraform {
  required_providers {
    postgresql = {
      source  = "cyrilgdn/postgresql"
      version = "~> 1.20.0"
    }
    # We don't need a MySQL provider if we use the job runner for it.
  }

  backend "kubernetes" {
    secret_suffix = "tfstate-db-schemas"
    config_path   = "~/.kube/config"
  }
}

# Read the outputs from the database creation layer
data "terraform_remote_state" "databases" {
  backend = "kubernetes"
  config = {
    secret_suffix = "tfstate-databases"
    config_path   = "~/.kube/config"
  }
}

# --- PostgreSQL Provider for the 'templates' database ---
provider "postgresql" {
  alias    = "templates_db_provider"
  host     = data.terraform_remote_state.databases.outputs.postgres_templates_db_endpoint
  port     = 5432
  database = "templatesdb"
  username = "templates_user"
  password = data.terraform_remote_state.databases.outputs.templates_db_password
  sslmode  = "disable" # Change to "require" if you configure SSL
}

# --- PostgreSQL Provider for the 'clients' database ---
provider "postgresql" {
  alias    = "clients_db_provider"
  host     = data.terraform_remote_state.databases.outputs.postgres_clients_db_endpoint
  port     = 5432
  database = "clientsdb"
  username = "clients_user"
  password = data.terraform_remote_state.databases.outputs.clients_db_password
  sslmode  = "disable" # Change to "require" if you configure SSL
}

# Read the SQL file for the pgvector extension
data "local_file" "pgvector_sql" {
  filename = "${path.module}/../../../../sql/001_enable_pgvector.sql" # Assuming this path
}

# Apply the pgvector extension to the clients database
resource "postgresql_query" "pgvector_extension" {
  provider = postgresql.clients_db_provider
  query    = data.local_file.pgvector_sql.content
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/080-kafka-topics/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/080-kafka-topics/variables.tf
variable "platform_topics" {
  description = "A list of Kafka topic names to be created for the application."
  type        = list(string)
  default = [
    # System & Orchestration Topics
    "system.commands.workflow.resume",
    "system.events.workflow.paused",
    "system.events.workflow.completed",

    # Core Service Topics
    "requests.auth.user.create",
    "events.auth.user.created",

    # Agent Communication Topics
    "requests.agent.task.execute",
    "events.agent.task.completed",
    "events.agent.task.failed",
    "events.agent.task.progress",

    # Specialized Agent Topics
    "requests.agent.reasoning",
    "requests.agent.web-search",
    "requests.agent.image-generation",
  ]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/080-kafka-topics/main.tf
terraform {
  required_providers {
    kafka = {
      source  = "mongey/kafka"
      version = "~> 0.11.0"
    }
  }
  backend "kubernetes" {
    secret_suffix = "tfstate-kafka-topics"
    config_path   = "~/.kube/config"
  }
}

# Read the outputs from the Kafka cluster layer
data "terraform_remote_state" "kafka_cluster" {
  backend = "kubernetes"
  config = {
    secret_suffix = "tfstate-kafka-cluster" # Assuming this is the suffix used in 040
    config_path   = "~/.kube/config"
  }
}

provider "kafka" {
  bootstrap_servers = data.terraform_remote_state.kafka_cluster.outputs.kafka_bootstrap_servers
  # Add TLS/SASL config here if your production Kafka cluster requires it
}

# Define all required Kafka topics for the platform
resource "kafka_topic" "topics" {
  for_each = toset(var.platform_topics)

  name               = each.key
  partitions         = 3  # A good starting point for production
  replication_factor = 3  # Should match the number of Kafka brokers for resilience
  config = {
    "retention.ms" = "-1" # Retain messages indefinitely by default
    "cleanup.policy" = "compact,delete"
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/080-kafka-topics/outputs.tf
output "topic_names" {
  description = "The names of the created Kafka topics."
  value       = [for topic in kafka_topic.topics : topic.name]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/terraform.tfvars
# terraform/environments/production/uk/uk001/010-infrastructure/terraform.tfvars

instance_cluster_name           = "uk001-prod-cluster"
instance_rackspace_region       = "uk-lon-1"
instance_ondemand_node_flavor = "mh.vs1.medium-lon"
instance_spot_node_flavor     = "gp.vs1.large-lon" #mh.vs1.medium-lon has much more memory gp. is general
instance_slack_webhook_url    = "https://hooks.slack.com/services/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"
instance_ondemand_node_count = 0
instance_spot_min_nodes = 3
instance_spot_max_nodes = 6

# general purpose
# gp.vs1.medium-lon
# gp.vs1.large-lon
# gp.vs1.xlarge-lon
# gp.vs1.2xlarge-lon

# memory
# mh.vs1.medium-lon
# mh.vs1.large-lon
# mh.vs1.xlarge-lon
# mh.vs1.2xlarge-lon

# gp.bm2.medium-lon
# gp.bm2.large-lon

# ch.vs1.medium-lon
# ch.vs1.large-lon
# ch.vs1.xlarge-lon
# ch.vs1.2xlarge-lon





-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/variables.tf
variable "rackspace_spot_token" {
  description = "Rackspace Spot API token for this environment."
  type        = string
  sensitive   = true
  # No default, provide via terraform.tfvars.secret or TF_VAR_... env var
}

variable "instance_cluster_name" { // Renamed to avoid conflict with module input name
  description = "Specific name for this instance of the Kubernetes cluster (e.g., sydney-prod-k8s)."
  type        = string
}

variable "instance_rackspace_region" { // Renamed
  description = "Specific Rackspace region for this instance (e.g., aus-syd-1)."
  type        = string
}

variable "instance_ondemand_node_flavor" { // Renamed
  description = "Flavor for on-demand nodes for this instance."
  type        = string
}

variable "instance_spot_node_flavor" { // Renamed
  description = "Flavor for spot nodes for this instance."
  type        = string
}

variable "instance_slack_webhook_url" { // Renamed
  description = "Slack webhook URL for preemption notices for this instance (optional)."
  type        = string
  sensitive   = true
  default     = null
}

variable "instance_ondemand_node_taints" {
  description = "Taints for on-demand nodes for this uk001 instance."
  type = list(object({
    key    = string
    value  = string
    effect = string
  }))
  default = []
}

variable "instance_ondemand_node_count" {
  description = "No. of on-demand instances for this uk001 deployment"
  type = number
  default = 2
}

variable "instance_spot_min_nodes" {
  description = "Min number of spot nodes for uk001 deployment"
  type = number
  default = 3
}

variable "instance_spot_max_nodes" {
  description = "Min number of spot nodes for uk001 deployment"
  type = number
  default = 4
}

variable "instance_spot_max_price" {
  description = "Max price for spot nodes in uk001 deployment"
  type = number
  default = 0.035
}

// Add variables here if you want to override module defaults for this instance
// e.g., var.instance_k8s_version, var.instance_ondemand_node_count-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/terraform.tfstate.uk001-prod-cluster
{
  "version": 4,
  "terraform_version": "1.12.0",
  "serial": 19,
  "lineage": "f13659cb-8dfd-565a-e5bc-d61834baeb9b",
  "outputs": {
    "cluster_endpoint": {
      "value": "https://hcp-8750ffa8-7ad8-40e7-973d-b33c79cd1a11.spot.rackspace.com/",
      "type": "string",
      "sensitive": true
    },
    "cluster_kubeconfig_raw": {
      "value": "apiVersion: v1\nclusters:\n  - cluster:\n      insecure-skip-tls-verify: true\n      server: \u003e-\n        https://hcp-8750ffa8-7ad8-40e7-973d-b33c79cd1a11.spot.rackspace.com/\n    name: uk001-prod-cluster\ncontexts:\n  - context:\n      cluster: uk001-prod-cluster\n      namespace: default\n      user: ngpc-user\n    name: personae-uk001-prod-cluster\n  - context:\n      cluster: uk001-prod-cluster\n      namespace: default\n      user: oidc\n    name: personae-uk001-prod-cluster-oidc\ncurrent-context: personae-uk001-prod-cluster\nkind: Config\npreferences: {}\nusers:\n  - name: ngpc-user\n    user:\n      token: \u003e-\n        eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IkNYQ25nd1NFS0JWQl9DbWxvalo0eCJ9.eyJncm91cCI6ImNsb3Vkc3BhY2UtYWRtaW4iLCJuaWNrbmFtZSI6ImFhYSIsIm5hbWUiOiJhYWFAZGVzaWduY29uc3VsdGFuY3kuY28udWsiLCJwaWN0dXJlIjoiaHR0cHM6Ly9zLmdyYXZhdGFyLmNvbS9hdmF0YXIvYzRhNDgzZjBiNTA0NmFmZGEwMzUyM2I4MDBjOTFiYzc_cz00ODAmcj1wZyZkPWh0dHBzJTNBJTJGJTJGY2RuLmF1dGgwLmNvbSUyRmF2YXRhcnMlMkZhYS5wbmciLCJ1cGRhdGVkX2F0IjoiMjAyNS0wNS0yN1QxMjo1NjoxMy42MDlaIiwiZW1haWwiOiJhYWFAZGVzaWduY29uc3VsdGFuY3kuY28udWsiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaXNzIjoiaHR0cHM6Ly9sb2dpbi5zcG90LnJhY2tzcGFjZS5jb20vIiwiYXVkIjoibXdHM2xVTVY4S3llTXFIZTRmSjVCYjNuTTF2QnZSTmEiLCJzdWIiOiJhdXRoMHw2N2U2Zjg1NDZmYzVkM2I4ZDVhMTRiMWQiLCJpYXQiOjE3NDg1MTY4ODUsImV4cCI6MTc0ODc3NjA4NSwic2lkIjoiU2tLUGp5dUQyWFc5ZkVHbFRNZ1hFZE1VeWhydlNPclciLCJvcmdfaWQiOiJvcmdfSmp5alptb3hWTElYUlZEaiJ9.mWZ8IdFZZPP87S0Peiul6WqJDIfksilmNcLdbM0oHDjfI7JrqrY-bK0G1Uxg4RBhCVyEN1lE5h5IKT-S4UXHtvTZfSal26weGw2Rd-9sdlbHdmwI3eOQ_W-Nb_g9jynJBLJdFLFxiKG1IqvV40LL0UDdaRKfW6JXvN4Er1mWNggOJtTv9cpHsK4DQo8YL8xo6O5Eq4KwKVgxIk6mRuFTz7n1IB_w_54AIItQHnUhFmpdTi0qKJVurCfZFiCM-s2D0BZ5-lLlrYV680UX1QAN9QuglaIt35miXgBcQTcAknjnmnv2GzDgHszPD-XxT0S7WDoKz62jNHNc-eF5ffPEAg\n  - name: oidc\n    user:\n      exec:\n        apiVersion: client.authentication.k8s.io/v1beta1\n        args:\n          - oidc-login\n          - get-token\n          - '--oidc-issuer-url=https://login.spot.rackspace.com/'\n          - '--oidc-client-id=mwG3lUMV8KyeMqHe4fJ5Bb3nM1vBvRNa'\n          - '--oidc-extra-scope=openid'\n          - '--oidc-extra-scope=profile'\n          - '--oidc-extra-scope=email'\n          - '--oidc-auth-request-extra-params=organization=org_JjyjZmoxVLIXRVDj'\n          - '--token-cache-dir=~/.kube/cache/oidc-login/org_JjyjZmoxVLIXRVDj'\n        command: kubectl\n        env: null\n        interactiveMode: IfAvailable\n        provideClusterInfo: false\n",
      "type": "string",
      "sensitive": true
    },
    "cluster_name": {
      "value": "uk001-prod-cluster",
      "type": "string"
    },
    "ingress_controller_node_external_ips": {
      "value": [
        "134.213.222.28",
        "134.213.222.34",
        "134.213.222.35",
        "134.213.222.32"
      ],
      "type": [
        "tuple",
        [
          "string",
          "string",
          "string",
          "string"
        ]
      ]
    }
  },
  "resources": [
    {
      "module": "module.kubernetes_cluster",
      "mode": "data",
      "type": "spot_serverclasses",
      "name": "available_flavors",
      "provider": "provider[\"registry.terraform.io/rackerlabs/spot\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filters": null,
            "id": null,
            "names": [
              "gp.vs1.small-dfw",
              "gp.vs1.small-iad",
              "gp.bm2.large-dfw",
              "gp.bm2.small-lon",
              "gp.bm2.small-iad",
              "gp.bm2.medium-dfw",
              "gp.bm2.large-lon",
              "gp.bm2.medium-lon",
              "io.bm2-lon",
              "io.bm2-iad",
              "ch.vs2.medium-dfw2",
              "mh.vs2.xlarge-dfw2",
              "ch.vs1.medium-hkg",
              "ch.vs2.large-sjc",
              "ch.vs2.xlarge-sjc",
              "gp.vs2.2xlarge-sjc",
              "gp.vs2.large-sjc",
              "gp.vs2.xlarge-sjc",
              "mh.vs2.large-sjc",
              "mh.vs2.medium-sjc",
              "mh.vs2.xlarge-sjc",
              "mh.vs1.medium-syd",
              "gpu.vs2.megaxlarge-sjc",
              "mh.vs2.medium-dfw2",
              "ch.vs1.large-syd",
              "mh.vs1.2xlarge-syd",
              "ch.vs1.large-hkg",
              "ch.vs1.medium-ord",
              "mh.vs1.medium-ord",
              "ch.vs1.large-ord",
              "mh.vs1.large-ord",
              "gp.vs1.large-hkg",
              "gp.vs1.xlarge-lon",
              "ch.vs1.2xlarge-hkg",
              "mh.vs1.2xlarge-hkg",
              "gp.vs1.2xlarge-hkg",
              "ch.vs1.2xlarge-lon",
              "ch.vs1.xlarge-ord",
              "gp.vs1.2xlarge-iad",
              "mh.vs1.2xlarge-ord",
              "gp.vs1.medium-ord",
              "ch.vs1.2xlarge-syd",
              "mh.vs1.xlarge-syd",
              "mh.vs1.2xlarge-lon",
              "ch.vs1.medium-iad",
              "mh.vs1.medium-iad",
              "ch.vs1.2xlarge-ord",
              "gp.vs1.xlarge-syd",
              "ch.vs1.xlarge-hkg",
              "gp.vs1.2xlarge-ord",
              "ch.vs1.xlarge-syd",
              "ch.vs1.2xlarge-dfw",
              "gp.vs1.2xlarge-syd",
              "gp.vs1.2xlarge-dfw",
              "mh.vs1.xlarge-ord",
              "mh.vs1.medium-hkg",
              "mh.vs1.xlarge-hkg",
              "mh.vs1.large-hkg",
              "gp.vs1.medium-hkg",
              "ch.vs1.medium-syd",
              "mh.vs1.large-syd",
              "gp.vs1.medium-syd",
              "mh.vs1.2xlarge-dfw",
              "mh.vs1.medium-dfw",
              "ch.vs1.medium-dfw",
              "gp.vs2.2xlarge-dfw2",
              "ch.vs2.medium-sjc",
              "gpu.vs2.xlargeplusplus-sjc",
              "ch.vs1.large-lon",
              "gp.vs1.medium-lon",
              "ch.vs1.medium-lon",
              "gp.vs1.large-ord",
              "gp.vs1.xlarge-ord",
              "gp.vs1.large-syd",
              "ch.vs2.xlarge-dfw2",
              "ch.vs2.large-dfw2",
              "mh.vs1.xlarge-dfw",
              "ch.vs1.xlarge-dfw",
              "mh.vs1.medium-lon",
              "ch.vs1.xlarge-lon",
              "gp.vs1.2xlarge-lon",
              "gp.vs1.large-dfw",
              "gp.vs1.xlarge-iad",
              "mh.vs2.large-dfw2",
              "mh.vs1.large-iad",
              "gp.vs1.large-iad",
              "gp.vs2.xlarge-dfw2",
              "gp.vs1.medium-iad",
              "gp.vs2.large-dfw2",
              "mh.vs1.xlarge-lon",
              "mh.vs1.large-lon",
              "gp.vs2.medium-dfw2",
              "gp.vs2.medium-sjc",
              "gp.vs1.xlarge-hkg",
              "gp.vs1.medium-dfw",
              "mh.vs1.large-dfw",
              "ch.vs1.large-dfw",
              "ch.vs1.xlarge-iad",
              "mh.vs1.xlarge-iad",
              "gp.vs1.large-lon",
              "ch.vs1.2xlarge-iad",
              "mh.vs1.2xlarge-iad",
              "ch.vs1.large-iad",
              "gp.vs1.xlarge-dfw"
            ]
          },
          "sensitive_attributes": [],
          "identity_schema_version": 0
        }
      ]
    },
    {
      "module": "module.kubernetes_cluster",
      "mode": "managed",
      "type": "spot_cloudspace",
      "name": "cluster",
      "provider": "provider[\"registry.terraform.io/rackerlabs/spot\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "bids": [
              {
                "bid_name": "80c3f728-ed3a-4540-9fd7-ffda0cbeb728",
                "won_count": 4
              }
            ],
            "cloudspace_name": "uk001-prod-cluster",
            "cni": "calico",
            "deployment_type": "gen2",
            "first_ready_timestamp": "2025-05-28T16:29:41Z",
            "hacontrol_plane": false,
            "id": "uk001-prod-cluster",
            "kubernetes_version": "1.31.1",
            "last_updated": null,
            "name": "uk001-prod-cluster",
            "pending_allocations": null,
            "preemption_webhook": "https://hooks.slack.com/services/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",
            "region": "uk-lon-1",
            "spotnodepool_ids": [
              "80c3f728-ed3a-4540-9fd7-ffda0cbeb728"
            ],
            "timeouts": null,
            "wait_until_ready": null
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "preemption_webhook"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJyZXNvdXJjZV92ZXJzaW9uIjoiTVRnME1qZ3pNems9In0="
        }
      ]
    },
    {
      "module": "module.kubernetes_cluster",
      "mode": "managed",
      "type": "spot_ondemandnodepool",
      "name": "default_pool",
      "provider": "provider[\"registry.terraform.io/rackerlabs/spot\"]",
      "instances": []
    }
  ],
  "check_results": null
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/outputs_nodes.tf
# In terraform/environments/production/uk001/010-infrastructure/outputs_nodes.tf

# This provider block allows Terraform to connect to your newly created/existing
# Kubernetes cluster using the kubeconfig file that your 'infra-kubeconfig'
# Makefile target should be generating.
# Ensure KUBECONFIG_FILE in your Makefile points to the correct path for this environment.
provider "kubernetes" {
  alias = "k8s_cluster_access" # Alias to avoid conflict with other k8s provider configs if any
  config_path = abspath(pathexpand("~/.kube/config_production_uk001")) # Adjust if your Makefile saves it elsewhere
  # This assumes the kubeconfig exists from a previous apply.
}

data "kubernetes_nodes" "worker_nodes" {
  provider = kubernetes.k8s_cluster_access # Use the aliased provider

  # Optional: Filter for nodes that are expected to run Ingress controllers.
  # Your DaemonSet for ingress-nginx might have a nodeSelector or tolerations.
  # If it runs on all general worker nodes, you might select based on a common worker label.
  # Your nodes show "node-role.kubernetes.io/worker=".
  # If your Ingress DaemonSet is scheduled on all nodes with this role (and no other specific selector),
  # then this is appropriate. If your DaemonSet has a more specific nodeSelector (e.g., role=ingress-node),
  # you should match that here.
  metadata {
    labels = {
      # Adjust this label selector if your Ingress DaemonSet targets specific nodes.
      # If it runs on all schedulable worker nodes without a specific selector,
      # you might rely on all nodes returned or a general worker role.
      "node-role.kubernetes.io/worker" = ""
    }
  }
  # Ensure this data source depends on the cluster being fully provisioned.
  # The provider configuration using the kubeconfig implies this dependency.
  depends_on = [module.kubernetes_cluster]
}

output "ingress_controller_node_external_ips" {
  description = "List of external IP addresses (labelled as INTERNAL in Rackspace) for Kubernetes nodes expected to run Ingress controllers."
  value = [
    for node in data.kubernetes_nodes.worker_nodes.nodes :
    one([ # Use one() to ensure exactly one ExternalIP is found per node, or fail if none/multiple
      for addr in node.status[0].addresses : addr.address if addr.type == "InternalIP"
    ]) if length([for addr in node.status[0].addresses : addr.address if addr.type == "InternalIP"]) > 0
    # This 'if' at the end of the list comprehension filters out nodes that might not have an ExternalIP
  ]
  depends_on = [data.kubernetes_nodes.worker_nodes]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/providers.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/010-infrastructure/providers.tf
provider "spot" {
  token = var.rackspace_spot_token
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/backend.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/backend.tf
# Configure Terraform state backend (e.g., local, S3, Terraform Cloud)
terraform {
  backend "local" {
    path = "terraform.tfstate.uk001-prod-cluster" # Specific state file for this instance
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/main.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/010-infrastructure/main.tf
module "kubernetes_cluster" { # Local name for this instance of the module
  source = "../../../../modules/kubernetes_cluster_rackspace" # Path to the reusable module

  # Map variables from this root module (010-infrastructure) to the module's input variables
  cluster_name         = var.instance_cluster_name
  rackspace_region     = var.instance_rackspace_region
  preemption_webhook_url = var.instance_slack_webhook_url
  ondemand_node_flavor = var.instance_ondemand_node_flavor
  spot_node_flavor     = var.instance_spot_node_flavor
  ondemand_node_taints   = var.instance_ondemand_node_taints
  ondemand_node_count = var.instance_ondemand_node_count
  spot_min_nodes = var.instance_spot_min_nodes
  spot_max_nodes = var.instance_spot_max_nodes
  spot_max_price = var.instance_spot_max_price

  # Pass values for other variables defined in the module's variables.tf
  # If the module has defaults for these, you only need to pass them if you want to override.
  # kubernetes_version   = "1.31.1" # Or use var.instance_k8s_version
  # ondemand_node_count  = 1
  # spot_min_nodes       = 1
  # spot_max_nodes       = 2
  # ... etc. for cni, hacontrol_plane, spot_max_price, labels


  # Ensure ondemand_node_count is set, likely via terraform.tfvars
  # For example, if you have 'instance_ondemand_node_count' in your root variables.tf:
  # ondemand_node_count    = var.instance_ondemand_node_count
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/outputs.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/010-infrastructure/outputs.tf
output "cluster_kubeconfig_raw" { // Used by Makefile
  description = "Raw Kubeconfig for the Sydney production cluster."
  value       = module.kubernetes_cluster.kubeconfig_raw
  sensitive   = true
}

output "cluster_endpoint" {
  description = "API Endpoint for the Sydney production cluster."
  value       = module.kubernetes_cluster.cluster_endpoint_actual
  sensitive   = true
}

output "cluster_name" {
  description = "Actual name of the created cluster."
  value       = module.kubernetes_cluster.cluster_name_actual
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/versions.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/010-infrastructure/versions.tf
terraform {
  required_providers {
    # Configure the Rackspace Spot provider
    spot = {
      source  = "rackerlabs/spot"
      version = "~> 0.1.4" # Match the version constraint in your module
    }
    # Add kubernetes and helm here if you plan to deploy services from this root module later
    # For now, just the spot provider is needed for the cluster module.
  }
  required_version = ">= 1.0"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/090-monitoring/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/090-monitoring/variables.tf
variable "monitoring_namespace" {
  description = "The Kubernetes namespace to deploy the monitoring stack into."
  type        = string
  default     = "monitoring"
}

variable "grafana_admin_password" {
  description = "The admin password for the Grafana dashboard."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/090-monitoring/main.tf
terraform {
  required_providers {
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.11.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.20"
    }
  }
  backend "kubernetes" {
    secret_suffix = "tfstate-monitoring"
    config_path   = "~/.kube/config"
  }
}

# Add the prometheus-community Helm repository
data "helm_repository" "prometheus_community" {
  name = "prometheus-community"
  url  = "https://prometheus-community.github.io/helm-charts"
}

# Create a dedicated namespace for monitoring tools
resource "kubernetes_namespace" "monitoring_ns" {
  metadata {
    name = var.monitoring_namespace
  }
}

# Deploy the kube-prometheus-stack Helm chart
resource "helm_release" "prometheus_stack" {
  name       = "prometheus-stack"
  repository = data.helm_repository.prometheus_community.metadata[0].name
  chart      = "kube-prometheus-stack"
  namespace  = kubernetes_namespace.monitoring_ns.metadata[0].name
  version    = "51.8.0" # Pin to a specific chart version for consistency

  values = [
    templatefile("${path.module}/values.yaml.tpl", {
      grafana_admin_password = var.grafana_admin_password
    })
  ]

  depends_on = [kubernetes_namespace.monitoring_ns]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/090-monitoring/values.yaml.tpl
# values.yaml.tpl
# See all possible values here:
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

# Grafana configuration
grafana:
  # Use the password from our variables.tf
  adminPassword: "${grafana_admin_password}"

  # To access Grafana, you'll typically set up an Ingress.
  # For now, we can expose it via a LoadBalancer for direct access.
  # In a real production setup, you would use an Ingress controller.
  service:
    type: LoadBalancer

# Prometheus configuration
prometheus:
  prometheusSpec:
    # Set retention for production workloads
    retention: 30d
    storageSpec:
      volumeClaimTemplate:
        spec:
          # Use your production storage class
          storageClassName: premium-storage
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/090-monitoring/outputs.tf
output "grafana_service_name" {
  description = "The name of the Grafana service."
  value       = "${helm_release.prometheus_stack.name}-grafana"
}

output "prometheus_service_name" {
  description = "The name of the Prometheus service."
  value       = "${helm_release.prometheus_stack.name}-prometheus"
}

output "alertmanager_service_name" {
  description = "The name of the Alertmanager service."
  value       = "${helm_release.prometheus_stack.name}-alertmanager"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/050-storage/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/050-storage/variables.tf
variable "region" {
  description = "The region where resources will be deployed."
  type        = string
}

variable "s3_use_path_style" {
  description = "Whether to use path-style addressing for S3. Set to true for MinIO."
  type        = bool
  default     = false
}

variable "image_bucket_name" {
  description = "Name for the bucket to store generated images."
  type        = string
  default     = "personae-prod-uk001-images"
}

variable "site_assets_bucket_name" {
  description = "Name for the bucket to store generated static site assets."
  type        = string
  default     = "personae-prod-uk001-site-assets"
}

variable "b2_application_key_id" {
  description = "The application key ID for Backblaze B2."
  type        = string
  sensitive   = true
}

variable "b2_application_key" {
  description = "The application key for Backblaze B2."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/050-storage/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix    = "tfstate-storage"
    config_path      = "~/.kube/config"
    # In a real CI/CD pipeline, you might use in_cluster_config = true
  }
}

provider "b2" {
  application_key_id = var.b2_application_key_id
  application_key    = var.b2_application_key
}

module "storage_buckets" {
  source = "../../../modules/s3-buckets"

  bucket_names = [
    var.image_bucket_name,
    var.site_assets_bucket_name
  ]

  tags = {
    environment = "production"
    region      = var.region
    managed_by  = "terraform"
  }
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/050-storage/outputs.tf
output "image_bucket_id" {
  description = "The ID of the image storage bucket."
  value       = module.storage_buckets.bucket_ids[var.image_bucket_name]
}

output "site_assets_bucket_id" {
  description = "The ID of the site assets storage bucket."
  value       = module.storage_buckets.bucket_ids[var.site_assets_bucket_name]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/README.md
deployment order...
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/060-databases/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/060-databases/variables.tf
variable "k8s_namespace" {
  description = "The Kubernetes namespace to deploy database resources into."
  type        = string
  default     = "personae-prod-db"
}

variable "postgres_storage_class" {
  description = "The name of the Kubernetes StorageClass to use for PostgreSQL volumes."
  type        = string
  # Note: Change this to your actual production storage class name.
  default     = "premium-storage"
}

# --- External MySQL Variables ---
variable "external_mysql_host" {
  description = "The endpoint for the external MySQL database."
  type        = string
  sensitive   = true
}

variable "external_mysql_password" {
  description = "Password for the external MySQL database."
  type        = string
  sensitive   = true
}

# --- In-Cluster PostgreSQL Variables ---
variable "templates_db_password" {
  description = "Password for the templates PostgreSQL database."
  type        = string
  sensitive   = true
}

variable "clients_db_password" {
  description = "Password for the multi-tenant clients PostgreSQL database."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/060-databases/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-databases"
    config_path   = "~/.kube/config"
  }
}

# --- External MySQL Secret ---
module "external_mysql_auth_db" {
  source = "../../../modules/mysql-instance"

  instance_name = "personae-prod-uk001-auth-db"
  namespace     = var.k8s_namespace
  db_host       = var.external_mysql_host
  database_name = "authservicedb"
  database_user = "auth_user"
  database_pass = var.external_mysql_password
}

# --- In-Cluster PostgreSQL for Templates ---
module "postgres_templates_db" {
  source = "../../../modules/postgres-instance"

  instance_name      = "postgres-templates"
  namespace          = var.k8s_namespace
  database_name      = "templatesdb"
  database_user      = "templates_user"
  database_pass      = var.templates_db_password
  storage_class_name = var.postgres_storage_class
  storage_size       = "5Gi"
}

# --- In-Cluster PostgreSQL for Client Data ---
module "postgres_clients_db" {
  source = "../../../modules/postgres-instance"

  instance_name      = "postgres-clients"
  namespace          = var.k8s_namespace
  database_name      = "clientsdb"
  database_user      = "clients_user"
  database_pass      = var.clients_db_password
  storage_class_name = var.postgres_storage_class
  storage_size       = "20Gi"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/060-databases/outputs.tf
output "mysql_auth_db_endpoint" {
  description = "The connection endpoint for the auth service MySQL database."
  value       = module.mysql_auth_db.db_instance_endpoint
}

output "postgres_templates_db_endpoint" {
  description = "The connection endpoint for the templates PostgreSQL database."
  value       = module.postgres_templates_db.db_instance_endpoint
}

output "postgres_clients_db_endpoint" {
  description = "The connection endpoint for the clients PostgreSQL database."
  value       = module.postgres_clients_db.db_instance_endpoint
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/terraform.tfvars
// kubeconfig_path = "/home/ant/.kube/config_production_uk001" // Will be set by Makefile
// strimzi_operator_target_namespace = "strimzi" // Using default
// watched_namespaces_for_sydney = ["kafka", "personae", "strimzi"] // Using default
// strimzi_yaml_bundle_path_for_sydney = "../../../../modules/strimzi_operator/strimzi-yaml-0.45.0/" // Using default-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/variables.tf
# terraform/environments/production/uk001/030-strimzi-operator/variables.tf
variable "kubeconfig_path" {
  description = "Path to the kubeconfig file for the uk001 cluster."
  type        = string
  sensitive   = true
}

variable "strimzi_operator_target_namespace" {
  description = "Namespace for the Strimzi operator in uk001."
  type        = string
  default     = "strimzi"
}

variable "watched_namespaces_for_uk001" { // Changed from _for_sydney
  description = "List of namespaces for the Strimzi operator to watch in uk001."
  type        = list(string)
  default     = ["kafka", "personae", "strimzi"] // Assuming same watched namespaces for now
}

variable "strimzi_yaml_bundle_path_for_uk001" { // Changed from _for_sydney
  description = "Path to the Strimzi YAML files directory for this instance."
  type        = string
  # This relative path should still be correct from the new uk001 directory
  default     = "../../../../modules/strimzi_operator/strimzi-yaml-0.45.0/"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/providers.tf
# terraform/environments/production/uk001/030-strimzi-operator/providers.tf

provider "kubernetes" {
  config_path = var.kubeconfig_path # Provided by Makefile or root tfvars
}
# No Helm provider needed if this component only uses kubectl apply via null_resourceer "helm" { kubernetes {} }-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/main.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/030-strimzi-operator/main.tf

# Define/Ensure the namespaces exist
resource "kubernetes_namespace" "operator_ns" {
  metadata {
    name = var.strimzi_operator_target_namespace # "strimzi"
  }
}
resource "kubernetes_namespace" "kafka_ns_for_watch" {
  # Ensure this namespace exists if it's in the watched list and not created elsewhere
  # Could also use a data source if creation is handled by a different TF config
  metadata {
    name = "kafka"
  }
}
resource "kubernetes_namespace" "personae_ns_for_watch" {
  metadata {
    name = "personae"
  }
}

module "strimzi_operator_service" {
  source = "../../../../modules/strimzi_operator" # Path to your reusable module

  operator_namespace    = kubernetes_namespace.operator_ns.metadata[0].name
  watched_namespaces_list = var.watched_namespaces_for_uk001
  strimzi_yaml_source_path = var.strimzi_yaml_bundle_path_for_uk001
  cluster_kubeconfig_path = var.kubeconfig_path # Module needs this for its local-exec
}

output "operator_namespace" {
  value = module.strimzi_operator_service.operator_namespace_used
}
output "watched_namespaces" {
  value = module.strimzi_operator_service.watched_namespaces_configured
}


resource "kubernetes_role" "strimzi_ingress_reader_kafka_ns" {
  metadata {
    name      = "strimzi-ingress-reader"
    namespace = "kafka" # Permissions within the 'kafka' namespace
  }
  rule {
    api_groups = ["networking.k8s.io"]
    resources  = ["ingresses"]
    verbs      = ["get", "list", "watch"]
  }
  # Ensure this depends on the kafka namespace existing if created by this TF
  depends_on = [kubernetes_namespace.kafka_ns_for_watch]
}

resource "kubernetes_role_binding" "strimzi_ingress_reader_kafka_ns_binding" {
  metadata {
    name      = "strimzi-ingress-reader-binding"
    namespace = "kafka" # RoleBinding in the 'kafka' namespace
  }
  subject {
    kind      = "ServiceAccount"
    name      = "strimzi-cluster-operator"
    namespace = "strimzi" # The SA is in the 'strimzi' namespace
  }
  role_ref {
    kind      = "Role"
    name      = kubernetes_role.strimzi_ingress_reader_kafka_ns.metadata[0].name
    api_group = "rbac.authorization.k8s.io"
  }
  depends_on = [kubernetes_role.strimzi_ingress_reader_kafka_ns]
}

# Explicitly create/manage the RoleBinding in the 'kafka' namespace.
# This binds the strimzi-cluster-operator SA (from 'strimzi' namespace)
# to the 'strimzi-cluster-operator-namespaced' ClusterRole
# for actions *within* the 'kafka' namespace.
resource "kubernetes_role_binding" "strimzi_operator_permissions_in_kafka_ns" {
  metadata {
    name      = "strimzi-cluster-operator-kafka-namespace-permissions" # A descriptive name
    namespace = "kafka"                                                # Binding is in the 'kafka' namespace
  }

  subject {
    kind      = "ServiceAccount"
    name      = "strimzi-cluster-operator"            # Name of the ServiceAccount
    namespace = var.strimzi_operator_target_namespace # Namespace of the SA (e.g., "strimzi")
  }

  role_ref {
    kind      = "ClusterRole"
    name      = "strimzi-cluster-operator-namespaced" # The ClusterRole that has the necessary permissions
    # (including for "roles" and "rolebindings")
    api_group = "rbac.authorization.k8s.io"
  }

  depends_on = [
    kubernetes_namespace.kafka_ns_for_watch,    // Ensures 'kafka' namespace exists
    module.strimzi_operator_service             // Ensures Strimzi operator YAMLs (which define SA and ClusterRole) are applied first
  ]
}

# Explicitly create/manage the RoleBinding in the 'personae' namespace.
resource "kubernetes_role_binding" "strimzi_operator_permissions_in_personae_ns" {
  metadata {
    name      = "strimzi-cluster-operator-personae-namespace-permissions"
    namespace = "personae" # Binding is in the 'personae' namespace
  }

  subject {
    kind      = "ServiceAccount"
    name      = "strimzi-cluster-operator"
    namespace = var.strimzi_operator_target_namespace
  }

  role_ref {
    kind      = "ClusterRole"
    name      = "strimzi-cluster-operator-namespaced"
    api_group = "rbac.authorization.k8s.io"
  }

  depends_on = [
    kubernetes_namespace.personae_ns_for_watch, // Ensures 'personae' namespace exists
    module.strimzi_operator_service
  ]
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/versions.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/uk001/030-strimzi-operator/versions.tf
terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.36.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "~> 3.2.4"
    }
  }
  required_version = ">= 1.0"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/terraform.tfvars
// kubeconfig_path = "/home/ant/.kube/config_production_uk001" // Makefile will pass this via -var
// target_kafka_namespace = "kafka" // Using default
// kafka_cluster_cr_yaml_path_uk001 = "../../../../projects/personae/config/kafka-cluster-zk.yaml" // Using default
// kafka_cluster_name_uk001 = "personae-kafka-cluster" // Using default-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/variables.tf
# terraform/environments/production/uk001/040-kafka-cluster/variables.tf
variable "kubeconfig_path" {
  description = "Path to the kubeconfig file for the uk001 cluster."
  type        = string
  sensitive   = true
}

variable "target_kafka_namespace" {
  description = "Namespace where the Kafka CR for uk001 will be deployed."
  type        = string
  default     = "kafka"
}

variable "kafka_cluster_cr_yaml_path_uk001" { // Changed from _sydney
  description = "Path to the Kafka CR YAML file for the uk001 instance."
  type        = string
  # This relative path should still correctly point to the shared module's config
  default     = "../../../../modules/kafka_cluster/config/kafka-cluster-cr.yaml"
}

variable "kafka_cluster_name_uk001" { // Changed from _sydney
  description = "The metadata.name of the Kafka cluster being deployed in uk001 (must match name in YAML)."
  type        = string
  default     = "personae-kafka-cluster" # Assuming you want to use the same Kafka cluster name internally
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/providers.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/sydney/040-kafka-cluster/providers.tf
provider "kubernetes" {
  config_path = var.kubeconfig_path # Will be provided by Makefile/tfvars
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/main.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/sydney/040-kafka-cluster/main.tf

module "kafka_cluster_service" {
  source = "../../../../modules/kafka_cluster" # Path to your reusable module

  kafka_cr_namespace      = var.target_kafka_namespace
  kafka_cr_yaml_file_path = var.kafka_cluster_cr_yaml_path_uk001
  kubeconfig_path = var.kubeconfig_path
  kafka_cr_cluster_name   = var.kafka_cluster_name_uk001
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/outputs.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/sydney/040-kafka-cluster/outputs.tf
output "deployed_kafka_cluster_name" {
  value = module.kafka_cluster_service.cluster_name_applied
}
output "deployed_kafka_cluster_namespace" {
  value = module.kafka_cluster_service.cluster_namespace_applied
}
output "kafka_bootstrap_servers_plain" {
  value = module.kafka_cluster_service.bootstrap_servers_plain
}
output "kafka_bootstrap_servers_tls" {
  value = module.kafka_cluster_service.bootstrap_servers_tls
}

output "cluster_context_name" {
  description = "The kubectl context name for the production cluster."
  value       = "personae-uk001-prod-cluster" // This is usually derived or is a known value from the kubeconfig
  // You can often find this in the data.spot_kubeconfig.cluster_kubeconfig.kubeconfigs[0].name
  // value = data.spot_kubeconfig.cluster_kubeconfig.kubeconfigs[0].name
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/versions.tf
# ~/projects/terraform/rackspace_generic/terraform/environments/production/sydney/040-kafka-cluster/versions.tf
terraform {
  required_providers {
    kubernetes = { # Even if module doesn't use it directly, root config might for data sources
      source  = "hashicorp/kubernetes"
      version = "~> 2.36.0"
    }
    null = { # Because the module uses null_resource
      source  = "hashicorp/null"
      version = "~> 3.2.4"
    }
  }
  required_version = ">= 1.0"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2230-web-search-adapter/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2230-web-search-adapter/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2230-web-search-adapter/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-web-search-adapter"
    config_path   = "~/.kube/config"
  }
}

module "web_search_adapter_deployment" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the production overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/web-search-adapter/overlays/production/uk_001"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2230-web-search-adapter/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the web-search-adapter."
  value       = module.web_search_adapter_deployment.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2210-agent-chassis/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2210-agent-chassis/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2210-agent-chassis/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-agent-chassis"
    config_path   = "~/.kube/config"
  }
}

module "agent_chassis_deployment" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the production overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/agent-chassis/overlays/production/uk_001"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2210-agent-chassis/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the agent-chassis."
  value       = module.agent_chassis_deployment.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2240-image-generator-adapter/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2240-image-generator-adapter/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2240-image-generator-adapter/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-image-generator-adapter"
    config_path   = "~/.kube/config"
  }
}

module "image_generator_adapter_deployment" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the production overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/image-generator-adapter/overlays/production/uk_001"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2240-image-generator-adapter/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the image-generator-adapter."
  value       = module.image_generator_adapter_deployment.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2220-reasoning-agent/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2220-reasoning-agent/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2220-reasoning-agent/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-reasoning-agent"
    config_path   = "~/.kube/config"
  }
}

module "reasoning_agent_deployment" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the production overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/reasoning-agent/overlays/production/uk_001"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2220-reasoning-agent/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the reasoning-agent."
  value       = module.reasoning_agent_deployment.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3330-agent-playground/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3330-agent-playground/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3330-agent-playground/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3330-agent-playground/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3320-user-portal/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3320-user-portal/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3320-user-portal/main.tf
# This instance deploys the main user-facing React application.
module "user_frontend_app" {
  source = "../../../../../../modules/kustomize-apply"

  service_name     = "user-frontend"
  namespace        = "personae-system"
  image_repository = "aqls/personae-web-interface" # Your frontend image
  image_tag        = var.image_tag

  # Point to the production Kustomize overlay for the frontend.
  # This directory would contain the deployment.yaml, service.yaml, ingress.yaml, etc.
  kustomize_path = "../../../../../../../deployments/kustomize/frontends/user-frontend/overlays/production"
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3320-user-portal/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3310-admin-dashboard/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3310-admin-dashboard/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3310-admin-dashboard/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3310-admin-dashboard/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1120-core-manager/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1120-core-manager/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1120-core-manager/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-core-manager"
    config_path   = "~/.kube/config"
  }
}

module "core_manager_deployment" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the production overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/core-manager/overlays/production/uk_001"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1120-core-manager/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the core-manager."
  value       = module.core_manager_deployment.status
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1110-auth-service/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1110-auth-service/variables.tf
# No variables needed as the path is static for this service definition.-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1110-auth-service/main.tf
terraform {
  backend "kubernetes" {
    secret_suffix = "tfstate-svc-auth"
    config_path   = "~/.kube/config"
  }
}

module "auth_service_deployment" {
  source = "../../../../../modules/kustomize-apply"

  # Path to the production overlay for this service
  kustomize_path = "../../../../../deployments/kustomize/services/auth-service/overlays/production/uk_001"
}-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1110-auth-service/outputs.tf
output "kustomize_apply_status" {
  description = "The status of the Kustomize deployment for the auth-service."
  value       = module.auth_service_deployment.status
}-------------------------------------------------
filepath = ./deployments/kustomize/base/configmap-common.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: common-config
  namespace: ai-persona-system
data:
  # Kafka configuration
  kafka_brokers: "kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092"

  # Database hosts
  clients_db_host: "postgres-clients"
  clients_db_port: "5432"
  clients_db_name: "clients_db"
  clients_db_user: "clients_user"

  templates_db_host: "postgres-templates"
  templates_db_port: "5432"
  templates_db_name: "templates_db"
  templates_db_user: "templates_user"

  auth_db_host: "mysql-auth"
  auth_db_port: "3306"
  auth_db_name: "auth_db"
  auth_db_user: "auth_user"

  # Object storage
  minio_endpoint: "http://minio:9000"
  minio_bucket: "agent-artifacts"

  # Service URLs
  core_manager_url: "http://core-manager:8088"
  auth_service_url: "http://auth-service:8081"

  # Observability
  tracing_endpoint: "otel-collector.monitoring.svc.cluster.local:4317"-------------------------------------------------
filepath = ./deployments/kustomize/base/network-policies.yaml
# Default deny all ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress

---
# Allow ingress from same namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector: {}

---
# Allow ingress from ingress controller
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-controller
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: auth-service
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8081

---
# Allow Prometheus scraping
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-prometheus
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090-------------------------------------------------
filepath = ./deployments/kustomize/base/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ai-persona-system
  labels:
    name: ai-persona-system
    monitoring: enabled-------------------------------------------------
filepath = ./deployments/kustomize/base/rbac-security.yaml
# FILE: k8s/rbac-security.yaml
# Service Account for applications
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ai-persona-app
  namespace: ai-persona-system
  labels:
    app: ai-persona-system

---
# Role for application access
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: ai-persona-system
  name: ai-persona-app-role
rules:
  # Allow reading secrets for configuration
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  # Allow reading configmaps
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list"]
  # Allow pod operations for health checks
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]

---
# Bind the role to the service account
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ai-persona-app-binding
  namespace: ai-persona-system
subjects:
  - kind: ServiceAccount
    name: ai-persona-app
    namespace: ai-persona-system
roleRef:
  kind: Role
  name: ai-persona-app-role
  apiGroup: rbac.authorization.k8s.io

---
# Pod Security Policy (if using older Kubernetes versions)
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: ai-persona-restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  runAsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'

---
# Network Policy - Default deny all ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress

---
# Network Policy - Allow same namespace communication
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector: {}

---
# Network Policy - Allow ingress controller access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-controller
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: auth-service
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8081

---
# Network Policy - Allow monitoring
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090

---
# Network Policy - Database access (only from specific apps)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: postgres-clients
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: core-manager
        - podSelector:
            matchLabels:
              app: agent-chassis
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 5432

---
# Network Policy - MySQL access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mysql-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: mysql-auth
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: auth-service
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 3306

---
# Network Policy - Kafka access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kafka-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: kafka
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: agent-chassis
        - podSelector:
            matchLabels:
              app: reasoning-agent
        - podSelector:
            matchLabels:
              app: image-generator-adapter
        - podSelector:
            matchLabels:
              app: web-search-adapter
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 9092-------------------------------------------------
filepath = ./deployments/kustomize/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/security/network-policy.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/security/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/monitoring/service-monitor.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/monitoring/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/monitoring/pod-monitor.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/kafka/kafka.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
  namespace: ai-persona-system
  labels:
    app: kafka
spec:
  ports:
    - port: 9092
      name: broker
    - port: 9093
      name: controller
  clusterIP: None
  selector:
    app: kafka

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-ui
  namespace: ai-persona-system
spec:
  ports:
    - port: 8080
      targetPort: 8080
  selector:
    app: kafka-ui
  type: ClusterIP

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: ai-persona-system
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:7.5.0
          ports:
            - containerPort: 9092
              name: broker
            - containerPort: 9093
              name: controller
          env:
            - name: KAFKA_NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_PROCESS_ROLES
              value: "broker,controller"
            - name: KAFKA_LISTENERS
              value: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
            - name: KAFKA_ADVERTISED_LISTENERS
              value: "PLAINTEXT://$(KAFKA_NODE_ID).kafka-headless:9092"
            - name: KAFKA_CONTROLLER_LISTENER_NAMES
              value: "CONTROLLER"
            - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
              value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
            - name: KAFKA_CONTROLLER_QUORUM_VOTERS
              value: "kafka-0@kafka-0.kafka-headless:9093,kafka-1@kafka-1.kafka-headless:9093,kafka-2@kafka-2.kafka-headless:9093"
            - name: KAFKA_LOG_DIRS
              value: "/var/lib/kafka/data"
            - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
              value: "false"
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "3"
            - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "3"
            - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
              value: "2"
            - name: KAFKA_DEFAULT_REPLICATION_FACTOR
              value: "3"
            - name: KAFKA_MIN_INSYNC_REPLICAS
              value: "2"
          volumeMounts:
            - name: kafka-storage
              mountPath: /var/lib/kafka/data
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
  volumeClaimTemplates:
    - metadata:
        name: kafka-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 10Gi

---
# Kafka UI for monitoring
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-ui
  namespace: ai-persona-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-ui
  template:
    metadata:
      labels:
        app: kafka-ui
    spec:
      containers:
        - name: kafka-ui
          image: provectuslabs/kafka-ui:latest
          ports:
            - containerPort: 8080
          env:
            - name: KAFKA_CLUSTERS_0_NAME
              value: "ai-persona-cluster"
            - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS
              value: "kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092"
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/kafka/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/configs/development/configmap-dev.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: personae-dev-config
data:
  # Development Endpoints (for in-cluster services)
  kafka_brokers: "personae-kafka-cluster-kafka-bootstrap.kafka.svc:9092"
  clients_db_host: "postgres-clients.personae-system.svc.cluster.local"
  templates_db_host: "postgres-templates.personae-system.svc.cluster.local"
  auth_db_host: "mysql-auth.personae-system.svc.cluster.local"
  minio_endpoint: "http://minio.personae-system.svc.cluster.local:9000"
  core_manager_url: "http://core-manager.personae-system.svc.cluster.local:8088"
  auth_service_url: "http://auth-service.personae-system.svc.cluster.local:8081"

  # Development Settings
  tracing_enabled: "false"
  log_level: "debug"
  go_env: "development"-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/configs/development/secrets-dev.yaml
apiVersion: v1
kind: Secret
metadata:
  name: personae-dev-secrets
type: Opaque
data:
  # Base64 encoded development passwords. E.g., echo -n 'dev-password' | base64
  clients-db-password: "ZGV2LXBhc3N3b3Jk"
  templates-db-password: "ZGV2LXBhc3N3b3Jk"
  auth-db-password: "ZGV2LXBhc3N3b3Jk"
  minio-access-key: "bWluaW8="      # "minio"
  secret-key: "bWluaW9hZG1pbg==" # "minioadmin"
  jwt-secret: "ZGV2LXNlY3JldA=="    # "dev-secret"
  # API keys can be dummy values for dev if not used
  stability-api-key: "bm90LWEtcmVhbC1rZXk="
  serp-api-key: "bm90LWEtcmVhbC1rZXk="
  anthropic-api-key: "bm90LWEtcmVhbC1rZXk="-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/configs/production/uk_001/configmap-prod-uk001.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/configs/production/uk_001/secrets-prod-uk001.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/prometheus.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: ai-persona-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - nodes/proxy
      - services
      - endpoints
      - pods
    verbs: ["get", "list", "watch"]
  - apiGroups:
      - extensions
    resources:
      - ingresses
    verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: ai-persona-system

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: ai-persona-system
  labels:
    app: prometheus
spec:
  ports:
    - port: 9090
      targetPort: 9090
      name: http
  selector:
    app: prometheus
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: ai-persona-system
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
        - name: prometheus
          image: prom/prometheus:latest
          args:
            - '--config.file=/etc/prometheus/prometheus.yml'
            - '--storage.tsdb.path=/prometheus/'
            - '--web.console.libraries=/usr/share/prometheus/console_libraries'
            - '--web.console.templates=/usr/share/prometheus/consoles'
            - '--web.enable-lifecycle'
          ports:
            - containerPort: 9090
              name: http
          volumeMounts:
            - name: prometheus-config
              mountPath: /etc/prometheus
            - name: prometheus-storage
              mountPath: /prometheus
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
      volumes:
        - name: prometheus-config
          configMap:
            name: prometheus-config
        - name: prometheus-storage
          emptyDir: {}-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/grafana-dashboard-configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/prometheus-config.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/grafana.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/minio/minio.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: ai-persona-system
  labels:
    app: minio
spec:
  ports:
    - port: 9000
      targetPort: 9000
      name: api
    - port: 9001
      targetPort: 9001
      name: console
  selector:
    app: minio
  type: ClusterIP

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
  namespace: ai-persona-system
spec:
  serviceName: minio
  replicas: 1
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
        - name: minio
          image: minio/minio:latest
          command:
            - /bin/sh
            - -c
          args:
            - |
              mkdir -p /data/agent-artifacts
              minio server /data --console-address :9001
          ports:
            - containerPort: 9000
              name: api
            - containerPort: 9001
              name: console
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: access-key
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: secret-key
            - name: MINIO_BROWSER
              value: "on"
          volumeMounts:
            - name: minio-storage
              mountPath: /data
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /minio/health/ready
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 20
  volumeClaimTemplates:
    - metadata:
        name: minio-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 20Gi-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/minio/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/postgres-templates/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/postgres-templates/postgres-templates.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-templates
  namespace: ai-persona-system
  labels:
    app: postgres-templates
spec:
  ports:
    - port: 5432
      targetPort: 5432
      name: postgres
  selector:
    app: postgres-templates
  type: ClusterIP
  clusterIP: None

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-templates
  namespace: ai-persona-system
spec:
  serviceName: postgres-templates
  replicas: 1
  selector:
    matchLabels:
      app: postgres-templates
  template:
    metadata:
      labels:
        app: postgres-templates
    spec:
      containers:
        - name: postgres
          image: postgres:16-alpine
          ports:
            - containerPort: 5432
              name: postgres
          env:
            - name: POSTGRES_DB
              value: "templates_db"
            - name: POSTGRES_USER
              value: "templates_user"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
            - name: PGDATA
              value: /var/lib/postgresql/data/pgdata
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - templates_user
            initialDelaySeconds: 30
            periodSeconds: 10
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 5Gi-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/postgres-clients/postgres-clients.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-clients
  namespace: ai-persona-system
  labels:
    app: postgres-clients
spec:
  ports:
    - port: 5432
      targetPort: 5432
      name: postgres
  selector:
    app: postgres-clients
  type: ClusterIP
  clusterIP: None  # Headless service for StatefulSet

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-clients
  namespace: ai-persona-system
spec:
  serviceName: postgres-clients
  replicas: 1
  selector:
    matchLabels:
      app: postgres-clients
  template:
    metadata:
      labels:
        app: postgres-clients
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9187"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: postgres
          image: pgvector/pgvector:pg16
          ports:
            - containerPort: 5432
              name: postgres
          env:
            - name: POSTGRES_DB
              value: "clients_db"
            - name: POSTGRES_USER
              value: "clients_user"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: PGDATA
              value: /var/lib/postgresql/data/pgdata
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - clients_user
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - clients_user
            initialDelaySeconds: 5
            periodSeconds: 5
        - name: postgres-exporter
          image: prometheuscommunity/postgres-exporter:latest
          ports:
            - containerPort: 9187
              name: metrics
          env:
            - name: DATA_SOURCE_NAME
              value: "postgresql://clients_user:$(POSTGRES_PASSWORD)@localhost:5432/clients_db?sslmode=disable"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 10Gi-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/postgres-clients/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/mysql-auth/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/mysql-auth/mysql-auth.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-auth
  namespace: ai-persona-system
  labels:
    app: mysql-auth
spec:
  ports:
    - port: 3306
      targetPort: 3306
      name: mysql
  selector:
    app: mysql-auth
  type: ClusterIP
  clusterIP: None

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-auth
  namespace: ai-persona-system
spec:
  serviceName: mysql-auth
  replicas: 1
  selector:
    matchLabels:
      app: mysql-auth
  template:
    metadata:
      labels:
        app: mysql-auth
    spec:
      containers:
        - name: mysql
          image: mysql:8.0
          ports:
            - containerPort: 3306
              name: mysql
          env:
            - name: MYSQL_DATABASE
              value: "auth_db"
            - name: MYSQL_USER
              value: "auth_user"
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: auth-db-password
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: mysql-root-password
          volumeMounts:
            - name: mysql-storage
              mountPath: /var/lib/mysql
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          livenessProbe:
            exec:
              command:
                - mysqladmin
                - ping
                - -h
                - localhost
            initialDelaySeconds: 30
            periodSeconds: 10
  volumeClaimTemplates:
    - metadata:
        name: mysql-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 5Gi-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: user-frontend-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - host: "personae.yourdomain.com" # This will be patched by the overlay
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: user-frontend
                port:
                  number: 80
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
resources:
  - deployment.yaml
  - service.yaml
  - ingress.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-frontend
  labels:
    app: user-frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: user-frontend
  template:
    metadata:
      labels:
        app: user-frontend
    spec:
      containers:
        - name: user-frontend
          image: user-frontend:IMAGE_TAG_PLACEHOLDER
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
              name: http
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: user-frontend
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: user-frontend
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/overlays/development/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/overlays/staging/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/overlays/production/ingress-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/ingress.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/deployment.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/overlays/development/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/overlays/staging/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/overlays/production/ingress-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/ingress.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/deployment.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/overlays/development/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/kafka-topics/job.yaml
# FILE: k8s/jobs/kafka-topics-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-topics-init
  namespace: ai-persona-system
  labels:
    app: kafka-topics-init
    component: initialization
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: kafka-topics-init
        component: initialization
    spec:
      restartPolicy: Never
      initContainers:
        # Wait for Kafka to be ready
        - name: wait-for-kafka
          image: confluentinc/cp-kafka:7.5.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for Kafka to be ready..."
              until kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list >/dev/null 2>&1; do
                echo "Kafka not ready, waiting..."
                sleep 5
              done
              echo "Kafka is ready!"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

      containers:
        - name: topic-creator
          image: confluentinc/cp-kafka:7.5.0
          command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "🔧 Creating Kafka topics..."
              
              # Function to create topic with error handling
              create_topic() {
                local topic_name=$1
                local partitions=$2
                local replication_factor=$3
                local description=$4
              
                echo "Creating topic: $topic_name ($description)"
                kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 \
                  --create \
                  --topic "$topic_name" \
                  --partitions "$partitions" \
                  --replication-factor "$replication_factor" \
                  --if-not-exists || {
                  echo "Failed to create topic: $topic_name"
                  return 1
                }
              }
              
              # System-level topics
              echo "📨 Creating system topics..."
              create_topic "orchestrator.state-changes" 12 1 "Orchestrator state change notifications"
              create_topic "human.approvals" 6 1 "Human approval workflow messages"
              create_topic "system.events" 3 1 "General system events"
              create_topic "system.notifications.ui" 3 1 "UI notifications"
              create_topic "system.commands.workflow.resume" 3 1 "Workflow resume commands"
              
              # Agent communication topics
              echo "🤖 Creating agent communication topics..."
              create_topic "system.agent.reasoning.process" 6 1 "Reasoning agent requests"
              create_topic "system.responses.reasoning" 6 1 "Reasoning agent responses"
              create_topic "system.adapter.image.generate" 3 1 "Image generation requests"
              create_topic "system.responses.image" 6 1 "Image generation responses"
              create_topic "system.adapter.web.search" 3 1 "Web search requests"
              create_topic "system.responses.websearch" 6 1 "Web search responses"
              
              # Generic agent chassis topics
              echo "🏗️ Creating generic agent topics..."
              create_topic "system.agent.generic.process" 6 1 "Generic agent chassis requests"
              create_topic "system.tasks.copywriter" 6 1 "Copywriter agent tasks"
              create_topic "system.tasks.researcher" 6 1 "Research agent tasks"
              create_topic "system.tasks.content-creator" 6 1 "Content creator tasks"
              create_topic "system.tasks.multimedia-creator" 6 1 "Multimedia creator tasks"
              
              # Response topics for agents
              create_topic "system.responses.copywriter" 6 1 "Copywriter responses"
              create_topic "system.responses.researcher" 6 1 "Research responses"
              create_topic "system.responses.content-creator" 6 1 "Content creator responses"
              create_topic "system.responses.multimedia-creator" 6 1 "Multimedia creator responses"
              
              # Dead letter queues
              echo "💀 Creating dead letter queue topics..."
              create_topic "dlq.reasoning-agent" 1 1 "Reasoning agent DLQ"
              create_topic "dlq.image-generator" 1 1 "Image generator DLQ"
              create_topic "dlq.web-search" 1 1 "Web search DLQ"
              create_topic "dlq.agent-chassis" 1 1 "Agent chassis DLQ"
              create_topic "dlq.orchestrator" 1 1 "Orchestrator DLQ"
              
              # Monitoring and logging topics
              echo "📊 Creating monitoring topics..."
              create_topic "system.metrics.agents" 3 1 "Agent performance metrics"
              create_topic "system.logs.errors" 3 1 "Error logs aggregation"
              create_topic "system.audit.actions" 6 1 "Audit trail for user actions"
              
              echo "✅ All Kafka topics created successfully!"
              
              # List all topics to verify
              echo "📋 Current topics:"
              kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list

          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "500m"-------------------------------------------------
filepath = ./deployments/kustomize/jobs/kafka-topics/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/data-seeder/job.yaml
# k8s/data-seeder-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: seed-default-data
  namespace: ai-persona-system
spec:
  template:
    spec:
      containers:
        - name: seeder
          image: ai-persona-system/data-seeder:latest
          env:
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password-------------------------------------------------
filepath = ./deployments/kustomize/jobs/data-seeder/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/schema-creator/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: create-client-schema
  namespace: ai-persona-system
spec:
  template:
    spec:
      containers:
        - name: schema-creator
          image: ai-persona-system/schema-creator:latest
          env:
            - name: CLIENT_ID
              value: "{{ .Values.clientId }}"
          command: ['/app/create-client-schema.sh']-------------------------------------------------
filepath = ./deployments/kustomize/jobs/schema-creator/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/database-init/job.yaml
# FILE: k8s/jobs/database-init-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: database-init
  namespace: ai-persona-system
  labels:
    app: database-init
    component: initialization
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: database-init
        component: initialization
    spec:
      restartPolicy: Never
      initContainers:
        # Wait for PostgreSQL clients database
        - name: wait-for-postgres-clients
          image: postgres:16-alpine
          command:
            - sh
            - -c
            - |
              echo "Waiting for PostgreSQL clients database..."
              until pg_isready -h postgres-clients -p 5432; do
                echo "PostgreSQL clients not ready, waiting..."
                sleep 2
              done
              echo "PostgreSQL clients is ready!"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Wait for PostgreSQL templates database
        - name: wait-for-postgres-templates
          image: postgres:16-alpine
          command:
            - sh
            - -c
            - |
              echo "Waiting for PostgreSQL templates database..."
              until pg_isready -h postgres-templates -p 5432; do
                echo "PostgreSQL templates not ready, waiting..."
                sleep 2
              done
              echo "PostgreSQL templates is ready!"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Wait for MySQL auth database
        - name: wait-for-mysql-auth
          image: mysql:8.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for MySQL auth database..."
              until mysqladmin ping -h mysql-auth --silent; do
                echo "MySQL auth not ready, waiting..."
                sleep 2
              done
              echo "MySQL auth is ready!"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

      containers:
        - name: database-migrator
          image: ai-persona-system/database-migrator:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
            - name: AUTH_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: auth-db-password
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: mysql-root-password
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          command: ["/app/run-migrations.sh"]

---
apiVersion: batch/v1
kind: Job
metadata:
  name: data-seeder
  namespace: ai-persona-system
  labels:
    app: data-seeder
    component: initialization
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: data-seeder
        component: initialization
    spec:
      restartPolicy: Never
      containers:
        - name: data-seeder
          image: ai-persona-system/data-seeder:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
            - name: AUTH_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: auth-db-password
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          command: ["/app/seed-data.sh"]-------------------------------------------------
filepath = ./deployments/kustomize/jobs/database-init/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/backup/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/backup/cronjob.yaml
# FILE: k8s/backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: database-backup
  namespace: ai-persona-system
  labels:
    app: database-backup
    component: backup
spec:
  # Run daily at 2 AM
  schedule: "0 2 * * *"
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        metadata:
          labels:
            app: database-backup
            component: backup
        spec:
          restartPolicy: OnFailure
          containers:
            - name: backup-postgresql
              image: postgres:16-alpine
              env:
                - name: CLIENTS_DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-secrets
                      key: clients-db-password
                - name: TEMPLATES_DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-secrets
                      key: templates-db-password
                - name: BACKUP_DATE
                  value: "$(date +%Y%m%d_%H%M%S)"
              command:
                - /bin/bash
                - -c
                - |
                  set -e
                  echo "Starting PostgreSQL backup..."
                  
                  # Create backup directory
                  mkdir -p /backup
                  
                  # Backup clients database
                  echo "Backing up clients database..."
                  export PGPASSWORD="$CLIENTS_DB_PASSWORD"
                  pg_dump -h postgres-clients -U clients_user -d clients_db \
                    --verbose --clean --if-exists --create \
                    > /backup/clients_db_$(date +%Y%m%d_%H%M%S).sql
                  
                  # Backup templates database
                  echo "Backing up templates database..."
                  export PGPASSWORD="$TEMPLATES_DB_PASSWORD"
                  pg_dump -h postgres-templates -U templates_user -d templates_db \
                    --verbose --clean --if-exists --create \
                    > /backup/templates_db_$(date +%Y%m%d_%H%M%S).sql
                  
                  echo "PostgreSQL backups completed!"
                  ls -la /backup/

              volumeMounts:
                - name: backup-storage
                  mountPath: /backup
              resources:
                requests:
                  memory: "256Mi"
                  cpu: "200m"
                limits:
                  memory: "512Mi"
                  cpu: "500m"

            - name: backup-mysql
              image: mysql:8.0
              env:
                - name: AUTH_DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-secrets
                      key: auth-db-password
              command:
                - /bin/bash
                - -c
                - |
                  set -e
                  echo "Starting MySQL backup..."
                  
                  # Create backup directory
                  mkdir -p /backup
                  
                  # Backup auth database
                  echo "Backing up auth database..."
                  mysqldump -h mysql-auth -u auth_user -p"$AUTH_DB_PASSWORD" \
                    --routines --triggers --single-transaction \
                    auth_db > /backup/auth_db_$(date +%Y%m%d_%H%M%S).sql
                  
                  echo "MySQL backup completed!"
                  ls -la /backup/

              volumeMounts:
                - name: backup-storage
                  mountPath: /backup
              resources:
                requests:
                  memory: "256Mi"
                  cpu: "200m"
                limits:
                  memory: "512Mi"
                  cpu: "500m"

            # Cleanup old backups (keep last 7 days)
            - name: cleanup-old-backups
              image: alpine:latest
              command:
                - /bin/sh
                - -c
                - |
                  echo "Cleaning up old backups (keeping last 7 days)..."
                  find /backup -name "*.sql" -type f -mtime +7 -delete
                  echo "Cleanup completed!"
                  echo "Current backups:"
                  ls -la /backup/

              volumeMounts:
                - name: backup-storage
                  mountPath: /backup
              resources:
                requests:
                  memory: "64Mi"
                  cpu: "100m"
                limits:
                  memory: "128Mi"
                  cpu: "200m"

          volumes:
            - name: backup-storage
              persistentVolumeClaim:
                claimName: backup-storage-pvc

---
# PVC for backup storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-storage-pvc
  namespace: ai-persona-system
  labels:
    app: database-backup
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  resources:
    requests:
      storage: 50Gi-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agent-chassis
  labels:
    app: agent-chassis
spec:
  selector:
    matchLabels:
      app: agent-chassis
  template:
    metadata:
      labels:
        app: agent-chassis
    spec:
      containers:
        - name: agent-chassis
          image: your-container-registry/agent-chassis:latest # Patched by overlays
          envFrom:
            - configMapRef:
                name: placeholder-config # Patched by overlays
            - secretRef:
                name: placeholder-secrets # Patched by overlays
          resources:
            requests:
              cpu: "200m"
              memory: "256Mi"
            limits:
              cpu: "1"
              memory: "1Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
bases:
  - ../../base

commonLabels:
  environment: development

resources:
  - ../../../../infrastructure/configs/development/configmap-dev.yaml
  - ../../../../infrastructure/configs/development/secrets-dev.yaml

patches:
  - path: patch-deployment-dev.yaml
    target:
      kind: Deployment
      name: agent-chassis-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/development/patch-deployment-dev.yaml
- op: replace
  path: /spec/replicas
  value: 2 # A couple of replicas even for dev
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/agent-chassis:latest
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-dev-config
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-dev-secrets
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "100m"
    memory: "128Mi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "500m"
    memory: "512Mi"-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/staging/resources-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/staging/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/staging/deployment-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/production/uk_001/patch-deployment.yaml
- op: replace
  path: /spec/replicas
  value: 10 # Scaled up for production workloads
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/agent-chassis:v1.2.0 # Specific version tag
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-prod-config-uk001
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-prod-secrets-uk001
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "500m"
    memory: "1Gi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "1.5"
    memory: "2Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/production/uk_001/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-prod
bases:
  - ../../../base

commonLabels:
  environment: production
  region: uk

resources:
  - ../../../../../infrastructure/configs/production/uk_001/configmap-prod-uk001.yaml
  - ../../../../../infrastructure/configs/production/uk_001/secrets-prod-uk001.yaml

patches:
  - path: patch-deployment.yaml
    target:
      kind: Deployment
      name: agent-chassis-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/README.md
Just like the reasoning agent, this is a specialized worker that communicates only via Kafka, so it only needs a Deployment manifest.-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
resources:
  - deployment.yaml
  - service.yaml
  # ConfigMaps and Secrets will be managed by Terraform to inject dynamic values-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
  labels:
    app: auth-service
spec:
  selector:
    matchLabels:
      app: auth-service
  template:
    metadata:
      labels:
        app: auth-service
    spec:
      containers:
        - name: auth-service
          image: your-container-registry/auth-service:latest # This will be patched by overlays
          ports:
            - containerPort: 8081
              name: http
            - containerPort: 9090
              name: grpc
          envFrom:
            - configMapRef:
                # Patched by overlays
                name: placeholder-config
            - secretRef:
                # Patched by overlays
                name: placeholder-secrets
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: auth-service
  labels:
    app: auth-service
spec:
  ports:
    - port: 8081
      targetPort: http
      name: http
    - port: 9090
      targetPort: grpc
      name: grpc
  selector:
    app: auth-service
  type: ClusterIP-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
bases:
  - ../../base

# Add development-specific labels
commonLabels:
  environment: development

# Point to the new centralized development config and secrets
resources:
  - ../../../../infrastructure/configs/development/configmap-dev.yaml
  - ../../../../infrastructure/configs/development/secrets-dev.yaml

patches:
  - path: patch-deployment-dev.yaml
    target:
      kind: Deployment
      name: auth-service-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/development/patch-deployment-dev.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/staging/resources-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/staging/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/staging/deployment-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/production/uk_001/configs/secrets-prod.yaml
apiVersion: v1
kind: Secret
metadata:
  name: personae-prod-secrets
  namespace: personae-prod
type: Opaque
data:
  # These values should be base64 encoded and injected by your CI/CD system from a secure vault.
  # Example: echo -n 'your-production-password' | base64
  clients-db-password: "YOUR_PROD_CLIENTS_DB_PASSWORD"
  templates-db-password: "YOUR_PROD_TEMPLATES_DB_PASSWORD"
  auth-db-password: "YOUR_PROD_AUTH_DB_PASSWORD"
  minio-access-key: "YOUR_PROD_MINIO_ACCESS_KEY"
  minio-secret-key: "YOUR_PROD_MINIO_SECRET_KEY"
  stability-api-key: "YOUR_PROD_STABILITY_API_KEY"
  serp-api-key: "YOUR_PROD_SERP_API_KEY"
  anthropic-api-key: "YOUR_PROD_ANTHROPIC_API_KEY"
  jwt-secret: "YOUR_PROD_JWT_SECRET" # A long, secure, base64-encoded random string-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/production/uk_001/configs/configmap-prod.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: personae-prod-config
  namespace: personae-prod
data:
  # Production Endpoints (replace with your actual endpoints)
  kafka_brokers: "your-production-kafka-bootstrap-endpoint:9092"
  clients_db_host: "your-production-postgres-clients-endpoint"
  templates_db_host: "your-production-postgres-templates-endpoint"
  auth_db_host: "your-production-mysql-auth-endpoint"
  minio_endpoint: "your-production-s3-compatible-endpoint"
  core_manager_url: "http://core-manager.personae-prod.svc.cluster.local:8088"
  auth_service_url: "http://auth-service.personae-prod.svc.cluster.local:8081"

  # Production Settings
  tracing_enabled: "true"
  tracing_endpoint: "otel-collector.monitoring.svc.cluster.local:4317"
  log_level: "info"
  go_env: "production"-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/production/uk_001/patch-deployment.yaml
- op: replace
  path: /spec/replicas
  value: 3 # Production scale
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/auth-service:v1.2.0 # Use a specific version tag for production
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-prod-config
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-prod-secrets
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "250m"
    memory: "512Mi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "1"
    memory: "1Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/production/uk_001/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-prod
bases:
  - ../../../base

# Add production-specific labels
commonLabels:
  environment: production
  region: uk

# Point to the new centralized production config and secrets for this region
resources:
  - ../../../../../infrastructure/configs/production/uk_001/configmap-prod-uk001.yaml
  - ../../../../../infrastructure/configs/production/uk_001/secrets-prod-uk001.yaml

patches:
  - path: patch-deployment.yaml
    target:
      kind: Deployment
      name: auth-service-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/production/uk_001/README.md
We can now apply this entire production configuration for the auth-service with a single command:

kubectl apply -k deployments/kustomize/services/auth-service/overlays/production/uk_001-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-search-adapter
  labels:
    app: web-search-adapter
spec:
  selector:
    matchLabels:
      app: web-search-adapter
  template:
    metadata:
      labels:
        app: web-search-adapter
    spec:
      containers:
        - name: web-search-adapter
          image: your-container-registry/web-search-adapter:latest # Patched by overlays
          envFrom:
            - configMapRef:
                name: placeholder-config # Patched by overlays
            - secretRef:
                name: placeholder-secrets # Patched by overlays
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
bases:
  - ../../base

commonLabels:
  environment: development

resources:
  - ../../../../infrastructure/configs/development/configmap-dev.yaml
  - ../../../../infrastructure/configs/development/secrets-dev.yaml

patches:
  - path: patch-deployment-dev.yaml
    target:
      kind: Deployment
      name: web-search-adapter-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/overlays/development/patch-deployment-dev.yaml
- op: replace
  path: /spec/replicas
  value: 1
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/web-search-adapter:latest
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-dev-config
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-dev-secrets-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/overlays/production/uk_001/patch-deployment.yaml
- op: replace
  path: /spec/replicas
  value: 3
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/web-search-adapter:v1.2.0 # Specific version tag
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-prod-config-uk001
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-prod-secrets-uk001
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "200m"
    memory: "256Mi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "1"
    memory: "1Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/overlays/production/uk_001/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-prod
bases:
  - ../../../base

commonLabels:
  environment: production
  region: uk

resources:
  - ../../../../../infrastructure/configs/production/uk_001/configmap-prod-uk001.yaml
  - ../../../../../infrastructure/configs/production/uk_001/secrets-prod-uk001.yaml

patches:
  - path: patch-deployment.yaml
    target:
      kind: Deployment
      name: web-search-adapter-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reasoning-agent
  labels:
    app: reasoning-agent
spec:
  selector:
    matchLabels:
      app: reasoning-agent
  template:
    metadata:
      labels:
        app: reasoning-agent
    spec:
      containers:
        - name: reasoning-agent
          image: your-container-registry/reasoning-agent:latest # Patched by overlays
          envFrom:
            - configMapRef:
                name: placeholder-config # Patched by overlays
            - secretRef:
                name: placeholder-secrets # Patched by overlays
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "1"
              memory: "2Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
bases:
  - ../../base

commonLabels:
  environment: development

resources:
  - ../../../../infrastructure/configs/development/configmap-dev.yaml
  - ../../../../infrastructure/configs/development/secrets-dev.yaml

patches:
  - path: patch-deployment-dev.yaml
    target:
      kind: Deployment
      name: reasoning-agent-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/development/patch-deployment-dev.yaml
- op: replace
  path: /spec/replicas
  value: 1 # Single replica for dev
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/reasoning-agent:latest
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-dev-config
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-dev-secrets
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "125m"
    memory: "256Mi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "500m"
    memory: "1Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/production/uk_991/patch-deployment.yaml
- op: replace
  path: /spec/replicas
  value: 3
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/reasoning-agent:v1.2.0 # Specific version tag
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-prod-config-uk001
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-prod-secrets-uk001
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "500m"
    memory: "1Gi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "1.5"
    memory: "4Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/production/uk_991/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-prod
bases:
  - ../../../base

commonLabels:
  environment: production
  region: uk

resources:
  - ../../../../../infrastructure/configs/production/uk_001/configmap-prod-uk001.yaml
  - ../../../../../infrastructure/configs/production/uk_001/secrets-prod-uk001.yaml

patches:
  - path: patch-deployment.yaml
    target:
      kind: Deployment
      name: reasoning-agent-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/README.md
Just like the agent-chassis, this is a specialized worker that communicates only via Kafka, so it only needs a Deployment manifest.-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - service.yaml-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: core-manager
  labels:
    app: core-manager
spec:
  selector:
    matchLabels:
      app: core-manager
  template:
    metadata:
      labels:
        app: core-manager
    spec:
      containers:
        - name: core-manager
          image: your-container-registry/core-manager:latest # Patched by overlays
          ports:
            - containerPort: 8088
              name: http
            - containerPort: 9091
              name: grpc
          envFrom:
            - configMapRef:
                name: placeholder-config # Patched by overlays
            - secretRef:
                name: placeholder-secrets # Patched by overlays
          resources:
            requests:
              cpu: "150m"
              memory: "256Mi"
            limits:
              cpu: "750m"
              memory: "1Gi"
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: core-manager
  labels:
    app: core-manager
spec:
  ports:
    - port: 8088
      targetPort: http
      name: http
    - port: 9091
      targetPort: grpc
      name: grpc
  selector:
    app: core-manager
  type: ClusterIP-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
bases:
  - ../../base

commonLabels:
  environment: development

resources:
  - ../../../../infrastructure/configs/development/configmap-dev.yaml
  - ../../../../infrastructure/configs/development/secrets-dev.yaml

patches:
  - path: patch-deployment-dev.yaml
    target:
      kind: Deployment
      name: core-manager-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/overlays/development/patch-deployment-dev.yaml
- op: replace
  path: /spec/replicas
  value: 1
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/core-manager:latest
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-dev-config
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-dev-secrets
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "75m"
    memory: "128Mi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "300m"
    memory: "512Mi"-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/overlays/production/uk_001/patch-deployment.yaml
- op: replace
  path: /spec/replicas
  value: 3
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/core-manager:v1.2.0 # Specific version tag
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-prod-config-uk001
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-prod-secrets-uk001
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "300m"
    memory: "512Mi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "1"
    memory: "1.5Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/overlays/production/uk_001/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-prod
bases:
  - ../../../base

commonLabels:
  environment: production
  region: uk

resources:
  - ../../../../../infrastructure/configs/production/uk_001/configmap-prod-uk001.yaml
  - ../../../../../infrastructure/configs/production/uk_001/secrets-prod-uk001.yaml

patches:
  - path: patch-deployment.yaml
    target:
      kind: Deployment
      name: core-manager-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: image-generator-adapter
  labels:
    app: image-generator-adapter
spec:
  selector:
    matchLabels:
      app: image-generator-adapter
  template:
    metadata:
      labels:
        app: image-generator-adapter
    spec:
      containers:
        - name: image-generator-adapter
          image: your-container-registry/image-generator-adapter:latest # Patched by overlays
          envFrom:
            - configMapRef:
                name: placeholder-config # Patched by overlays
            - secretRef:
                name: placeholder-secrets # Patched by overlays
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
bases:
  - ../../base

commonLabels:
  environment: development

resources:
  - ../../../../infrastructure/configs/development/configmap-dev.yaml
  - ../../../../infrastructure/configs/development/secrets-dev.yaml

patches:
  - path: patch-deployment-dev.yaml
    target:
      kind: Deployment
      name: image-generator-adapter-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/overlays/development/patch-deployment-dev.yaml
- op: replace
  path: /spec/replicas
  value: 1
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/image-generator-adapter:latest
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-dev-config
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-dev-secrets-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/overlays/production/uk_001/patch-deployment.yaml
- op: replace
  path: /spec/replicas
  value: 3
- op: replace
  path: /spec/template/spec/containers/0/image
  value: your-container-registry/image-generator-adapter:v1.2.0 # Specific version tag
- op: replace
  path: /spec/template/spec/containers/0/envFrom/0/configMapRef/name
  value: personae-prod-config-uk001
- op: replace
  path: /spec/template/spec/containers/0/envFrom/1/secretRef/name
  value: personae-prod-secrets-uk001
- op: replace
  path: /spec/template/spec/resources/requests
  value:
    cpu: "200m"
    memory: "256Mi"
- op: replace
  path: /spec/template/spec/resources/limits
  value:
    cpu: "1"
    memory: "1Gi"-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/overlays/production/uk_001/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-prod
bases:
  - ../../../base

commonLabels:
  environment: production
  region: uk

resources:
  - ../../../../../infrastructure/configs/production/uk_001/configmap-prod-uk001.yaml
  - ../../../../../infrastructure/configs/production/uk_001/secrets-prod-uk001.yaml

patches:
  - path: patch-deployment.yaml
    target:
      kind: Deployment
      name: image-generator-adapter-------------------------------------------------
filepath = ./deployments/docker-compose/docker-compose.swagger.yaml
version: '3.8'

services:
  # Swagger UI for viewing API documentation
  swagger-ui:
    image: swaggerapi/swagger-ui:latest
    container_name: personae-swagger-ui
    ports:
      - "8082:8080"
    volumes:
      - ./internal/auth-service/api/openapi.yaml:/openapi.yaml
    environment:
      SWAGGER_JSON: /openapi.yaml
      BASE_URL: /swagger
    networks:
      - personae-docs

  # Redocly for beautiful API documentation
  redocly:
    image: redocly/redoc:latest
    container_name: personae-redoc
    ports:
      - "8083:80"
    volumes:
      - ./internal/auth-service/api/openapi.yaml:/usr/share/nginx/html/openapi.yaml
    environment:
      SPEC_URL: openapi.yaml
    networks:
      - personae-docs

  # Swagger Editor for editing OpenAPI specs
  swagger-editor:
    image: swaggerapi/swagger-editor:latest
    container_name: personae-swagger-editor
    ports:
      - "8084:8080"
    environment:
      SWAGGER_FILE: /openapi.yaml
    volumes:
      - ./internal/auth-service/api/openapi.yaml:/openapi.yaml
    networks:
      - personae-docs

networks:
  personae-docs:
    driver: bridge-------------------------------------------------
filepath = ./deployments/docker-compose/.env.example
-------------------------------------------------
filepath = ./deployments/docker-compose/docker-compose.yaml
# FILE: docker-compose.yml
version: '3.8'

services:
  # Infrastructure
  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    depends_on:
      - zookeeper

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  postgres-clients:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: clients_db
      POSTGRES_USER: clients_user
      POSTGRES_PASSWORD: ${CLIENTS_DB_PASSWORD}
    volumes:
      - clients_data:/var/lib/postgresql/data

  postgres-templates:
    image: postgres:16
    environment:
      POSTGRES_DB: templates_db
      POSTGRES_USER: templates_user
      POSTGRES_PASSWORD: ${TEMPLATES_DB_PASSWORD}
    volumes:
      - templates_data:/var/lib/postgresql/data

  mysql-auth:
    image: mysql:8
    environment:
      MYSQL_DATABASE: auth_db
      MYSQL_USER: auth_user
      MYSQL_PASSWORD: ${AUTH_DB_PASSWORD}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    volumes:
      - auth_data:/var/lib/mysql

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
    volumes:
      - minio_data:/data

  # Core Services
  auth-service:
    build:
      context: .
      dockerfile: Dockerfile.auth-service
    environment:
      AUTH_DB_PASSWORD: ${AUTH_DB_PASSWORD}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
    depends_on:
      - mysql-auth
    ports:
      - "8081:8081"

  core-manager:
    build:
      context: .
      dockerfile: Dockerfile.core-manager
    environment:
      CLIENTS_DB_PASSWORD: ${CLIENTS_DB_PASSWORD}
      TEMPLATES_DB_PASSWORD: ${TEMPLATES_DB_PASSWORD}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    depends_on:
      - postgres-clients
      - postgres-templates
      - kafka
    ports:
      - "8088:8088"

  # Agent Services
  agent-chassis:
    build:
      context: .
      dockerfile: Dockerfile.agent-chassis
    environment:
      CLIENTS_DB_PASSWORD: ${CLIENTS_DB_PASSWORD}
      TEMPLATES_DB_PASSWORD: ${TEMPLATES_DB_PASSWORD}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    depends_on:
      - postgres-clients
      - kafka
    deploy:
      replicas: 3

  reasoning-agent:
    build:
      context: .
      dockerfile: Dockerfile.reasoning
    environment:
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
    depends_on:
      - kafka

volumes:
  clients_data:
  templates_data:
  auth_data:
  minio_data:-------------------------------------------------
filepath = ./build/docker/frontend/react-nginx.dockerfile
-------------------------------------------------
filepath = ./build/docker/frontend/react-dev.dockerfile
-------------------------------------------------
filepath = ./build/docker/backend/seeder.dockerfile
FROM alpine:latest

# Install PostgreSQL client and MySQL client
RUN apk add --no-cache \
    postgresql16-client \
    mysql-client \
    bash \
    curl \
    jq

# Create app directory
WORKDIR /app

# Copy seeding scripts and data files
COPY docker/scripts/seed-data.sh /app/
COPY docker/data/ /app/data/
COPY docker/scripts/wait-for-services.sh /app/

# Make scripts executable
RUN chmod +x /app/seed-data.sh /app/wait-for-services.sh

# Set default command
CMD ["/app/seed-data.sh"]-------------------------------------------------
filepath = ./build/docker/backend/web-search-adapter.dockerfile
FROM golang:1.24-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o web-search-adapter ./cmd/web-search-adapter

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/web-search-adapter /app/
COPY configs/web-search-adapter.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./web-search-adapter", "-config", "configs/web-search-adapter.yaml"]
-------------------------------------------------
filepath = ./build/docker/backend/reasoning-agent.dockerfile
# Dockerfile for the reasoning-agent service

# --- Build Stage ---
FROM golang:1.24-alpine AS builder

# Set the working directory inside the container
WORKDIR /app

# Copy go.mod and go.sum to download dependencies first, leveraging Docker cache
COPY go.mod go.sum ./
RUN go mod download

# Copy the entire source code
COPY . .

# Build the reasoning-agent binary
# The output path is /app/reasoning-agent
RUN CGO_ENABLED=0 GOOS=linux go build -v -o reasoning-agent ./cmd/reasoning-agent

# --- Final Stage ---
FROM alpine:latest

# Set the working directory
WORKDIR /root/

# Copy the built binary from the builder stage
COPY --from=builder /app/reasoning-agent .

# Expose the port the service might use for health checks (if any)
# EXPOSE 8080

# The command to run when the container starts
ENTRYPOINT ["./reasoning-agent"]-------------------------------------------------
filepath = ./build/docker/backend/image-generator-adapter.dockerfile
FROM golang:1.24-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o image-generator-adapter ./cmd/image-generator-adapter

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/image-generator-adapter /app/
COPY configs/image-adapter.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./image-generator-adapter", "-config", "configs/image-adapter.yaml"]-------------------------------------------------
filepath = ./build/docker/backend/migrator.dockerfile
FROM alpine:latest

# Install PostgreSQL client and MySQL client
RUN apk add --no-cache \
    postgresql16-client \
    mysql-client \
    bash \
    curl

# Create app directory
WORKDIR /app

# Copy migration scripts and SQL files
COPY platform/database/migrations/ /app/migrations/
COPY docker/scripts/run-migrations.sh /app/
COPY docker/scripts/wait-for-services.sh /app/

# Make scripts executable
RUN chmod +x /app/run-migrations.sh /app/wait-for-services.sh

# Set default command
CMD ["/app/run-migrations.sh"]-------------------------------------------------
filepath = ./build/docker/backend/core-manager.dockerfile
FROM golang:1.24-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o core-manager ./cmd/core-manager

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/core-manager /app/
COPY configs/core-manager.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./core-manager", "-config", "configs/core-manager.yaml"]-------------------------------------------------
filepath = ./build/docker/backend/platform.dockerfile
# Base image with common dependencies
FROM golang:1.24-alpine AS base
RUN apk add --no-cache git ca-certificates
WORKDIR /app
-------------------------------------------------
filepath = ./build/docker/backend/auth-service.dockerfile
FROM golang:1.24-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o auth-service ./cmd/auth-service

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/auth-service /app/
COPY configs/auth-service.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./auth-service", "-config", "configs/auth-service.yaml"]-------------------------------------------------
filepath = ./build/docker/backend/agent-chassis.dockerfile
FROM golang:1.24-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o agent-chassis ./cmd/agent-chassis

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/agent-chassis /app/
COPY configs/agent-chassis.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./agent-chassis", "-config", "configs/agent-chassis.yaml"]-------------------------------------------------
