filepath = ./makefile
# Comprehensive Makefile for Agent-Managed Microservices

```makefile
# Project variables
PROJECT_NAME := personae-system
ENVIRONMENT ?= production
REGION ?= uk001
REGISTRY ?= registry.personae.io
IMAGE_TAG ?= latest

# Paths
TERRAFORM_DIR := deployments/terraform/environments/$(ENVIRONMENT)/$(REGION)
KUSTOMIZE_DIR := deployments/kustomize
SCRIPTS_DIR := scripts

# Colors for output
YELLOW := \033[1;33m
GREEN := \033[1;32m
RED := \033[1;31m
NC := \033[0m # No Color

# Default target
.DEFAULT_GOAL := help

#################################
# Help
#################################
.PHONY: help
help: ## Show this help message
	@echo '$(YELLOW)Personae System - Makefile Commands$(NC)'
	@echo ''
	@echo 'Usage:'
	@echo '  make $(GREEN)<target>$(NC) $(YELLOW)[ENVIRONMENT=production] [REGION=uk001] [IMAGE_TAG=latest]$(NC)'
	@echo ''
	@echo 'Targets:'
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  $(GREEN)%-30s$(NC) %s\n", $$1, $$2}' $(MAKEFILE_LIST) | sort

#################################
# Development Environment
#################################
.PHONY: dev-up
dev-up: ## Start local development environment
	@echo "$(YELLOW)Starting local development environment...$(NC)"
	docker-compose -f deployments/docker-compose/docker-compose.yaml up -d

.PHONY: dev-down
dev-down: ## Stop local development environment
	@echo "$(YELLOW)Stopping local development environment...$(NC)"
	docker-compose -f deployments/docker-compose/docker-compose.yaml down

.PHONY: dev-logs
dev-logs: ## Show logs from development environment
	docker-compose -f deployments/docker-compose/docker-compose.yaml logs -f

.PHONY: dev-reset
dev-reset: dev-down ## Reset development environment (removes volumes)
	@echo "$(YELLOW)Resetting development environment...$(NC)"
	docker-compose -f deployments/docker-compose/docker-compose.yaml down -v

#################################
# Building
#################################
.PHONY: build-all
build-all: build-backend build-frontends ## Build all images

.PHONY: build-backend
build-backend: build-auth-service build-core-manager build-agents build-adapters ## Build all backend services

.PHONY: build-frontends
build-frontends: build-admin-dashboard build-user-portal build-agent-playground ## Build all frontend applications

# Backend services
.PHONY: build-auth-service
build-auth-service: ## Build auth-service image
	@echo "$(YELLOW)Building auth-service...$(NC)"
	docker build -t $(REGISTRY)/auth-service:$(IMAGE_TAG) \
		-f build/docker/backend/auth-service.dockerfile .

.PHONY: build-core-manager
build-core-manager: ## Build core-manager image
	@echo "$(YELLOW)Building core-manager...$(NC)"
	docker build -t $(REGISTRY)/core-manager:$(IMAGE_TAG) \
		-f build/docker/backend/core-manager.dockerfile .

.PHONY: build-agent-chassis
build-agent-chassis: ## Build agent-chassis image
	@echo "$(YELLOW)Building agent-chassis...$(NC)"
	docker build -t $(REGISTRY)/agent-chassis:$(IMAGE_TAG) \
		-f build/docker/backend/agent-chassis.dockerfile .

.PHONY: build-reasoning-agent
build-reasoning-agent: ## Build reasoning-agent image
	@echo "$(YELLOW)Building reasoning-agent...$(NC)"
	docker build -t $(REGISTRY)/reasoning-agent:$(IMAGE_TAG) \
		-f build/docker/backend/reasoning-agent.dockerfile .

.PHONY: build-web-search-adapter
build-web-search-adapter: ## Build web-search-adapter image
	@echo "$(YELLOW)Building web-search-adapter...$(NC)"
	docker build -t $(REGISTRY)/web-search-adapter:$(IMAGE_TAG) \
		-f build/docker/backend/web-search-adapter.dockerfile .

.PHONY: build-image-generator-adapter
build-image-generator-adapter: ## Build image-generator-adapter image
	@echo "$(YELLOW)Building image-generator-adapter...$(NC)"
	docker build -t $(REGISTRY)/image-generator-adapter:$(IMAGE_TAG) \
		-f build/docker/backend/image-generator-adapter.dockerfile .

# Agent targets
.PHONY: build-agents
build-agents: build-agent-chassis build-reasoning-agent ## Build all agents

.PHONY: build-adapters
build-adapters: build-web-search-adapter build-image-generator-adapter ## Build all adapters

# Frontend applications
.PHONY: build-admin-dashboard
build-admin-dashboard: ## Build admin-dashboard image
	@echo "$(YELLOW)Building admin-dashboard...$(NC)"
	cd frontends/admin-dashboard && npm install && npm run build
	docker build -t $(REGISTRY)/admin-dashboard:$(IMAGE_TAG) \
		-f frontends/admin-dashboard/Dockerfile frontends/admin-dashboard

.PHONY: build-user-portal
build-user-portal: ## Build user-portal image
	@echo "$(YELLOW)Building user-portal...$(NC)"
	cd frontends/user-portal && npm install && npm run build
	docker build -t $(REGISTRY)/user-portal:$(IMAGE_TAG) \
		-f frontends/user-portal/Dockerfile frontends/user-portal

.PHONY: build-agent-playground
build-agent-playground: ## Build agent-playground image
	@echo "$(YELLOW)Building agent-playground...$(NC)"
	cd frontends/agent-playground && npm install && npm run build
	docker build -t $(REGISTRY)/agent-playground:$(IMAGE_TAG) \
		-f frontends/agent-playground/Dockerfile frontends/agent-playground

#################################
# Push Images
#################################
.PHONY: push-all
push-all: push-backend push-frontends ## Push all images to registry

.PHONY: push-backend
push-backend: ## Push all backend images
	@echo "$(YELLOW)Pushing backend images...$(NC)"
	docker push $(REGISTRY)/auth-service:$(IMAGE_TAG)
	docker push $(REGISTRY)/core-manager:$(IMAGE_TAG)
	docker push $(REGISTRY)/agent-chassis:$(IMAGE_TAG)
	docker push $(REGISTRY)/reasoning-agent:$(IMAGE_TAG)
	docker push $(REGISTRY)/web-search-adapter:$(IMAGE_TAG)
	docker push $(REGISTRY)/image-generator-adapter:$(IMAGE_TAG)

.PHONY: push-frontends
push-frontends: ## Push all frontend images
	@echo "$(YELLOW)Pushing frontend images...$(NC)"
	docker push $(REGISTRY)/admin-dashboard:$(IMAGE_TAG)
	docker push $(REGISTRY)/user-portal:$(IMAGE_TAG)
	docker push $(REGISTRY)/agent-playground:$(IMAGE_TAG)

#################################
# Infrastructure Deployment
#################################
.PHONY: deploy-infrastructure
deploy-infrastructure: ## Deploy all infrastructure components
	@echo "$(YELLOW)Deploying infrastructure to $(ENVIRONMENT)/$(REGION)...$(NC)"
	@$(MAKE) deploy-010-infrastructure
	@$(MAKE) deploy-020-ingress
	@$(MAKE) deploy-030-strimzi
	@$(MAKE) deploy-040-kafka
	@$(MAKE) deploy-050-storage
	@$(MAKE) deploy-060-databases
	@$(MAKE) deploy-070-schemas
	@$(MAKE) deploy-080-topics
	@$(MAKE) deploy-090-monitoring

# Individual infrastructure components
.PHONY: deploy-010-infrastructure
deploy-010-infrastructure: ## Deploy core infrastructure (Kubernetes cluster)
	@echo "$(GREEN)Deploying 010-infrastructure...$(NC)"
	cd $(TERRAFORM_DIR)/010-infrastructure && \
		terraform init && \
		terraform apply -auto-approve

.PHONY: deploy-020-ingress
deploy-020-ingress: ## Deploy ingress controller
	@echo "$(GREEN)Deploying 020-ingress-nginx...$(NC)"
	cd $(TERRAFORM_DIR)/020-ingress-nginx && \
		terraform init && \
		terraform apply -auto-approve

.PHONY: deploy-030-strimzi
deploy-030-strimzi: ## Deploy Strimzi operator
	@echo "$(GREEN)Deploying 030-strimzi-operator...$(NC)"
	cd $(TERRAFORM_DIR)/030-strimzi-operator && \
		terraform init && \
		terraform apply -auto-approve

.PHONY: deploy-040-kafka
deploy-040-kafka: ## Deploy Kafka cluster
	@echo "$(GREEN)Deploying 040-kafka-cluster...$(NC)"
	cd $(TERRAFORM_DIR)/040-kafka-cluster && \
		terraform init && \
		terraform apply -auto-approve

.PHONY: deploy-050-storage
deploy-050-storage: ## Deploy S3/storage buckets
	@echo "$(GREEN)Deploying 050-storage...$(NC)"
	cd $(TERRAFORM_DIR)/050-storage && \
		terraform init && \
		terraform apply -auto-approve

.PHONY: deploy-060-databases
deploy-060-databases: ## Deploy database instances
	@echo "$(GREEN)Deploying 060-databases...$(NC)"
	cd $(TERRAFORM_DIR)/060-databases && \
		terraform init && \
		terraform apply -auto-approve

.PHONY: deploy-070-schemas
deploy-070-schemas: ## Run database migrations
	@echo "$(GREEN)Deploying 070-database-schemas...$(NC)"
	cd $(TERRAFORM_DIR)/070-database-schemas && \
		terraform init && \
		terraform apply -auto-approve

.PHONY: deploy-080-topics
deploy-080-topics: ## Create Kafka topics
	@echo "$(GREEN)Deploying 080-kafka-topics...$(NC)"
	cd $(TERRAFORM_DIR)/080-kafka-topics && \
		terraform init && \
		terraform apply -auto-approve

.PHONY: deploy-090-monitoring
deploy-090-monitoring: ## Deploy monitoring stack
	@echo "$(GREEN)Deploying 090-monitoring...$(NC)"
	cd $(TERRAFORM_DIR)/090-monitoring && \
		terraform init && \
		terraform apply -auto-approve

#################################
# Application Deployment
#################################
.PHONY: deploy-all
deploy-all: deploy-infrastructure deploy-core deploy-agents deploy-frontends ## Deploy everything

.PHONY: deploy-core
deploy-core: ## Deploy core platform services
	@echo "$(YELLOW)Deploying core platform services...$(NC)"
	kubectl apply -k $(KUSTOMIZE_DIR)/services/auth-service/overlays/$(ENVIRONMENT)
	kubectl apply -k $(KUSTOMIZE_DIR)/services/core-manager/overlays/$(ENVIRONMENT)

.PHONY: deploy-agents
deploy-agents: ## Deploy all agent services
	@echo "$(YELLOW)Deploying agent services...$(NC)"
	kubectl apply -k $(KUSTOMIZE_DIR)/services/agent-chassis/overlays/$(ENVIRONMENT)
	kubectl apply -k $(KUSTOMIZE_DIR)/services/reasoning-agent/overlays/$(ENVIRONMENT)
	kubectl apply -k $(KUSTOMIZE_DIR)/services/web-search-adapter/overlays/$(ENVIRONMENT)
	kubectl apply -k $(KUSTOMIZE_DIR)/services/image-generator-adapter/overlays/$(ENVIRONMENT)

.PHONY: deploy-frontends
deploy-frontends: ## Deploy all frontend applications
	@echo "$(YELLOW)Deploying frontend applications...$(NC)"
	kubectl apply -k $(KUSTOMIZE_DIR)/frontends/admin-dashboard/overlays/$(ENVIRONMENT)
	kubectl apply -k $(KUSTOMIZE_DIR)/frontends/user-portal/overlays/$(ENVIRONMENT)
	kubectl apply -k $(KUSTOMIZE_DIR)/frontends/agent-playground/overlays/$(ENVIRONMENT)

# Individual service deployments
.PHONY: deploy-auth-service
deploy-auth-service: ## Deploy auth-service only
	@echo "$(GREEN)Deploying auth-service...$(NC)"
	kubectl apply -k $(KUSTOMIZE_DIR)/services/auth-service/overlays/$(ENVIRONMENT)

.PHONY: deploy-core-manager
deploy-core-manager: ## Deploy core-manager only
	@echo "$(GREEN)Deploying core-manager...$(NC)"
	kubectl apply -k $(KUSTOMIZE_DIR)/services/core-manager/overlays/$(ENVIRONMENT)

.PHONY: deploy-admin-dashboard
deploy-admin-dashboard: ## Deploy admin-dashboard only
	@echo "$(GREEN)Deploying admin-dashboard...$(NC)"
	kubectl apply -k $(KUSTOMIZE_DIR)/frontends/admin-dashboard/overlays/$(ENVIRONMENT)

.PHONY: deploy-user-portal
deploy-user-portal: ## Deploy user-portal only
	@echo "$(GREEN)Deploying user-portal...$(NC)"
	kubectl apply -k $(KUSTOMIZE_DIR)/frontends/user-portal/overlays/$(ENVIRONMENT)

#################################
# Full Stack Operations
#################################
.PHONY: full-deploy
full-deploy: build-all push-all deploy-all ## Build, push, and deploy everything

.PHONY: quick-deploy
quick-deploy: ## Deploy applications without building (uses existing images)
	@echo "$(YELLOW)Quick deployment using existing images...$(NC)"
	@$(MAKE) deploy-core
	@$(MAKE) deploy-agents
	@$(MAKE) deploy-frontends

#################################
# Status and Monitoring
#################################
.PHONY: status
status: ## Show status of all deployments
	@echo "$(YELLOW)Deployment Status:$(NC)"
	kubectl get deployments -n $(PROJECT_NAME)
	@echo "\n$(YELLOW)Services:$(NC)"
	kubectl get services -n $(PROJECT_NAME)
	@echo "\n$(YELLOW)Pods:$(NC)"
	kubectl get pods -n $(PROJECT_NAME)

.PHONY: logs
logs: ## Tail logs from all pods
	kubectl logs -f -n $(PROJECT_NAME) -l app.kubernetes.io/part-of=$(PROJECT_NAME) --all-containers=true

.PHONY: logs-auth
logs-auth: ## Tail logs from auth-service
	kubectl logs -f -n $(PROJECT_NAME) -l app=auth-service --all-containers=true

.PHONY: logs-core
logs-core: ## Tail logs from core-manager
	kubectl logs -f -n $(PROJECT_NAME) -l app=core-manager --all-containers=true

#################################
# Rollback Operations
#################################
.PHONY: rollback-auth-service
rollback-auth-service: ## Rollback auth-service deployment
	kubectl rollout undo deployment/auth-service -n $(PROJECT_NAME)

.PHONY: rollback-core-manager
rollback-core-manager: ## Rollback core-manager deployment
	kubectl rollout undo deployment/core-manager -n $(PROJECT_NAME)

#################################
# Testing
#################################
.PHONY: test
test: test-unit test-integration ## Run all tests

.PHONY: test-unit
test-unit: ## Run unit tests
	@echo "$(YELLOW)Running unit tests...$(NC)"
	go test ./... -v -short

.PHONY: test-integration
test-integration: ## Run integration tests
	@echo "$(YELLOW)Running integration tests...$(NC)"
	go test ./tests/integration/... -v

.PHONY: test-e2e
test-e2e: ## Run end-to-end tests
	@echo "$(YELLOW)Running E2E tests...$(NC)"
	go test ./tests/e2e/... -v

#################################
# Database Operations
#################################
.PHONY: db-migrate
db-migrate: ## Run database migrations
	@echo "$(YELLOW)Running database migrations...$(NC)"
	$(SCRIPTS_DIR)/migration/run-migrations.sh

.PHONY: db-seed
db-seed: ## Seed database with test data
	@echo "$(YELLOW)Seeding database...$(NC)"
	kubectl exec -it deployment/postgres-clients -n $(PROJECT_NAME) -- \
		psql -U postgres -f /scripts/seed-data.sql

#################################
# Utility Commands
#################################
.PHONY: clean
clean: ## Clean build artifacts
	@echo "$(YELLOW)Cleaning build artifacts...$(NC)"
	rm -rf dist/
	rm -rf frontends/*/build/
	rm -rf frontends/*/dist/

.PHONY: setup-registry
setup-registry: ## Set up local Docker registry
	@echo "$(YELLOW)Setting up local Docker registry...$(NC)"
	$(SCRIPTS_DIR)/utils/setup-local-registry.sh

.PHONY: generate-secrets
generate-secrets: ## Generate required secrets
	@echo "$(YELLOW)Generating secrets...$(NC)"
	$(SCRIPTS_DIR)/utils/generate-jwt-secret.sh

.PHONY: port-forward-admin
port-forward-admin: ## Port forward admin dashboard to localhost:3000
	kubectl port-forward -n $(PROJECT_NAME) svc/admin-dashboard 3000:80

.PHONY: port-forward-grafana
port-forward-grafana: ## Port forward Grafana to localhost:3001
	kubectl port-forward -n $(PROJECT_NAME) svc/grafana 3001:3000

#################################
# Individual Service Builds & Deploys
#################################
# Convenience targets for individual service development
.PHONY: auth-service
auth-service: build-auth-service push-auth-service deploy-auth-service ## Build, push and deploy auth-service

.PHONY: core-manager
core-manager: build-core-manager push-core-manager deploy-core-manager ## Build, push and deploy core-manager

.PHONY: admin-dashboard
admin-dashboard: build-admin-dashboard push-admin-dashboard deploy-admin-dashboard ## Build, push and deploy admin-dashboard

# Push individual services
.PHONY: push-auth-service
push-auth-service: ## Push auth-service image
	docker push $(REGISTRY)/auth-service:$(IMAGE_TAG)

.PHONY: push-core-manager
push-core-manager: ## Push core-manager image
	docker push $(REGISTRY)/core-manager:$(IMAGE_TAG)

.PHONY: push-admin-dashboard
push-admin-dashboard: ## Push admin-dashboard image
	docker push $(REGISTRY)/admin-dashboard:$(IMAGE_TAG)

#################################
# Terraform Operations
#################################
.PHONY: tf-plan
tf-plan: ## Run terraform plan for all infrastructure
	@echo "$(YELLOW)Running Terraform plan...$(NC)"
	@for dir in $(TERRAFORM_DIR)/0*; do \
		echo "$(GREEN)Planning $$dir...$(NC)"; \
		cd $$dir && terraform plan; \
	done

.PHONY: tf-destroy-apps
tf-destroy-apps: ## Destroy all applications (keeps infrastructure)
	@echo "$(RED)Destroying all applications...$(NC)"
	kubectl delete -k $(KUSTOMIZE_DIR)/services --recursive
	kubectl delete -k $(KUSTOMIZE_DIR)/frontends --recursive

.PHONY: tf-destroy-all
tf-destroy-all: ## Destroy everything (WARNING: This will delete everything!)
	@echo "$(RED)WARNING: This will destroy all infrastructure and data!$(NC)"
	@echo "Press Ctrl+C within 5 seconds to cancel..."
	@sleep 5
	@for dir in $$(ls -r $(TERRAFORM_DIR)/); do \
		echo "$(RED)Destroying $$dir...$(NC)"; \
		cd $(TERRAFORM_DIR)/$$dir && terraform destroy -auto-approve; \
	done
-------------------------------------------------
filepath = ./pkg/models/database.go
// FILE: pkg/models/database.go
package models

import (
	"context"
	"github.com/google/uuid"
	"time"
)

// Persona represents both templates and instances
type Persona struct {
	ID          uuid.UUID              `json:"id"`
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Category    string                 `json:"category"`
	Config      map[string]interface{} `json:"config"`
	IsTemplate  bool                   `json:"is_template"`
	IsActive    bool                   `json:"is_active"`
	CreatedAt   time.Time              `json:"created_at"`
	UpdatedAt   time.Time              `json:"updated_at"`
}

// PersonaRepository defines the interface for persona data access
type PersonaRepository interface {
	// Template methods
	CreateTemplate(ctx context.Context, template *Persona) (*Persona, error)
	GetTemplateByID(ctx context.Context, id string) (*Persona, error)
	ListTemplates(ctx context.Context) ([]Persona, error)
	UpdateTemplate(ctx context.Context, template *Persona) (*Persona, error)
	DeleteTemplate(ctx context.Context, id string) error

	// Instance methods
	CreateInstanceFromTemplate(ctx context.Context, templateID string, userID string, instanceName string) (*Persona, error)
	GetInstanceByID(ctx context.Context, id string) (*Persona, error)
	ListInstances(ctx context.Context, userID string) ([]Persona, error)
	UpdateInstance(ctx context.Context, id string, name *string, config map[string]interface{}) (*Persona, error)
	DeleteInstance(ctx context.Context, id string) error

	AdminUpdateInstanceConfig(ctx context.Context, clientID, instanceID string, config map[string]interface{}) error
}
-------------------------------------------------
filepath = ./pkg/models/contracts.go
// FILE: pkg/models/contracts.go (updated)
package models

import "time"

// AgentConfig defines the "mind" of an agent, loaded from the database
type AgentConfig struct {
	AgentID      string                 `json:"agent_id"`
	AgentType    string                 `json:"agent_type"`
	Version      int                    `json:"version"`
	CoreLogic    map[string]interface{} `json:"core_logic"`
	Workflow     WorkflowPlan           `json:"workflow"`
	MemoryConfig MemoryConfiguration    `json:"memory_config,omitempty"`
}

// MemoryConfiguration controls how the agent uses long-term memory
type MemoryConfiguration struct {
	Enabled            bool     `json:"enabled"`
	AutoStore          bool     `json:"auto_store"`
	AutoStoreThreshold float64  `json:"auto_store_threshold"`
	MaxMemories        int      `json:"max_memories"`
	RetrievalCount     int      `json:"retrieval_count"`
	EmbeddingModel     string   `json:"embedding_model"`
	IncludeTypes       []string `json:"include_types"`
}

// MemoryEntry represents a single memory to be stored
type MemoryEntry struct {
	Content   string                 `json:"content"`
	Type      string                 `json:"type"`
	Metadata  map[string]interface{} `json:"metadata"`
	Timestamp time.Time              `json:"timestamp"`
}

// WorkflowPlan defines the orchestration steps for an agent
type WorkflowPlan struct {
	StartStep string          `json:"start_step"`
	Steps     map[string]Step `json:"steps"`
}

// Step represents a single action or sub-workflow within a plan
type Step struct {
	Action       string    `json:"action"`
	Description  string    `json:"description"`
	Topic        string    `json:"topic,omitempty"`
	Dependencies []string  `json:"dependencies,omitempty"`
	NextStep     string    `json:"next_step,omitempty"`
	SubTasks     []SubTask `json:"sub_tasks,omitempty"`
	StoreMemory  bool      `json:"store_memory,omitempty"` // New field
}

// SubTask for fan-out operations
type SubTask struct {
	StepName string `json:"step_name"`
	Topic    string `json:"topic"`
}

// Standard message payloads
type TaskRequest struct {
	Action string                 `json:"action"`
	Data   map[string]interface{} `json:"data"`
}

type TaskResponse struct {
	Success bool                   `json:"success"`
	Data    map[string]interface{} `json:"data"`
	Error   string                 `json:"error,omitempty"`
}
-------------------------------------------------
filepath = ./scripts/README.setup.md
How It All Works Together

The scripts are designed to work in a specific, logical sequence that ensures a smooth deployment:

    Local Setup: A developer runs setup.sh once to prepare the cluster by creating the namespace and all necessary secrets.

    Deployment Orchestration: The developer runs make deploy (or make quickstart), which executes deploy-system.sh.

    Infrastructure Creation: deploy-system.sh applies the Kubernetes YAML files, creating the database and Kafka StatefulSets, services, and jobs.

    Container-Level Waiting: As the database-init job pod starts, its internal wait-for-services.sh script runs, pausing execution until the PostgreSQL and MySQL pods are fully ready to accept connections.

    Database Migration: Once the databases are ready, docker-run-migrations.sh runs within the job, applying all the table schemas.

    Data Seeding: The data-seeder job follows the same pattern, waiting for services before seed-data.sh inserts the initial persona templates.

    Service Startup: Finally, the main microservices (like auth-service and core-manager) start. Their initContainers also run wait scripts to ensure they don't start before their own dependencies (like databases or specific Kafka topics) are available.-------------------------------------------------
filepath = ./scripts/migration/seed-data.sql
-------------------------------------------------
filepath = ./scripts/migration/run-migrations.sh
-------------------------------------------------
filepath = ./scripts/deploy-system.sh
#!/bin/bash
# FILE: scripts/deploy-system.sh
# Complete deployment script that orchestrates the entire system deployment

set -e

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m'

# Configuration
NAMESPACE="ai-persona-system"
TIMEOUT="300s"
DOCKER_REGISTRY="ai-persona-system"

# Function to print colored output
print_header() {
    echo -e "${BLUE}=============================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}=============================================${NC}"
}

print_step() {
    echo -e "${YELLOW}üîß $1${NC}"
}

print_success() {
    echo -e "${GREEN}‚úÖ $1${NC}"
}

print_error() {
    echo -e "${RED}‚ùå $1${NC}"
}

# Function to wait for pods
wait_for_pods() {
    local label=$1
    local description=$2
    local timeout=${3:-300s}

    print_step "Waiting for $description to be ready..."
    if kubectl wait --for=condition=ready pod -l "$label" -n "$NAMESPACE" --timeout="$timeout"; then
        print_success "$description is ready"
    else
        print_error "$description failed to become ready"
        return 1
    fi
}

# Function to wait for jobs to complete
wait_for_job() {
    local job_name=$1
    local description=$2
    local timeout=${3:-600}

    print_step "Waiting for $description to complete..."
    local attempt=1
    local max_attempts=$((timeout / 10))

    while [ $attempt -le $max_attempts ]; do
        local status=$(kubectl get job "$job_name" -n "$NAMESPACE" -o jsonpath='{.status.conditions[0].type}' 2>/dev/null || echo "NotFound")

        if [ "$status" = "Complete" ]; then
            print_success "$description completed successfully"
            return 0
        elif [ "$status" = "Failed" ]; then
            print_error "$description failed"
            kubectl logs -n "$NAMESPACE" job/"$job_name" --tail=50
            return 1
        fi

        echo -e "${YELLOW}‚è≥ Attempt $attempt/$max_attempts - $description still running...${NC}"
        sleep 10
        ((attempt++))
    done

    print_error "$description timed out"
    return 1
}

# Check prerequisites
check_prerequisites() {
    print_header "CHECKING PREREQUISITES"

    print_step "Checking required tools..."
    for tool in kubectl docker; do
        if ! command -v "$tool" >/dev/null 2>&1; then
            print_error "$tool is required but not installed"
            exit 1
        fi
    done

    print_step "Checking Kubernetes connection..."
    if ! kubectl cluster-info >/dev/null 2>&1; then
        print_error "Cannot connect to Kubernetes cluster"
        exit 1
    fi

    print_success "Prerequisites check passed"
}

# Phase 1: Infrastructure
deploy_infrastructure() {
    print_header "PHASE 1: DEPLOYING INFRASTRUCTURE"

    print_step "Creating namespace..."
    kubectl apply -f k8s/namespace.yaml

    print_step "Waiting for namespace to be active..."
    kubectl wait --for=jsonpath='{.status.phase}'=Active namespace/"$NAMESPACE" --timeout=60s

    print_step "Applying RBAC and security policies..."
    kubectl apply -f k8s/rbac-security.yaml

    print_step "Creating ConfigMaps..."
    kubectl apply -f k8s/configmap-common.yaml

    print_success "Infrastructure deployed"
}

# Phase 2: Storage
deploy_storage() {
    print_header "PHASE 2: DEPLOYING STORAGE SYSTEMS"

    print_step "Deploying PostgreSQL databases..."
    kubectl apply -f k8s/postgres-clients.yaml
    kubectl apply -f k8s/postgres-templates.yaml

    print_step "Deploying MySQL database..."
    kubectl apply -f k8s/mysql-auth.yaml

    print_step "Deploying MinIO object storage..."
    kubectl apply -f k8s/minio.yaml

    print_step "Deploying backup storage..."
    kubectl apply -f k8s/backup-cronjob.yaml

    print_success "Storage systems deployed"
}

# Phase 3: Messaging
deploy_messaging() {
    print_header "PHASE 3: DEPLOYING MESSAGING SYSTEM"

    print_step "Deploying Kafka cluster..."
    kubectl apply -f k8s/kafka.yaml

    print_success "Messaging system deployed"
}

# Phase 4: Wait for infrastructure
wait_for_infrastructure() {
    print_header "PHASE 4: WAITING FOR INFRASTRUCTURE"

    wait_for_pods "app=postgres-clients" "PostgreSQL Clients"
    wait_for_pods "app=postgres-templates" "PostgreSQL Templates"
    wait_for_pods "app=mysql-auth" "MySQL Auth"
    wait_for_pods "app=minio" "MinIO"
    wait_for_pods "app=kafka" "Kafka Cluster" "600s"

    print_success "All infrastructure components are ready"
}

# Phase 5: Initialize system
initialize_system() {
    print_header "PHASE 5: INITIALIZING SYSTEM"

    print_step "Running database migrations..."
    kubectl apply -f k8s/jobs/database-init-job.yaml
    wait_for_job "database-init" "Database Migrations" 600

    print_step "Creating Kafka topics..."
    kubectl apply -f k8s/jobs/kafka-topics-job.yaml
    wait_for_job "kafka-topics-init" "Kafka Topics Creation" 300

    print_step "Seeding initial data..."
    wait_for_job "data-seeder" "Data Seeding" 300

    print_success "System initialization completed"
}

# Phase 6: Core services
deploy_core_services() {
    print_header "PHASE 6: DEPLOYING CORE SERVICES"

    print_step "Deploying Auth Service..."
    kubectl apply -f k8s/auth-service.yaml

    print_step "Deploying Core Manager..."
    kubectl apply -f k8s/core-manager.yaml

    wait_for_pods "app=auth-service" "Auth Service"
    wait_for_pods "app=core-manager" "Core Manager"

    print_success "Core services deployed"
}

# Phase 7: Agents
deploy_agents() {
    print_header "PHASE 7: DEPLOYING AGENT SERVICES"

    print_step "Deploying Agent Chassis..."
    kubectl apply -f k8s/agent-chassis.yaml

    print_step "Deploying Reasoning Agent..."
    kubectl apply -f k8s/reasoning-agent.yaml

    print_step "Deploying Image Generator Adapter..."
    kubectl apply -f k8s/image-generator-adapter.yaml

    print_step "Deploying Web Search Adapter..."
    kubectl apply -f k8s/web-search-adapter.yaml

    wait_for_pods "app=agent-chassis" "Agent Chassis" "600s"
    wait_for_pods "app=reasoning-agent" "Reasoning Agent"
    wait_for_pods "app=image-generator-adapter" "Image Generator"
    wait_for_pods "app=web-search-adapter" "Web Search"

    print_success "Agent services deployed"
}

# Phase 8: Monitoring and ingress
deploy_monitoring_ingress() {
    print_header "PHASE 8: DEPLOYING MONITORING AND INGRESS"

    print_step "Deploying monitoring stack..."
    kubectl apply -f k8s/monitoring/

    print_step "Deploying ingress..."
    kubectl apply -f k8s/ingress.yaml

    wait_for_pods "app=prometheus" "Prometheus" 300s
    wait_for_pods "app=grafana" "Grafana" 300s

    print_success "Monitoring and ingress deployed"
}

# System verification
verify_system() {
    print_header "SYSTEM VERIFICATION"

    print_step "Checking system status..."
    kubectl get pods -n "$NAMESPACE"

    print_step "Checking services..."
    kubectl get services -n "$NAMESPACE"

    print_step "Verifying Kafka topics..."
    kubectl exec -n "$NAMESPACE" kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --list | head -10

    print_step "Checking database tables..."
    kubectl exec -n "$NAMESPACE" postgres-templates-0 -- psql -U templates_user -d templates_db -c "\dt" >/dev/null 2>&1 && \
        echo -e "${GREEN}‚úÖ Templates database OK${NC}" || echo -e "${RED}‚ùå Templates database issue${NC}"

    kubectl exec -n "$NAMESPACE" postgres-clients-0 -- psql -U clients_user -d clients_db -c "\dt" >/dev/null 2>&1 && \
        echo -e "${GREEN}‚úÖ Clients database OK${NC}" || echo -e "${RED}‚ùå Clients database issue${NC}"

    print_success "System verification completed"
}

# Cleanup function
cleanup_failed_deployment() {
    print_error "Deployment failed. Cleaning up failed jobs..."
    kubectl delete jobs --field-selector status.successful=0 -n "$NAMESPACE" 2>/dev/null || true
}

# Main execution
main() {
    print_header "AI PERSONA SYSTEM DEPLOYMENT"
    echo -e "${BLUE}Starting complete system deployment...${NC}"
    echo ""

    # Set up error handling
    trap cleanup_failed_deployment ERR

    # Execute deployment phases
    check_prerequisites
    deploy_infrastructure
    deploy_storage
    deploy_messaging
    wait_for_infrastructure
    initialize_system
    deploy_core_services
    deploy_agents
    deploy_monitoring_ingress
    verify_system

    print_header "DEPLOYMENT COMPLETED SUCCESSFULLY!"
    echo ""
    echo -e "${GREEN}üéâ AI Persona System is now running!${NC}"
    echo ""
    echo -e "${YELLOW}Next steps:${NC}"
    echo -e "1. Set up port forwarding: ${BLUE}make port-forward${NC}"
    echo -e "2. Create your first client: ${BLUE}make create-client-schema CLIENT_ID=demo_client${NC}"
    echo -e "3. Test the system: ${BLUE}make test-api${NC}"
    echo ""
    echo -e "${YELLOW}Access URLs (after port forwarding):${NC}"
    echo -e "- Auth API: ${BLUE}http://localhost:8081${NC}"
    echo -e "- Core API: ${BLUE}http://localhost:8088${NC}"
    echo -e "- Grafana: ${BLUE}http://localhost:3000${NC} (admin/admin)"
    echo -e "- Kafka UI: ${BLUE}http://localhost:8080${NC}"
    echo ""
}

# Execute main function
main "$@"-------------------------------------------------
filepath = ./scripts/fix_uuid_migration_files.sh
#!/bin/bash
# Fix UUID generation in migration files

# Update auth_schema.sql
sed -i 's/gen_random_uuid()/gen_random_uuid()/g' ../platform/database/migrations/004_auth_schema.sql

# Update client_schema.sql - ensure we're using gen_random_uuid() consistently
sed -i 's/uuid_generate_v4()/gen_random_uuid()/g' ../platform/database/migrations/003_create_client_schema.sql

# Remove the CREATE EXTENSION IF NOT EXISTS "uuid-ossp" line as it's not needed with gen_random_uuid()
sed -i '/CREATE EXTENSION IF NOT EXISTS "uuid-ossp";/d' ../platform/database/migrations/003_create_client_schema.sql

echo "UUID generation standardized to use gen_random_uuid() across all migrations"-------------------------------------------------
filepath = ./scripts/docker/seed-data.sql
#!/bin/bash
# FILE: docker/scripts/seed-data.sh
set -e

echo "üå± Starting data seeding..."

# Wait for services to be ready
/app/wait-for-services.sh

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# Function to seed PostgreSQL data
seed_postgres_data() {
    local host=$1
    local user=$2
    local database=$3
    local password=$4
    local sql_command=$5
    local description=$6

    echo -e "${YELLOW}üå± Seeding $description...${NC}"

    export PGPASSWORD="$password"

    if psql -h "$host" -U "$user" -d "$database" -c "$sql_command"; then
        echo -e "${GREEN}‚úÖ $description seeded${NC}"
    else
        echo -e "${RED}‚ùå $description seeding failed${NC}"
        # Don't exit on seeding failures - they might already exist
    fi
}

# Function to seed MySQL data
seed_mysql_data() {
    local host=$1
    local user=$2
    local database=$3
    local password=$4
    local sql_command=$5
    local description=$6

    echo -e "${YELLOW}üå± Seeding $description...${NC}"

    if mysql -h "$host" -u "$user" -p"$password" "$database" -e "$sql_command"; then
        echo -e "${GREEN}‚úÖ $description seeded${NC}"
    else
        echo -e "${RED}‚ùå $description seeding failed${NC}"
        # Don't exit on seeding failures - they might already exist
    fi
}

# 1. Seed persona templates
echo -e "${YELLOW}ü§ñ Seeding persona templates...${NC}"

# Basic Copywriter Template
seed_postgres_data \
    "postgres-templates" \
    "templates_user" \
    "templates_db" \
    "$TEMPLATES_DB_PASSWORD" \
    "INSERT INTO persona_templates (id, name, description, category, config, is_active, created_at, updated_at)
     VALUES (
         '00000000-0000-0000-0000-000000000001',
         'Basic Copywriter',
         'A versatile copywriting assistant that can create engaging content across various formats and tones.',
         'copywriter',
         '{
             \"model\": \"claude-3-sonnet\",
             \"temperature\": 0.7,
             \"max_tokens\": 2000,
             \"system_prompt\": \"You are a professional copywriter. Create compelling, engaging content that resonates with the target audience. Always consider the tone, style, and purpose of the content.\",
             \"workflow\": {
                 \"start_step\": \"generate_content\",
                 \"steps\": {
                     \"generate_content\": {
                         \"action\": \"ai_text_generate_claude_sonnet\",
                         \"description\": \"Generate the requested content\",
                         \"next_step\": \"complete_workflow\"
                     },
                     \"complete_workflow\": {
                         \"action\": \"complete_workflow\",
                         \"description\": \"Mark workflow as complete\"
                     }
                 }
             }
         }',
         true,
         NOW(),
         NOW()
     ) ON CONFLICT (id) DO NOTHING;" \
    "Basic Copywriter Template"

# Research Assistant Template
seed_postgres_data \
    "postgres-templates" \
    "templates_user" \
    "templates_db" \
    "$TEMPLATES_DB_PASSWORD" \
    "INSERT INTO persona_templates (id, name, description, category, config, is_active, created_at, updated_at)
     VALUES (
         '00000000-0000-0000-0000-000000000002',
         'Research Assistant',
         'An in-depth research specialist that can gather, analyze, and synthesize information from multiple sources.',
         'researcher',
         '{
             \"model\": \"claude-3-opus\",
             \"temperature\": 0.3,
             \"max_tokens\": 4000,
             \"system_prompt\": \"You are a thorough research assistant. Provide comprehensive, well-sourced, and analytically rigorous research. Always cite sources and present balanced perspectives.\",
             \"workflow\": {
                 \"start_step\": \"web_search\",
                 \"steps\": {
                     \"web_search\": {
                         \"action\": \"web_search\",
                         \"description\": \"Search for relevant information\",
                         \"topic\": \"system.adapter.web.search\",
                         \"next_step\": \"analyze_research\"
                     },
                     \"analyze_research\": {
                         \"action\": \"ai_text_generate_claude_opus\",
                         \"description\": \"Analyze and synthesize research findings\",
                         \"next_step\": \"complete_workflow\"
                     },
                     \"complete_workflow\": {
                         \"action\": \"complete_workflow\",
                         \"description\": \"Mark workflow as complete\"
                     }
                 }
             }
         }',
         true,
         NOW(),
         NOW()
     ) ON CONFLICT (id) DO NOTHING;" \
    "Research Assistant Template"

# Blog Post Generator Template
seed_postgres_data \
    "postgres-templates" \
    "templates_user" \
    "templates_db" \
    "$TEMPLATES_DB_PASSWORD" \
    "INSERT INTO persona_templates (id, name, description, category, config, is_active, created_at, updated_at)
     VALUES (
         '00000000-0000-0000-0000-000000000003',
         'Blog Post Generator',
         'A specialized content creator for well-structured, engaging blog posts with research backing.',
         'content-creator',
         '{
             \"model\": \"claude-3-sonnet\",
             \"temperature\": 0.6,
             \"max_tokens\": 3000,
             \"system_prompt\": \"You are a professional blog writer. Create well-structured, engaging blog posts with clear introductions, informative body content, and compelling conclusions. Use proper headings and maintain consistent tone.\",
             \"workflow\": {
                 \"start_step\": \"research_topic\",
                 \"steps\": {
                     \"research_topic\": {
                         \"action\": \"fan_out\",
                         \"description\": \"Research the topic thoroughly\",
                         \"sub_tasks\": [
                             {\"step_name\": \"web_research\", \"topic\": \"system.adapter.web.search\"},
                             {\"step_name\": \"style_analysis\", \"topic\": \"system.agent.reasoning.process\"}
                         ],
                         \"next_step\": \"generate_blog_post\"
                     },
                     \"generate_blog_post\": {
                         \"action\": \"ai_text_generate_claude_sonnet\",
                         \"description\": \"Generate the blog post using research\",
                         \"next_step\": \"pause_for_review\"
                     },
                     \"pause_for_review\": {
                         \"action\": \"pause_for_human_input\",
                         \"description\": \"Allow human review and approval\",
                         \"next_step\": \"complete_workflow\"
                     },
                     \"complete_workflow\": {
                         \"action\": \"complete_workflow\",
                         \"description\": \"Mark workflow as complete\"
                     }
                 }
             }
         }',
         true,
         NOW(),
         NOW()
     ) ON CONFLICT (id) DO NOTHING;" \
    "Blog Post Generator Template"

# Image Content Creator Template
seed_postgres_data \
    "postgres-templates" \
    "templates_user" \
    "templates_db" \
    "$TEMPLATES_DB_PASSWORD" \
    "INSERT INTO persona_templates (id, name, description, category, config, is_active, created_at, updated_at)
     VALUES (
         '00000000-0000-0000-0000-000000000004',
         'Visual Content Creator',
         'Creates both textual content and accompanying images for comprehensive visual storytelling.',
         'multimedia-creator',
         '{
             \"model\": \"claude-3-sonnet\",
             \"temperature\": 0.7,
             \"max_tokens\": 2500,
             \"system_prompt\": \"You are a visual content creator. Create compelling text content and provide detailed image descriptions for visual elements that enhance the narrative.\",
             \"workflow\": {
                 \"start_step\": \"create_content_plan\",
                 \"steps\": {
                     \"create_content_plan\": {
                         \"action\": \"ai_text_generate_claude_sonnet\",
                         \"description\": \"Plan the content and visual elements\",
                         \"next_step\": \"generate_visuals\"
                     },
                     \"generate_visuals\": {
                         \"action\": \"ai_image_generate_sdxl\",
                         \"description\": \"Generate accompanying images\",
                         \"topic\": \"system.adapter.image.generate\",
                         \"next_step\": \"finalize_content\"
                     },
                     \"finalize_content\": {
                         \"action\": \"ai_text_generate_claude_sonnet\",
                         \"description\": \"Finalize content with visual references\",
                         \"next_step\": \"complete_workflow\"
                     },
                     \"complete_workflow\": {
                         \"action\": \"complete_workflow\",
                         \"description\": \"Mark workflow as complete\"
                     }
                 }
             }
         }',
         true,
         NOW(),
         NOW()
     ) ON CONFLICT (id) DO NOTHING;" \
    "Visual Content Creator Template"

# 2. Verify subscription tiers exist (they should be created by migration)
echo -e "${YELLOW}üí≥ Verifying subscription tiers...${NC}"
mysql -h mysql-auth -u auth_user -p"$AUTH_DB_PASSWORD" auth_db -e "SELECT COUNT(*) as tier_count FROM subscription_tiers;" 2>/dev/null || {
    echo -e "${RED}‚ùå Subscription tiers table not accessible${NC}"
}

# 3. Create default permissions if they don't exist
echo -e "${YELLOW}üîê Ensuring default permissions exist...${NC}"
seed_mysql_data \
    "mysql-auth" \
    "auth_user" \
    "auth_db" \
    "$AUTH_DB_PASSWORD" \
    "INSERT IGNORE INTO permissions (id, name, description) VALUES
        ('00000000-0000-0000-0000-000000000001', 'personas.create', 'Create new personas'),
        ('00000000-0000-0000-0000-000000000002', 'personas.delete', 'Delete personas'),
        ('00000000-0000-0000-0000-000000000003', 'projects.manage', 'Manage all projects'),
        ('00000000-0000-0000-0000-000000000004', 'admin.users', 'Manage users'),
        ('00000000-0000-0000-0000-000000000005', 'admin.subscriptions', 'Manage subscriptions'),
        ('00000000-0000-0000-0000-000000000006', '*', 'Super admin - all permissions');" \
    "Default Permissions"

echo -e "${GREEN}üéâ Data seeding completed successfully!${NC}"

# Show summary
echo -e "${YELLOW}üìä Seeding Summary:${NC}"
echo -e "${GREEN}‚úÖ 4 Persona templates created${NC}"
echo -e "${GREEN}‚úÖ 6 Default permissions ensured${NC}"
echo -e "${GREEN}‚úÖ Subscription tiers verified${NC}"-------------------------------------------------
filepath = ./scripts/docker/docker-run-migrations.sh
#!/bin/bash
# FILE: docker/scripts/run-migrations.sh
set -e

echo "üîß Starting database migrations..."

# Wait for services to be ready
/app/wait-for-services.sh

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# Function to run PostgreSQL migration
run_postgres_migration() {
    local host=$1
    local user=$2
    local database=$3
    local password=$4
    local migration_file=$5
    local description=$6

    echo -e "${YELLOW}üìù Running $description...${NC}"

    export PGPASSWORD="$password"

    if psql -h "$host" -U "$user" -d "$database" -f "$migration_file"; then
        echo -e "${GREEN}‚úÖ $description completed${NC}"
    else
        echo -e "${RED}‚ùå $description failed${NC}"
        exit 1
    fi
}

# Function to run MySQL migration
run_mysql_migration() {
    local host=$1
    local user=$2
    local database=$3
    local password=$4
    local migration_file=$5
    local description=$6

    echo -e "${YELLOW}üìù Running $description...${NC}"

    if mysql -h "$host" -u "$user" -p"$password" "$database" < "$migration_file"; then
        echo -e "${GREEN}‚úÖ $description completed${NC}"
    else
        echo -e "${RED}‚ùå $description failed${NC}"
        exit 1
    fi
}

# 1. Enable pgvector extension on clients database
echo -e "${YELLOW}üîß Enabling pgvector extension...${NC}"
export PGPASSWORD="$CLIENTS_DB_PASSWORD"
psql -h postgres-clients -U clients_user -d clients_db -c "CREATE EXTENSION IF NOT EXISTS vector;" || {
    echo -e "${RED}‚ùå Failed to enable pgvector${NC}"
    exit 1
}
echo -e "${GREEN}‚úÖ pgvector enabled${NC}"

# 2. Migrate templates database
run_postgres_migration \
    "postgres-templates" \
    "templates_user" \
    "templates_db" \
    "$TEMPLATES_DB_PASSWORD" \
    "/app/migrations/002_create_templates_schema.sql" \
    "Templates database migration"

# 3. Create base clients database structure
run_postgres_migration \
    "postgres-clients" \
    "clients_user" \
    "clients_db" \
    "$CLIENTS_DB_PASSWORD" \
    "/app/migrations/003_create_client_base.sql" \
    "Clients database base structure"

# 4. Create orchestrator state table
echo -e "${YELLOW}üìù Creating orchestrator state table...${NC}"
export PGPASSWORD="$CLIENTS_DB_PASSWORD"
psql -h postgres-clients -U clients_user -d clients_db -c "
CREATE TABLE IF NOT EXISTS orchestrator_state (
    correlation_id UUID PRIMARY KEY,
    status VARCHAR(50) NOT NULL,
    current_step VARCHAR(255) NOT NULL,
    awaited_steps JSONB DEFAULT '[]',
    collected_data JSONB DEFAULT '{}',
    initial_request_data JSONB,
    final_result JSONB,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_orchestrator_state_status ON orchestrator_state(status);
CREATE INDEX IF NOT EXISTS idx_orchestrator_state_updated_at ON orchestrator_state(updated_at);
" || {
    echo -e "${RED}‚ùå Failed to create orchestrator state table${NC}"
    exit 1
}
echo -e "${GREEN}‚úÖ Orchestrator state table created${NC}"

# 5. Migrate auth database (MySQL)
run_mysql_migration \
    "mysql-auth" \
    "auth_user" \
    "auth_db" \
    "$AUTH_DB_PASSWORD" \
    "/app/migrations/004_auth_schema.sql" \
    "Auth database schema migration"

run_mysql_migration \
    "mysql-auth" \
    "auth_user" \
    "auth_db" \
    "$AUTH_DB_PASSWORD" \
    "/app/migrations/005_projects_schema.sql" \
    "Projects and subscriptions schema migration"

echo -e "${GREEN}üéâ All database migrations completed successfully!${NC}"-------------------------------------------------
filepath = ./scripts/deploy/deploy-infrastructure.sh
-------------------------------------------------
filepath = ./scripts/deploy/deploy-service.sh
-------------------------------------------------
filepath = ./scripts/deploy/deploy-all.sh
-------------------------------------------------
filepath = ./scripts/deploy/deploy-frontend.sh
-------------------------------------------------
filepath = ./scripts/deploy/rollback-service.sh
-------------------------------------------------
filepath = ./scripts/utils/wait-for-services.sh
#!/bin/bash
# FILE: docker/scripts/wait-for-services.sh
set -e

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

echo -e "${YELLOW}‚è≥ Waiting for services to be ready...${NC}"

# Function to wait for PostgreSQL
wait_for_postgres() {
    local host=$1
    local user=$2
    local database=$3
    local password=$4
    local service_name=$5

    echo -e "${YELLOW}üîç Waiting for PostgreSQL: $service_name...${NC}"

    export PGPASSWORD="$password"

    local max_attempts=60
    local attempt=1

    while [ $attempt -le $max_attempts ]; do
        if pg_isready -h "$host" -U "$user" -d "$database" >/dev/null 2>&1; then
            if psql -h "$host" -U "$user" -d "$database" -c "SELECT 1;" >/dev/null 2>&1; then
                echo -e "${GREEN}‚úÖ $service_name is ready${NC}"
                return 0
            fi
        fi

        if [ $attempt -eq $max_attempts ]; then
            echo -e "${RED}‚ùå $service_name failed to become ready after $max_attempts attempts${NC}"
            return 1
        fi

        echo -e "${YELLOW}‚è≥ Attempt $attempt/$max_attempts - $service_name not ready yet...${NC}"
        sleep 5
        ((attempt++))
    done
}

# Function to wait for MySQL
wait_for_mysql() {
    local host=$1
    local user=$2
    local database=$3
    local password=$4
    local service_name=$5

    echo -e "${YELLOW}üîç Waiting for MySQL: $service_name...${NC}"

    local max_attempts=60
    local attempt=1

    while [ $attempt -le $max_attempts ]; do
        if mysqladmin ping -h "$host" --silent >/dev/null 2>&1; then
            if mysql -h "$host" -u "$user" -p"$password" -e "SELECT 1;" "$database" >/dev/null 2>&1; then
                echo -e "${GREEN}‚úÖ $service_name is ready${NC}"
                return 0
            fi
        fi

        if [ $attempt -eq $max_attempts ]; then
            echo -e "${RED}‚ùå $service_name failed to become ready after $max_attempts attempts${NC}"
            return 1
        fi

        echo -e "${YELLOW}‚è≥ Attempt $attempt/$max_attempts - $service_name not ready yet...${NC}"
        sleep 5
        ((attempt++))
    done
}

# Function to wait for Kafka
wait_for_kafka() {
    local host=$1
    local port=$2
    local service_name=$3

    echo -e "${YELLOW}üîç Waiting for Kafka: $service_name...${NC}"

    local max_attempts=60
    local attempt=1

    while [ $attempt -le $max_attempts ]; do
        if timeout 5 bash -c "</dev/tcp/$host/$port" >/dev/null 2>&1; then
            # Additional check: try to list topics
            if kafka-topics --bootstrap-server "$host:$port" --list >/dev/null 2>&1; then
                echo -e "${GREEN}‚úÖ $service_name is ready${NC}"
                return 0
            fi
        fi

        if [ $attempt -eq $max_attempts ]; then
            echo -e "${RED}‚ùå $service_name failed to become ready after $max_attempts attempts${NC}"
            return 1
        fi

        echo -e "${YELLOW}‚è≥ Attempt $attempt/$max_attempts - $service_name not ready yet...${NC}"
        sleep 5
        ((attempt++))
    done
}

# Wait for all required services
echo -e "${YELLOW}üöÄ Starting service readiness checks...${NC}"

# Wait for PostgreSQL databases
if [ ! -z "$CLIENTS_DB_PASSWORD" ]; then
    wait_for_postgres "postgres-clients" "clients_user" "clients_db" "$CLIENTS_DB_PASSWORD" "PostgreSQL Clients"
fi

if [ ! -z "$TEMPLATES_DB_PASSWORD" ]; then
    wait_for_postgres "postgres-templates" "templates_user" "templates_db" "$TEMPLATES_DB_PASSWORD" "PostgreSQL Templates"
fi

# Wait for MySQL database
if [ ! -z "$AUTH_DB_PASSWORD" ]; then
    wait_for_mysql "mysql-auth" "auth_user" "auth_db" "$AUTH_DB_PASSWORD" "MySQL Auth"
fi

# Wait for Kafka (optional - only if we need to create topics)
if [ "$WAIT_FOR_KAFKA" = "true" ]; then
    wait_for_kafka "kafka-0.kafka-headless" "9092" "Kafka"
fi

echo -e "${GREEN}üéâ All required services are ready!${NC}"-------------------------------------------------
filepath = ./scripts/utils/setup-local-registry.sh
-------------------------------------------------
filepath = ./scripts/utils/generate_jwt_secret.sh
#!/bin/bash
# FILE: scripts/generate-jwt-secret.sh

# Generate a secure JWT secret
JWT_SECRET=$(openssl rand -base64 64 | tr -d '\n')

echo "Generated JWT Secret (save this securely):"
echo "$JWT_SECRET"
echo ""

# Create/update Kubernetes secret
kubectl create secret generic auth-secrets \
  --from-literal=jwt-secret="$JWT_SECRET" \
  -n ai-persona-system \
  --dry-run=client -o yaml | kubectl apply -f -

echo "JWT secret stored in Kubernetes"-------------------------------------------------
filepath = ./scripts/utils/package_module.sh
#!/bin/bash
#
# package_context.sh - A script to package the relevant files for a specific
#                      microservice, frontend, or infrastructure component into a
#                      single context file for AI assistants.
#
# This script is designed to work with the new agent-managed project structure.
#
# Usage: ./scripts/utils/package_context.sh [-o /path/to/output_dir] [component_name]
# Example: ./scripts/utils/package_context.sh auth-service
# Example: ./scripts/utils/package_context.sh code-all

set -e

# --- Self-locating Logic ---
# Ensures the script can be run from anywhere in the project.
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
# Get the full path to this script
SCRIPT_PATH="${SCRIPT_DIR}/$(basename "${BASH_SOURCE[0]}")"
PROJECT_ROOT=$( realpath "$SCRIPT_DIR/../../" )
cd "$PROJECT_ROOT"

# --- Configuration ---
DEFAULT_OUTPUT_DIR=$SCRIPT_DIR"/output_contexts"
ENVIRONMENT="production"
REGION="uk001"

# --- Component List ---
# List of all individual components for the 'all' option
ALL_COMPONENTS=(
    "code-all"
    "deployments-all"
    "environment-prod"
    "auth-service"
    "core-manager"
    "agent-chassis"
    "reasoning-agent"
    "user-frontend"
    "admin-dashboard"
    "agent-playground"
    "infra-cluster"
    "infra-kafka"
)

# --- Main Functions ---
# Helper function to write a single file's content to the output.
function write_file() {
  local file_path=$1
  local output_file=$2
  if [ -f "$file_path" ]; then
    echo "filepath = ./$file_path" >> "$output_file"
    cat "$file_path" >> "$output_file"
    echo "-------------------------------------------------" >> "$output_file"
  fi
}

# Helper function to write all files in a directory to the output.
function write_directory() {
  local dir_path=$1
  local output_file=$2
  if [ -d "$dir_path" ]; then
    # Using find with -print0 and while read is safe for filenames with spaces.
    while IFS= read -r -d $'\0' file; do
      write_file "$file" "$output_file"
    done < <(find "$dir_path" -type f -print0)
  fi
}

# --- Script Argument Parsing ---
OUTPUT_DIR=$DEFAULT_OUTPUT_DIR

while [[ "$1" =~ ^- && ! "$1" == "--" ]]; do
  case $1 in
    -o | --output)
      shift
      OUTPUT_DIR=$1
      ;;
  esac
  shift
done

COMPONENT_NAME=$1

# --- Help and Usage ---
function show_help() {
  echo "Usage: $0 [-o /path/to/output_dir] [component_name]"
  echo "Please provide the name of the component to package."
  echo ""
  echo "Available components:"
  echo "  - all                     # Package all individual components into separate files"
  echo ""
  echo "  # Horizontal Slices"
  echo "  - code-all                # All Go source code (cmd, internal, pkg, platform)"
  echo "  - deployments-all         # All deployment configurations (Terraform & Kustomize)"
  echo "  - environment-prod        # Production environment Terraform configurations"
  echo ""
  echo "  # Backend Services (Vertical Slices)"
  echo "  - auth-service"
  echo "  - core-manager"
  echo "  - agent-chassis"
  echo "  - reasoning-agent"
  echo ""
  echo "  # Frontend Applications (Vertical Slices)"
  echo "  - user-frontend"
  echo "  - admin-dashboard"
  echo "  - agent-playground"
  echo ""
  echo "  # Infrastructure Layers"
  echo "  - infra-cluster           # The core Rackspace Kubernetes cluster"
  echo "  - infra-kafka             # The Kafka cluster deployment"
}


if [ -z "$COMPONENT_NAME" ]; then
  show_help
  exit 1
fi

# If the component is 'all', loop and call the script for each component.
if [ "$COMPONENT_NAME" = "all" ]; then
  echo "Packaging all components into separate files..."
  mkdir -p "$OUTPUT_DIR"

  for component in "${ALL_COMPONENTS[@]}"; do
    echo "-------------------------------------------------"
    echo "--> Packaging component: $component"

    # Call the script recursively using its full path
    if [[ -n "$OUTPUT_DIR" && "$OUTPUT_DIR" != "$DEFAULT_OUTPUT_DIR" ]]; then
      bash "$SCRIPT_PATH" -o "$OUTPUT_DIR" "$component"
    else
      bash "$SCRIPT_PATH" "$component"
    fi

    # Display the file size for the component just created
    COMPONENT_FILE="${OUTPUT_DIR}/${component}_context.txt"
    if [ -f "$COMPONENT_FILE" ]; then
      FILE_SIZE=$(du -h "$COMPONENT_FILE" | cut -f1)
      echo "    üì¶ File size: $FILE_SIZE"
    fi
  done

  echo "-------------------------------------------------"
  echo "‚úÖ All components packaged."
  echo ""
  echo "Summary of generated files:"
  for component in "${ALL_COMPONENTS[@]}"; do
    COMPONENT_FILE="${OUTPUT_DIR}/${component}_context.txt"
    if [ -f "$COMPONENT_FILE" ]; then
      FILE_SIZE=$(du -h "$COMPONENT_FILE" | cut -f1)
      printf "  %-25s %10s\n" "${component}_context.txt" "$FILE_SIZE"
    fi
  done
  exit 0
fi

mkdir -p "$OUTPUT_DIR"
OUTPUT_FILE="${OUTPUT_DIR}/${COMPONENT_NAME}_context.txt"
> "$OUTPUT_FILE"

echo "Packaging component '$COMPONENT_NAME' into $OUTPUT_FILE..."

# --- Component Definitions ---
# Each case defines the specific source code, build, and deployment files
# that make up a complete, independent component.

# Shared files are included where necessary to provide full context.
SHARED_PLATFORM_CODE=("platform/" "pkg/")
SHARED_DEPLOYMENT_MODULES=("deployments/terraform/modules/kustomize-apply/")
SHARED_KUSTOMIZE_BASE=("deployments/kustomize/base/")
SHARED_ROOT_FILES=("Makefile" "go.mod" "go.sum" "docker-compose.yaml")

case "$COMPONENT_NAME" in
  # --- New Horizontal Slices ---
  code-all)
    MODULE_DIRS=( "cmd/" "internal/" "pkg/" "platform/" )
    MODULE_FILES=( "go.mod" "go.sum" )
    ;;

  deployments-all)
    MODULE_DIRS=( "deployments/" "build/docker/" )
    MODULE_FILES=( "Makefile" "docker-compose.yaml" )
    ;;

  environment-prod)
    MODULE_DIRS=( "deployments/terraform/environments/$ENVIRONMENT/" )
    MODULE_FILES=( "Makefile" )
    ;;

  # --- Backend Services ---
  auth-service)
    MODULE_DIRS=(
      "cmd/auth-service/" "internal/auth-service/"
      "deployments/kustomize/services/auth-service/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/services/core-platform/110-auth-service/"
      "${SHARED_PLATFORM_CODE[@]}" "${SHARED_DEPLOYMENT_MODULES[@]}" "${SHARED_KUSTOMIZE_BASE[@]}"
    )
    MODULE_FILES=(
      "build/docker/auth-service.dockerfile" "configs/auth-service.yaml"
      "${SHARED_ROOT_FILES[@]}"
    )
    ;;

  core-manager)
    MODULE_DIRS=(
      "cmd/core-manager/" "internal/core-manager/"
      "deployments/kustomize/services/core-manager/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/services/core-platform/120-core-manager/"
      "${SHARED_PLATFORM_CODE[@]}" "${SHARED_DEPLOYMENT_MODULES[@]}" "${SHARED_KUSTOMIZE_BASE[@]}"
    )
    MODULE_FILES=(
      "build/docker/core-manager.dockerfile" "configs/core-manager.yaml"
      "${SHARED_ROOT_FILES[@]}"
    )
    ;;

  agent-chassis)
    MODULE_DIRS=(
      "cmd/agent-chassis/" "platform/agentbase/"
      "deployments/kustomize/services/agent-chassis/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/services/agents/2210-agent-chassis/"
      "${SHARED_PLATFORM_CODE[@]}" "${SHARED_DEPLOYMENT_MODULES[@]}" "${SHARED_KUSTOMIZE_BASE[@]}"
    )
    MODULE_FILES=(
      "build/docker/agent-chassis.dockerfile" "configs/agent-chassis.yaml"
      "${SHARED_ROOT_FILES[@]}"
    )
    ;;

  reasoning-agent)
    MODULE_DIRS=(
      "cmd/reasoning-agent/" "internal/agents/reasoning/"
      "deployments/kustomize/services/reasoning-agent/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/services/agents/2220-reasoning-agent/"
      "${SHARED_PLATFORM_CODE[@]}" "${SHARED_DEPLOYMENT_MODULES[@]}" "${SHARED_KUSTOMIZE_BASE[@]}"
    )
    MODULE_FILES=(
      "build/docker/reasoning-agent.dockerfile" "configs/reasoning-agent.yaml"
      "${SHARED_ROOT_FILES[@]}"
    )
    ;;

  # --- Frontend Applications ---
  user-frontend)
    MODULE_DIRS=(
      "frontends/user-portal/"
      "deployments/kustomize/frontends/user-portal/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/services/frontends/3320-user-portal/"
      "${SHARED_DEPLOYMENT_MODULES[@]}" "${SHARED_KUSTOMIZE_BASE[@]}"
    )
    MODULE_FILES=( "Makefile" )
    ;;

  admin-dashboard)
    MODULE_DIRS=(
      "frontends/admin-dashboard/"
      "deployments/kustomize/frontends/admin-dashboard/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/services/frontends/3310-admin-dashboard/"
      "${SHARED_DEPLOYMENT_MODULES[@]}" "${SHARED_KUSTOMIZE_BASE[@]}"
    )
    MODULE_FILES=( "Makefile" )
    ;;

  agent-playground)
    MODULE_DIRS=(
      "frontends/agent-playground/"
      "deployments/kustomize/frontends/agent-playground/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/services/frontends/3330-agent-playground/"
      "${SHARED_DEPLOYMENT_MODULES[@]}" "${SHARED_KUSTOMIZE_BASE[@]}"
    )
    MODULE_FILES=( "Makefile" )
    ;;

  # --- Infrastructure Layers ---
  infra-cluster)
    MODULE_DIRS=(
      "deployments/terraform/modules/rackspace-kubernetes/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/010-infrastructure/"
    )
    MODULE_FILES=("Makefile")
    ;;

  infra-kafka)
    MODULE_DIRS=(
      "deployments/terraform/modules/strimzi-operator/"
      "deployments/terraform/modules/kafka-cluster/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/030-strimzi-operator/"
      "deployments/terraform/environments/$ENVIRONMENT/$REGION/040-kafka-cluster/"
      "deployments/kustomize/infrastructure/kafka/"
    )
    MODULE_FILES=("Makefile")
    ;;

  *)
    echo "Error: Unknown component '$COMPONENT_NAME'."
    show_help
    exit 1
    ;;
esac

# --- Packaging Logic ---
# This ensures that directories are processed before loose files.
for dir in "${MODULE_DIRS[@]}"; do
  write_directory "$dir" "$OUTPUT_FILE"
done
for file in "${MODULE_FILES[@]}"; do
  write_file "$file" "$OUTPUT_FILE"
done

echo "‚úÖ Done. Component context saved to $OUTPUT_FILE"
FILE_SIZE=$(du -h "$OUTPUT_FILE" | cut -f1)
echo "üì¶ File size: $FILE_SIZE"-------------------------------------------------
filepath = ./scripts/build/build-service.sh
-------------------------------------------------
filepath = ./scripts/build/build-frontend.sh
-------------------------------------------------
filepath = ./scripts/build/push-images.sh
-------------------------------------------------
filepath = ./scripts/build/build-all-images.sh
-------------------------------------------------
filepath = ./scripts/local/reset-dev-env.sh
-------------------------------------------------
filepath = ./scripts/local/deploy-from-laptop.sh
-------------------------------------------------
filepath = ./scripts/local/start-dev-env.sh
-------------------------------------------------
filepath = ./scripts/local/stop-dev-env.sh
-------------------------------------------------
filepath = ./scripts/test-system.sh
#!/bin/bash
# Test script to verify the system is working

set -e

echo "üß™ Testing AI Persona System"
echo "=========================="

# Colors
GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m'

# Test health endpoints
test_health() {
    local service=$1
    local port=$2

    echo -n "Testing $service health... "

    if curl -s -f http://localhost:$port/health > /dev/null; then
        echo -e "${GREEN}‚úì${NC}"
        return 0
    else
        echo -e "${RED}‚úó${NC}"
        return 1
    fi
}

# Test auth flow
test_auth() {
    echo -n "Testing authentication flow... "

    # Register a test user
    REGISTER_RESPONSE=$(curl -s -X POST http://localhost:8081/api/v1/auth/register \
        -H "Content-Type: application/json" \
        -d '{
            "email": "test@example.com",
            "password": "TestPass123!",
            "client_id": "test-client",
            "first_name": "Test",
            "last_name": "User"
        }')

    # Extract token
    TOKEN=$(echo $REGISTER_RESPONSE | jq -r '.access_token')

    if [ "$TOKEN" != "null" ] && [ ! -z "$TOKEN" ]; then
        echo -e "${GREEN}‚úì${NC}"
        echo "  Token: ${TOKEN:0:20}..."
        return 0
    else
        echo -e "${RED}‚úó${NC}"
        echo "  Response: $REGISTER_RESPONSE"
        return 1
    fi
}

# Test template listing
test_templates() {
    echo -n "Testing template listing... "

    # Assuming we have a token from previous test
    TEMPLATES=$(curl -s -X GET http://localhost:8088/api/v1/templates \
        -H "Authorization: Bearer $TOKEN")

    if echo $TEMPLATES | jq -e '.templates' > /dev/null; then
        echo -e "${GREEN}‚úì${NC}"
        TEMPLATE_COUNT=$(echo $TEMPLATES | jq '.templates | length')
        echo "  Found $TEMPLATE_COUNT templates"
        return 0
    else
        echo -e "${RED}‚úó${NC}"
        return 1
    fi
}

# Main test flow
echo ""
echo "1. Testing service health endpoints:"
test_health "Auth Service" 8081
test_health "Core Manager" 8088

echo ""
echo "2. Testing authentication:"
test_auth

echo ""
echo "3. Testing API endpoints:"
test_templates

echo ""
echo "‚úÖ Basic system tests complete!"
-------------------------------------------------
filepath = ./scripts/setup.sh
#!/bin/bash
# FILE: scripts/setup.sh
# Initial setup script for AI Persona System

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Functions
print_success() {
    echo -e "${GREEN}‚úÖ $1${NC}"
}

print_error() {
    echo -e "${RED}‚ùå $1${NC}"
}

print_info() {
    echo -e "${YELLOW}‚ÑπÔ∏è  $1${NC}"
}

# Check prerequisites
check_prerequisites() {
    echo "üîç Checking prerequisites..."

    local missing_deps=()

    # Check kubectl
    if ! command -v kubectl &> /dev/null; then
        missing_deps+=("kubectl")
    fi

    # Check docker
    if ! command -v docker &> /dev/null; then
        missing_deps+=("docker")
    fi

    # Check if Kubernetes is accessible
    if ! kubectl cluster-info &> /dev/null; then
        print_error "Cannot connect to Kubernetes cluster. Please ensure kubectl is configured."
        exit 1
    fi

    if [ ${#missing_deps[@]} -ne 0 ]; then
        print_error "Missing required dependencies: ${missing_deps[*]}"
        echo "Please install missing dependencies and try again."
        exit 1
    fi

    print_success "All prerequisites met"
}

# Create namespace
create_namespace() {
    print_info "Creating Kubernetes namespace..."
    kubectl apply -f k8s/namespace.yaml || {
        print_error "Failed to create namespace"
        exit 1
    }
    print_success "Namespace created"
}

# Generate random passwords
generate_password() {
    openssl rand -base64 32 | tr -d "=+/" | cut -c1-25
}

# Create secrets
create_secrets() {
    print_info "Creating secrets..."

    # Database secrets
    kubectl create secret generic db-secrets -n ai-persona-system \
        --from-literal=clients-db-password=$(generate_password) \
        --from-literal=templates-db-password=$(generate_password) \
        --from-literal=auth-db-password=$(generate_password) \
        --from-literal=mysql-root-password=$(generate_password) \
        --dry-run=client -o yaml | kubectl apply -f - || {
        print_error "Failed to create database secrets"
        exit 1
    }

    # MinIO secrets
    kubectl create secret generic minio-secrets -n ai-persona-system \
        --from-literal=access-key=$(generate_password) \
        --from-literal=secret-key=$(generate_password) \
        --dry-run=client -o yaml | kubectl apply -f - || {
        print_error "Failed to create MinIO secrets"
        exit 1
    }

    # Auth secrets
    kubectl create secret generic auth-secrets -n ai-persona-system \
        --from-literal=jwt-secret=$(generate_password) \
        --dry-run=client -o yaml | kubectl apply -f - || {
        print_error "Failed to create auth secrets"
        exit 1
    }

    # Grafana secrets
    kubectl create secret generic grafana-secrets -n ai-persona-system \
        --from-literal=admin-password=$(generate_password) \
        --dry-run=client -o yaml | kubectl apply -f - || {
        print_error "Failed to create Grafana secrets"
        exit 1
    }

    print_success "Basic secrets created"
}

# Get API keys from user
get_api_keys() {
    print_info "Setting up AI service API keys..."
    echo "Please provide your API keys (or press Enter to skip):"

    # Anthropic
    read -p "Anthropic API Key (for Claude): " anthropic_key
    if [ -z "$anthropic_key" ]; then
        anthropic_key="dummy-key-replace-me"
        print_info "Skipping Anthropic API key - reasoning agent will not work"
    fi

    # Stability AI
    read -p "Stability AI API Key (for image generation): " stability_key
    if [ -z "$stability_key" ]; then
        stability_key="dummy-key-replace-me"
        print_info "Skipping Stability API key - image generation will not work"
    fi

    # SERP API
    read -p "SERP API Key (for web search): " serp_key
    if [ -z "$serp_key" ]; then
        serp_key="dummy-key-replace-me"
        print_info "Skipping SERP API key - web search will not work"
    fi

    # Create AI secrets
    kubectl create secret generic ai-secrets -n ai-persona-system \
        --from-literal=anthropic-api-key="$anthropic_key" \
        --from-literal=stability-api-key="$stability_key" \
        --from-literal=serp-api-key="$serp_key" \
        --dry-run=client -o yaml | kubectl apply -f - || {
        print_error "Failed to create AI secrets"
        exit 1
    }

    print_success "AI service secrets created"
}

# Create service account and RBAC
create_rbac() {
    print_info "Setting up RBAC..."

    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: ai-persona-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: ai-persona-system
EOF

    print_success "RBAC configured"
}

# Main setup flow
main() {
    echo "üöÄ AI Persona System Setup"
    echo "========================="
    echo ""

    # Run checks
    check_prerequisites

    # Create namespace
    create_namespace

    # Create secrets
    create_secrets

    # Get API keys
    get_api_keys

    # Setup RBAC
    create_rbac

    echo ""
    print_success "Setup complete!"
    echo ""
    echo "Next steps:"
    echo "1. Build Docker images:    make build"
    echo "2. Deploy to Kubernetes:   make deploy"
    echo "3. Run migrations:         make migrate-up"
    echo "4. Seed initial data:      make seed-data"
    echo "5. Set up port forward:    make port-forward"
    echo ""
    echo "Or run everything at once: make quickstart"
    echo ""
}

# Run main function
main-------------------------------------------------
filepath = ./.env
# Database passwords
CLIENTS_DB_PASSWORD=your_password
TEMPLATES_DB_PASSWORD=your_password
AUTH_DB_PASSWORD=your_password
MYSQL_ROOT_PASSWORD=your_password

# Object storage
MINIO_ACCESS_KEY=your_access_key
MINIO_SECRET_KEY=your_secret_key

# AI Services
ANTHROPIC_API_KEY=your_api_key

# Auth
JWT_SECRET_KEY=your_jwt_secret

# repo specific private token
GITHUB_TOKEN=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-------------------------------------------------
filepath = ./configs/agent-chassis.yaml
# FILE: configs/agent-chassis.yaml
service_info:
  name: "agent-chassis"
  version: "1.0.0"
  environment: "development"

server:
  port: "8085"

logging:
  level: "info"

observability:
  tracing_endpoint: "otel-collector.monitoring.svc.cluster.local:4317"

infrastructure:
  kafka_brokers:
    - "kafka-0.kafka-headless:9092"
    - "kafka-1.kafka-headless:9092"
    - "kafka-2.kafka-headless:9092"

  clients_database:
    host: "postgres-clients.database.svc.cluster.local"
    port: 5432
    user: "clients_user"
    password_env_var: "CLIENTS_DB_PASSWORD"
    db_name: "clients_db"
    sslmode: "disable"

  templates_database:
    host: "postgres-templates.database.svc.cluster.local"
    port: 5432
    user: "templates_user"
    password_env_var: "TEMPLATES_DB_PASSWORD"
    db_name: "templates_db"
    sslmode: "disable"

  auth_database: {}

  object_storage:
    provider: "s3"
    endpoint: "http://minio.storage.svc.cluster.local:9000"
    bucket: "agent-artifacts"
    access_key_env_var: "MINIO_ACCESS_KEY"
    secret_key_env_var: "MINIO_SECRET_KEY"-------------------------------------------------
filepath = ./configs/web-search-adapter.yaml
// FILE: configs/web-search-adapter.yaml
service_info:
  name: "web-search-adapter"
  version: "1.0.0"
  environment: "development"

server:
  port: "8083"

logging:
  level: "info"

infrastructure:
  kafka_brokers:
    - "kafka-0.kafka-headless:9092"
    - "kafka-1.kafka-headless:9092"
    - "kafka-2.kafka-headless:9092"

  clients_database: {}
  templates_database: {}
  auth_database: {}
  object_storage: {}

custom:
  search_provider: "serpapi"
  max_results_default: 10
  timeout_seconds: 30-------------------------------------------------
filepath = ./configs/auth-service.yaml
# FILE: configs/auth-service.yaml
service_info:
  name: "personae-auth-service"
  version: "1.1.0"
  environment: "development"

server:
  port: "8081"

logging:
  level: "debug"

observability:
  tracing_endpoint: "otel-collector.monitoring.svc.cluster.local:4317"

infrastructure:
  auth_database:
    host: "mysql-auth.database.svc.cluster.local"
    port: 3306
    user: "auth_user"
    password_env_var: "AUTH_DB_PASSWORD"
    db_name: "auth_db"
    ssl_mode: "disable"
  
  kafka_brokers: []
  clients_database: {}
  templates_database: {}
  object_storage: {}

custom:
  jwt_secret_key_env_var: "JWT_SECRET_KEY"
  jwt_expiry_access_minutes: 60
  allowed_origins:
    - "http://localhost:3000"
    - "http://localhost:8080"
  core_manager_url: "http://core-manager:8088"
  tiers:
    free_tier:
      max_personas_allowed: 1
      max_content_allowed: 5
    premium_tier:
      max_personas_allowed: -1
      max_content_allowed: -1-------------------------------------------------
filepath = ./configs/reasoning-agent.yaml
# FILE: configs/reasoning-agent.yaml
service_info:
  name: "reasoning-agent"
  version: "1.0.0"
  environment: "development"

server:
  port: "8082"

logging:
  level: "info"

infrastructure:
  kafka_brokers:
    - "kafka-0.kafka-headless:9092"
    - "kafka-1.kafka-headless:9092"
    - "kafka-2.kafka-headless:9092"

  clients_database: {}
  templates_database: {}
  auth_database: {}
  object_storage: {}

custom:
  ai_service:
    provider: "anthropic"
    model: "claude-3-opus-20240229"
    temperature: 0.2
    max_tokens: 2048
    api_key_env_var: "ANTHROPIC_API_KEY"-------------------------------------------------
filepath = ./configs/core-manager.yaml
# FILE: configs/core-manager.yaml
service_info:
  name: "core-manager"
  version: "1.0.0"
  environment: "development"

server:
  port: "8088"

logging:
  level: "info"

observability:
  tracing_endpoint: "otel-collector.monitoring.svc.cluster.local:4317"

infrastructure:
  kafka_brokers:
    - "kafka-0.kafka-headless:9092"
    - "kafka-1.kafka-headless:9092"
    - "kafka-2.kafka-headless:9092"
  
  clients_database:
    host: "postgres-clients.database.svc.cluster.local"
    port: 5432
    user: "clients_user"
    password_env_var: "CLIENTS_DB_PASSWORD"
    db_name: "clients_db"
    sslmode: "disable"
  
  templates_database:
    host: "postgres-templates.database.svc.cluster.local"
    port: 5432
    user: "templates_user"
    password_env_var: "TEMPLATES_DB_PASSWORD"
    db_name: "templates_db"
    sslmode: "disable"
  
  auth_database: {}
  
  object_storage:
    provider: "s3"
    endpoint: "http://minio.storage.svc.cluster.local:9000"
    bucket: "agent-artifacts"
    access_key_env_var: "MINIO_ACCESS_KEY"
    secret_key_env_var: "MINIO_SECRET_KEY"

custom:
  jwt_secret_env_var: "JWT_SECRET_KEY"  # Add this
  auth_service_url: "http://auth-service:8081"  # Add this for validation-------------------------------------------------
filepath = ./cmd/agent-chassis/main.go
// FILE: cmd/agent-chassis/main.go (updated)
package main

import (
	"context"
	"flag"
	"log"
	"os"
	"os/signal"
	"syscall"

	"github.com/gqls/agentchassis/platform/agentbase"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/logger"
	"go.uber.org/zap"
)

func main() {
	// Load configuration
	configPath := flag.String("config", "configs/agent-chassis.yaml", "Path to config file")
	flag.Parse()

	cfg, err := config.Load(*configPath)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	// Initialize logger
	appLogger, err := logger.New(cfg.Logging.Level)
	if err != nil {
		log.Fatalf("Failed to initialize logger: %v", err)
	}
	defer appLogger.Sync()

	// Create context
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Initialize agent
	agent, err := agentbase.New(ctx, cfg, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to initialize agent", zap.Error(err))
	}

	// Handle shutdown
	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)

	// Run agent in goroutine
	errCh := make(chan error, 1)
	go func() {
		if err := agent.Run(); err != nil {
			errCh <- err
		}
	}()

	// Wait for shutdown signal or error
	select {
	case <-sigCh:
		appLogger.Info("Shutdown signal received")
		cancel()
		if err := agent.Shutdown(); err != nil {
			appLogger.Error("Shutdown error", zap.Error(err))
		}
	case err := <-errCh:
		appLogger.Error("Agent failed", zap.Error(err))
		cancel()
		agent.Shutdown()
	}

	appLogger.Info("Agent stopped")
}
-------------------------------------------------
filepath = ./cmd/auth-service/main.go
// FILE: cmd/auth-service/main.go
package main

import (
	"context"
	"errors"
	"flag"
	"log"
	"net/http"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/gqls/agentchassis/internal/auth-service/admin"
	"github.com/gqls/agentchassis/internal/auth-service/auth"
	"github.com/gqls/agentchassis/internal/auth-service/gateway"
	"github.com/gqls/agentchassis/internal/auth-service/jwt"
	"github.com/gqls/agentchassis/internal/auth-service/middleware"
	"github.com/gqls/agentchassis/internal/auth-service/project"
	"github.com/gqls/agentchassis/internal/auth-service/subscription"
	"github.com/gqls/agentchassis/internal/auth-service/user"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/database"
	"github.com/gqls/agentchassis/platform/logger"
	"github.com/rs/cors"
	"go.uber.org/zap"
)

func main() {
	// --- Step 1: Load Configuration using the Platform Library ---
	configPath := flag.String("config", "configs/auth-service.yaml", "Path to config file")
	flag.Parse()

	cfg, err := config.Load(*configPath)
	if err != nil {
		log.Fatalf("CRITICAL: Failed to load configuration: %v", err)
	}

	// --- Step 2: Initialize Logger using the Platform Library ---
	appLogger, err := logger.New(cfg.Logging.Level)
	if err != nil {
		log.Fatalf("CRITICAL: Failed to initialize logger: %v", err)
	}
	defer appLogger.Sync()

	appLogger.Info("Auth Service starting",
		zap.String("service_name", cfg.ServiceInfo.Name),
		zap.String("version", cfg.ServiceInfo.Version),
		zap.String("environment", cfg.ServiceInfo.Environment),
	)

	// --- Step 3: Initialize Database Connection using the Platform Library ---
	// The auth service uses MySQL.
	db, err := database.NewMySQLConnection(context.Background(), cfg.Infrastructure.AuthDatabase, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to connect to the auth database", zap.Error(err))
	}
	defer db.Close()

	// --- Step 4: Initialize All Services and Handlers ---

	// Extract JWT configuration from environment and config
	jwtSecret := os.Getenv("JWT_SECRET_KEY")
	if jwtSecret == "" {
		appLogger.Fatal("JWT_SECRET_KEY environment variable not set")
	}

	// Get JWT expiry from config
	jwtExpiryMinutes := 60 // default
	if cfg.Custom != nil {
		if expiry, ok := cfg.Custom["jwt_expiry_access_minutes"].(float64); ok {
			jwtExpiryMinutes = int(expiry)
		}
	}

	// Initialize JWT service
	jwtSvc, err := jwt.NewService(jwtSecret, jwtExpiryMinutes, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to initialize JWT Service", zap.Error(err))
	}

	// Initialize repositories
	userRepo := user.NewRepository(db, appLogger)
	//adminRepo := admin.NewRepository(db, appLogger, nil) // admin repo doesn't need config
	projectRepo := project.NewRepository(db, appLogger)
	subscriptionRepo := subscription.NewRepository(db, appLogger)

	// Initialize services
	userSvc := user.NewService(userRepo, appLogger)
	authSvc := auth.NewService(userSvc, jwtSvc, appLogger)
	gatewaySvc := gateway.NewService(cfg, appLogger)
	subscriptionSvc := subscription.NewService(subscriptionRepo, appLogger)

	// Initialize handlers
	authHandlers := auth.NewHandlers(authSvc)
	userHandlers := user.NewHandlers(userSvc)
	projectHandler := project.NewHTTPHandler(projectRepo, appLogger)
	subscriptionHandlers := subscription.NewHandlers(subscriptionSvc)
	subscriptionAdminHandlers := subscription.NewAdminHandlers(subscriptionSvc, appLogger)
	gatewayHandler := gateway.NewHTTPHandler(gatewaySvc, appLogger)
	adminHandlers := admin.NewHandlers(userRepo, appLogger)

	// --- Step 5: Setup Routing and Middleware ---
	// Using Gin router for consistency with handlers
	router := gin.New()
	router.Use(gin.Recovery())

	// Public routes (no auth required)
	router.GET("/health", func(c *gin.Context) {
		c.JSON(http.StatusOK, gin.H{
			"status":  "healthy",
			"service": cfg.ServiceInfo.Name,
			"version": cfg.ServiceInfo.Version,
		})
	})

	// Auth endpoints (public)
	authGroup := router.Group("/api/v1/auth")
	{
		authGroup.POST("/register", authHandlers.HandleRegister)
		authGroup.POST("/login", authHandlers.HandleLogin)
		authGroup.POST("/refresh", authHandlers.HandleRefresh)
		authGroup.POST("/validate", authHandlers.HandleValidate)
		authGroup.POST("/logout", middleware.RequireAuth(jwtSvc, appLogger), authHandlers.HandleLogout)
	}

	// User endpoints (protected)
	userGroup := router.Group("/api/v1/user")
	userGroup.Use(middleware.RequireAuth(jwtSvc, appLogger))
	{
		userGroup.GET("/profile", userHandlers.HandleGetCurrentUser)
		userGroup.PUT("/profile", userHandlers.HandleUpdateCurrentUser)
		userGroup.POST("/password", userHandlers.HandleChangePassword)
		userGroup.DELETE("/delete", userHandlers.HandleDeleteAccount)
	}

	// Subscription endpoints (protected)
	subGroup := router.Group("/api/v1/subscription")
	subGroup.Use(middleware.RequireAuth(jwtSvc, appLogger))
	{
		subGroup.GET("", subscriptionHandlers.HandleGetSubscription)
		subGroup.GET("/usage", subscriptionHandlers.HandleGetUsageStats)
		subGroup.GET("/check-quota", subscriptionHandlers.HandleCheckQuota)
	}

	// Project endpoints (protected)
	projectGroup := router.Group("/api/v1/projects")
	projectGroup.Use(middleware.RequireAuth(jwtSvc, appLogger))
	{
		projectGroup.GET("", wrapHTTPHandler(projectHandler.ListProjects))
		projectGroup.POST("", wrapHTTPHandler(projectHandler.CreateProject))
		projectGroup.GET("/:id", wrapProjectHandler(projectHandler.GetProject))
		projectGroup.PUT("/:id", wrapProjectHandler(projectHandler.UpdateProject))
		projectGroup.DELETE("/:id", wrapProjectHandler(projectHandler.DeleteProject))
	}

	// Admin endpoints (protected + admin role)
	adminGroup := router.Group("/api/v1/admin")
	adminGroup.Use(middleware.RequireAuth(jwtSvc, appLogger))
	adminGroup.Use(middleware.RequireRole("admin"))
	{
		// User management (handled by auth-service)
		adminGroup.GET("/users", adminHandlers.HandleListUsers)
		adminGroup.GET("/users/:user_id", adminHandlers.HandleGetUser)
		adminGroup.PUT("/users/:user_id", adminHandlers.HandleUpdateUser)
		adminGroup.DELETE("/users/:user_id", adminHandlers.HandleDeleteUser)
		adminGroup.GET("/users/:user_id/activity", adminHandlers.HandleGetUserActivity)
		adminGroup.POST("/users/:user_id/permissions", adminHandlers.HandleGrantPermission)
		adminGroup.DELETE("/users/:user_id/permissions/:permission_name", adminHandlers.HandleRevokePermission)

		// Subscription management (handled by auth-service)
		adminGroup.GET("/subscriptions", subscriptionAdminHandlers.HandleListSubscriptions)
		adminGroup.POST("/subscriptions", subscriptionAdminHandlers.HandleCreateSubscription)
		adminGroup.PUT("/subscriptions/:user_id", wrapAdminSubscriptionHandler(subscriptionAdminHandlers.HandleUpdateSubscription))

		// Routes to be proxied to core-manager
		adminGroup.Any("/clients", gatewayHandler.HandleAdminRoutes)
		adminGroup.Any("/clients/*path", gatewayHandler.HandleAdminRoutes)
		adminGroup.Any("/system/*path", gatewayHandler.HandleAdminRoutes)
		adminGroup.Any("/workflows/*path", gatewayHandler.HandleAdminRoutes)
		adminGroup.Any("/agent-definitions/*path", gatewayHandler.HandleAdminRoutes)

	}

	// Gateway proxy endpoints (protected)
	gatewayGroup := router.Group("/api/v1")
	gatewayGroup.Use(middleware.RequireAuth(jwtSvc, appLogger))
	{
		// Template management (admin only)
		templateGroup := gatewayGroup.Group("/templates")
		templateGroup.Use(middleware.RequireRole("admin"))
		{
			templateGroup.Any("", gatewayHandler.HandleTemplateRoutes)
			templateGroup.Any("/*path", gatewayHandler.HandleTemplateRoutes)
		}

		// Instance management
		gatewayGroup.Any("/personas/instances", gatewayHandler.HandleInstanceRoutes)
		gatewayGroup.Any("/personas/instances/*path", gatewayHandler.HandleInstanceRoutes)
	}

	// WebSocket endpoint
	router.GET("/ws", middleware.RequireAuth(jwtSvc, appLogger), gatewayHandler.HandleWebSocket)

	// Apply CORS middleware
	allowedOrigins := []string{"*"} // default
	if cfg.Custom != nil {
		if origins, ok := cfg.Custom["allowed_origins"].([]interface{}); ok {
			allowedOrigins = make([]string, len(origins))
			for i, origin := range origins {
				allowedOrigins[i] = origin.(string)
			}
		}
	}

	corsConfig := cors.New(cors.Options{
		AllowedOrigins:   allowedOrigins,
		AllowedMethods:   []string{"GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"},
		AllowedHeaders:   []string{"Authorization", "Content-Type", "X-Requested-With"},
		AllowCredentials: true,
		MaxAge:           300,
	})

	// --- Step 6: Start Server and Handle Graceful Shutdown ---
	server := &http.Server{
		Addr:    ":" + cfg.Server.Port,
		Handler: corsConfig.Handler(router),
	}

	// Start server in a goroutine
	go func() {
		appLogger.Info("Auth Service listening", zap.String("address", server.Addr))
		if err := server.ListenAndServe(); err != nil && !errors.Is(err, http.ErrServerClosed) {
			appLogger.Fatal("Auth Service listen and serve error", zap.Error(err))
		}
	}()

	// Wait for interrupt signal
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit
	appLogger.Info("Shutdown signal received, shutting down auth server...")

	// Graceful shutdown with timeout
	ctxShutdown, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	if err := server.Shutdown(ctxShutdown); err != nil {
		appLogger.Fatal("Auth Server forced to shutdown due to error", zap.Error(err))
	}
	appLogger.Info("Auth Server exited gracefully")
}

// wrapHTTPHandler wraps standard http handlers to work with gin
func wrapHTTPHandler(fn func(http.ResponseWriter, *http.Request)) gin.HandlerFunc {
	return func(c *gin.Context) {
		fn(c.Writer, c.Request)
	}
}

// wrapProjectHandler wraps project handlers that take an ID parameter
func wrapProjectHandler(fn func(http.ResponseWriter, *http.Request, string)) gin.HandlerFunc {
	return func(c *gin.Context) {
		id := c.Param("id")
		fn(c.Writer, c.Request, id)
	}
}

// wrapAdminSubscriptionHandler wraps the admin subscription update handler
func wrapAdminSubscriptionHandler(fn func(*gin.Context)) gin.HandlerFunc {
	return func(c *gin.Context) {
		// The user_id is already available as a URL parameter
		c.Set("param_user_id", c.Param("user_id"))
		fn(c)
	}
}
-------------------------------------------------
filepath = ./cmd/web-search-adapter/main.go
// FILE: cmd/web-search-adapter/main.go
package main

import (
	"context"
	"flag"
	"log"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/gqls/agentchassis/internal/adapters/websearch"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/logger"
	"go.uber.org/zap"
)

func main() {
	configPath := flag.String("config", "configs/web-search-adapter.yaml", "Path to config file")
	flag.Parse()

	cfg, err := config.Load(*configPath)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	appLogger, err := logger.New(cfg.Logging.Level)
	if err != nil {
		log.Fatalf("Failed to initialize logger: %v", err)
	}
	defer appLogger.Sync()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	adapter, err := websearch.NewAdapter(ctx, cfg, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to initialize web search adapter", zap.Error(err))
	}

	go func() {
		if err := adapter.Run(); err != nil {
			appLogger.Error("Web search adapter failed", zap.Error(err))
			cancel()
		}
	}()

	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit
	appLogger.Info("Shutdown signal received")

	cancel()
	time.Sleep(2 * time.Second)
	appLogger.Info("Web search adapter stopped")
}
-------------------------------------------------
filepath = ./cmd/reasoning-agent/Dockerfile
# FILE: Dockerfile.reasoning
# A dedicated Dockerfile for building the reasoning agent service.

# Stage 1: Build the application
FROM golang:1.21-alpine AS builder

WORKDIR /app

# Copy modules and download dependencies
COPY go.mod go.sum ./
RUN go mod download

# Copy the entire project context
COPY . .

# Build the specific service binary
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /app/reasoning-agent ./cmd/reasoning-agent

# Stage 2: Create the final small image
FROM alpine:latest
RUN apk --no-cache add ca-certificates

# Create a non-root user for security
RUN addgroup -S appgroup && adduser -S -G appgroup appuser

WORKDIR /app

# Copy only the compiled binary from the builder stage
COPY --from=builder /app/reasoning-agent /app/reasoning-agent

# Copy its specific configuration file
COPY configs/reasoning-agent.yaml /app/configs/reasoning-agent.yaml

RUN chown appuser:appgroup /app/reasoning-agent

USER appuser

# The command to run the service, pointing to its own config
CMD ["./reasoning-agent", "-config", "configs/reasoning-agent.yaml"]
-------------------------------------------------
filepath = ./cmd/reasoning-agent/main.go
// FILE: cmd/reasoning-agent/main.go
package main

import (
	"context"
	"flag"
	"log"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/gqls/agentchassis/internal/agents/reasoning"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/logger"
	"go.uber.org/zap"
)

func main() {
	configPath := flag.String("config", "configs/reasoning-agent.yaml", "Path to config file")
	flag.Parse()

	cfg, err := config.Load(*configPath)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	appLogger, err := logger.New(cfg.Logging.Level)
	if err != nil {
		log.Fatalf("Failed to initialize logger: %v", err)
	}
	defer appLogger.Sync()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	agent, err := reasoning.NewAgent(ctx, cfg, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to initialize reasoning agent", zap.Error(err))
	}

	// Start health endpoint HERE
	agent.StartHealthServer("9090")

	// Start the agent's main run loop in a goroutine
	go func() {
		if err := agent.Run(); err != nil {
			appLogger.Error("Reasoning agent failed to run", zap.Error(err))
			cancel()
		}
	}()

	// Wait for shutdown signal
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit
	appLogger.Info("Shutdown signal received, shutting down reasoning agent...")

	cancel()
	time.Sleep(2 * time.Second)
	appLogger.Info("Reasoning agent service stopped.")
}
-------------------------------------------------
filepath = ./cmd/core-manager/main.go
// FILE: cmd/core-manager/main.go
package main

import (
	"context"
	"errors"
	"flag"
	"log"
	"net/http"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/gqls/agentchassis/internal/core-manager/api"

	// Platform packages
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/database"
	"github.com/gqls/agentchassis/platform/logger"

	"go.uber.org/zap"
)

func main() {
	// --- Step 1: Load Configuration using the Platform Library ---
	configPath := flag.String("config", "configs/core-manager.yaml", "Path to config file")
	flag.Parse()

	// Use the standardized platform loader
	cfg, err := config.Load(*configPath)
	if err != nil {
		log.Fatalf("Failed to load configuration: %v", err)
	}

	// --- Step 2: Initialize Logger using the Platform Library ---
	appLogger, err := logger.New(cfg.Logging.Level)
	if err != nil {
		log.Fatalf("Failed to initialize logger: %v", err)
	}
	defer appLogger.Sync()

	appLogger.Info("Core Manager Service starting",
		zap.String("service_name", cfg.ServiceInfo.Name),
		zap.String("version", cfg.ServiceInfo.Version),
		zap.String("environment", cfg.ServiceInfo.Environment),
		zap.String("log_level", cfg.Logging.Level),
	)

	// Create a main context that can be cancelled for graceful shutdown
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// --- Step 3: Initialize Database Connections using the Platform Library ---
	// 3a. Create connection pool for the Templates Database
	templatesPool, err := database.NewPostgresConnection(ctx, cfg.Infrastructure.TemplatesDatabase, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to initialize templates database connection", zap.Error(err))
	}
	defer templatesPool.Close()

	// 3b. Create connection pool for the Clients Database
	clientsPool, err := database.NewPostgresConnection(ctx, cfg.Infrastructure.ClientsDatabase, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to initialize clients database connection", zap.Error(err))
	}
	defer clientsPool.Close()

	// --- Step 4: Initialize and Start the API Server ---
	apiServer, err := api.NewServer(ctx, cfg, appLogger, templatesPool, clientsPool)
	if err != nil {
		appLogger.Fatal("Failed to initialize API server", zap.Error(err))
	}

	// Run the server in a goroutine so it doesn't block
	go func() {
		appLogger.Info("Starting HTTP server", zap.String("address", apiServer.Address()))
		if err := apiServer.Start(); err != nil && !errors.Is(err, http.ErrServerClosed) {
			appLogger.Error("API server failed", zap.Error(err))
			cancel() // Trigger shutdown on server error
		}
	}()

	// --- Step 5: Handle Graceful Shutdown ---
	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)
	receivedSignal := <-sigCh
	appLogger.Info("Shutdown signal received", zap.String("signal", receivedSignal.String()))

	// Graceful shutdown with timeout
	shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer shutdownCancel()

	if err := apiServer.Shutdown(shutdownCtx); err != nil {
		appLogger.Error("Error during graceful shutdown", zap.Error(err))
	}

	appLogger.Info("Core Manager Service stopped")
}
-------------------------------------------------
filepath = ./cmd/image-generator-adapter/main.go
// FILE: cmd/image-generator-adapter/main.go
package main

import (
	"context"
	"flag"
	"log"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/gqls/agentchassis/internal/adapters/imagegenerator"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/logger"
	"go.uber.org/zap"
)

func main() {
	configPath := flag.String("config", "configs/image-adapter.yaml", "Path to config file")
	flag.Parse()

	cfg, err := config.Load(*configPath)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	appLogger, err := logger.New(cfg.Logging.Level)
	if err != nil {
		log.Fatalf("Failed to initialize logger: %v", err)
	}
	defer appLogger.Sync()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	adapter, err := imagegenerator.NewAdapter(ctx, cfg, appLogger)
	if err != nil {
		appLogger.Fatal("Failed to initialize image generator adapter", zap.Error(err))
	}

	// Start health endpoint HERE
	adapter.StartHealthServer("9090")

	// Start the adapter's main run loop in a goroutine
	go func() {
		if err := adapter.Run(); err != nil {
			appLogger.Error("Image generator adapter failed to run", zap.Error(err))
			cancel()
		}
	}()

	// Wait for shutdown signal
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit
	appLogger.Info("Shutdown signal received, shutting down image generator adapter...")

	cancel()
	time.Sleep(2 * time.Second)
	appLogger.Info("Image generator adapter stopped.")
}
-------------------------------------------------
filepath = ./frontends/user-portal/env.example
-------------------------------------------------
filepath = ./frontends/user-portal/nginx.conf
-------------------------------------------------
filepath = ./frontends/user-portal/src/App.tsx
-------------------------------------------------
filepath = ./frontends/user-portal/tsconfig.json
-------------------------------------------------
filepath = ./frontends/user-portal/package.json
-------------------------------------------------
filepath = ./frontends/user-portal/Dockerfile
-------------------------------------------------
filepath = ./frontends/admin-dashboard/env.example
-------------------------------------------------
filepath = ./frontends/admin-dashboard/nginx.conf
-------------------------------------------------
filepath = ./frontends/admin-dashboard/src/App.tsx
-------------------------------------------------
filepath = ./frontends/admin-dashboard/tsconfig.json
-------------------------------------------------
filepath = ./frontends/admin-dashboard/package.json
-------------------------------------------------
filepath = ./frontends/admin-dashboard/Dockerfile
-------------------------------------------------
filepath = ./frontends/agent-playground/env.example
-------------------------------------------------
filepath = ./frontends/agent-playground/nginx.conf
-------------------------------------------------
filepath = ./frontends/agent-playground/src/App.tsx
-------------------------------------------------
filepath = ./frontends/agent-playground/tsconfig.json
-------------------------------------------------
filepath = ./frontends/agent-playground/package.json
-------------------------------------------------
filepath = ./frontends/agent-playground/Dockerfile
-------------------------------------------------
filepath = ./frontends/shared/api-client/package.json
-------------------------------------------------
filepath = ./frontends/shared/ui-components/package.json
-------------------------------------------------
filepath = ./k8s/secrets-template.yaml
# This is a template - DO NOT apply directly
# Use the setup.sh script or create manually
apiVersion: v1
kind: Secret
metadata:
  name: db-secrets
  namespace: ai-persona-system
type: Opaque
stringData:
  clients-db-password: "CHANGE_ME"
  templates-db-password: "CHANGE_ME"
  auth-db-password: "CHANGE_ME"
  mysql-root-password: "CHANGE_ME"

---
apiVersion: v1
kind: Secret
metadata:
  name: minio-secrets
  namespace: ai-persona-system
type: Opaque
stringData:
  access-key: "CHANGE_ME"
  secret-key: "CHANGE_ME"

---
apiVersion: v1
kind: Secret
metadata:
  name: auth-secrets
  namespace: ai-persona-system
type: Opaque
stringData:
  jwt-secret: "CHANGE_ME"

---
apiVersion: v1
kind: Secret
metadata:
  name: ai-secrets
  namespace: ai-persona-system
type: Opaque
stringData:
  anthropic-api-key: "CHANGE_ME"
  stability-api-key: "CHANGE_ME"
  serp-api-key: "CHANGE_ME"

---
apiVersion: v1
kind: Secret
metadata:
  name: grafana-secrets
  namespace: ai-persona-system
type: Opaque
stringData:
  admin-password: "CHANGE_ME"-------------------------------------------------
filepath = ./k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ai-persona-ingress
  namespace: ai-persona-system
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  tls:
    - hosts:
        - api.aipersona.example.com
        - grafana.aipersona.example.com
      secretName: api-tls
  rules:
    - host: api.aipersona.example.com
      http:
        paths:
          - path: /api/v1/auth
            pathType: Prefix
            backend:
              service:
                name: auth-service
                port:
                  number: 8081
          - path: /api/v1
            pathType: Prefix
            backend:
              service:
                name: auth-service
                port:
                  number: 8081
          - path: /ws
            pathType: Prefix
            backend:
              service:
                name: core-manager
                port:
                  number: 8088
    - host: grafana.aipersona.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: grafana
                port:
                  number: 3000-------------------------------------------------
filepath = ./k8s/monitoring/service-monitor.yaml
# ServiceMonitor for Auth Service
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: auth-service
  namespace: ai-persona-system
  labels:
    app: auth-service
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: auth-service
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
      scheme: http

---
# ServiceMonitor for Core Manager
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: core-manager
  namespace: ai-persona-system
  labels:
    app: core-manager
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: core-manager
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
      scheme: http

---
# ServiceMonitor for Agent Chassis
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: agent-chassis
  namespace: ai-persona-system
  labels:
    app: agent-chassis
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: agent-chassis
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
      scheme: http

---
# ServiceMonitor for Reasoning Agent
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: reasoning-agent
  namespace: ai-persona-system
  labels:
    app: reasoning-agent
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: reasoning-agent
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
      scheme: http

---
# ServiceMonitor for Image Generator Adapter
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: image-generator-adapter
  namespace: ai-persona-system
  labels:
    app: image-generator-adapter
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: image-generator-adapter
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
      scheme: http

---
# ServiceMonitor for PostgreSQL Exporters
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: postgres-exporter
  namespace: ai-persona-system
  labels:
    app: postgres-exporter
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: postgres-exporter
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
      scheme: http

---
# ServiceMonitor for Kafka
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kafka-metrics
  namespace: ai-persona-system
  labels:
    app: kafka
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: kafka
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
      scheme: http

---
# ServiceMonitor for MinIO
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: minio-metrics
  namespace: ai-persona-system
  labels:
    app: minio
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: minio
  endpoints:
    - port: api
      interval: 30s
      path: /minio/v2/metrics/cluster
      scheme: http-------------------------------------------------
filepath = ./k8s/monitoring/grafana-dashboard-configmap.yaml
// FILE: k8s/grafana-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-ai-persona
  namespace: ai-persona-system
data:
  ai-persona-dashboard.json: |
    {
      "dashboard": {
        "title": "AI Persona System Dashboard",
        "panels": [
          {
            "title": "Workflow Success Rate",
            "targets": [
              {
                "expr": "rate(workflows_total{status=\"completed\"}[5m]) / rate(workflows_total[5m])"
              }
            ],
            "type": "graph"
          },
          {
            "title": "Agent Response Times",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(agent_response_duration_bucket[5m]))"
              }
            ],
            "type": "graph"
          },
          {
            "title": "Kafka Consumer Lag",
            "targets": [
              {
                "expr": "kafka_consumer_lag"
              }
            ],
            "type": "graph"
          },
          {
            "title": "Active Workflows",
            "targets": [
              {
                "expr": "orchestrator_active_workflows"
              }
            ],
            "type": "stat"
          }
        ],
        "refresh": "5s",
        "time": {
          "from": "now-1h",
          "to": "now"
        }
      }
    }-------------------------------------------------
filepath = ./k8s/monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: ai-persona-system
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    # Alertmanager configuration
    alerting:
      alertmanagers:
        - static_configs:
            - targets: []

    # Load rules once and periodically evaluate them
    rule_files:
      - '/etc/prometheus/rules/*.yml'

    scrape_configs:
      # Scrape Prometheus itself
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
      
      # Kubernetes SD for pods
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - ai-persona-system
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

  alerts.yml: |
    groups:
      - name: ai-persona-alerts
        interval: 30s
        rules:
          - alert: HighWorkflowFailureRate
            expr: rate(workflows_total{status="failed"}[5m]) > 0.1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High workflow failure rate detected"
              description: "{{ $labels.instance }} has a workflow failure rate above 10% (current value: {{ $value }})"
          
          - alert: LowAgentAvailability
            expr: up{job="kubernetes-pods", app="agent-chassis"} < 3
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Low agent chassis availability"
              description: "Less than 3 agent chassis pods are running"
          
          - alert: DatabaseDown
            expr: up{job="kubernetes-pods", app=~"postgres-.*|mysql-.*"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Database {{ $labels.app }} is down"
              description: "Database {{ $labels.app }} has been down for more than 1 minute"
          
          - alert: KafkaLag
            expr: kafka_consumer_lag > 1000
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High Kafka consumer lag"
              description: "Consumer group {{ $labels.group }} has lag > 1000 messages"-------------------------------------------------
filepath = ./k8s/monitoring/grafana.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: ai-persona-system
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        url: http://prometheus:9090
        access: proxy
        isDefault: true
        editable: true

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: ai-persona-system
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        options:
          path: /etc/grafana/dashboards

---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: ai-persona-system
  labels:
    app: grafana
spec:
  ports:
    - port: 3000
      targetPort: 3000
      name: http
  selector:
    app: grafana
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: ai-persona-system
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - name: grafana
          image: grafana/grafana:latest
          ports:
            - containerPort: 3000
              name: http
          env:
            - name: GF_SECURITY_ADMIN_USER
              value: "admin"
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: grafana-secrets
                  key: admin-password
            - name: GF_INSTALL_PLUGINS
              value: "grafana-clock-panel,grafana-simple-json-datasource"
          volumeMounts:
            - name: grafana-datasources
              mountPath: /etc/grafana/provisioning/datasources
            - name: grafana-dashboards-config
              mountPath: /etc/grafana/provisioning/dashboards
            - name: grafana-storage
              mountPath: /var/lib/grafana
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"
      volumes:
        - name: grafana-datasources
          configMap:
            name: grafana-datasources
        - name: grafana-dashboards-config
          configMap:
            name: grafana-dashboards
        - name: grafana-storage
          emptyDir: {}-------------------------------------------------
filepath = ./k8s/monitoring/pod-monitor.yaml
# k8s/monitoring/pod-monitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: ai-persona-pods
  namespace: ai-persona-system
  labels:
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      prometheus.io/scrape: "true"
  namespaceSelector:
    matchNames:
      - ai-persona-system
  podMetricsEndpoints:
    - port: metrics
      interval: 30s
      path: /metrics-------------------------------------------------
filepath = ./k8s/database-init-job.yaml
# k8s/database-init-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: database-init
  namespace: ai-persona-system
spec:
  template:
    spec:
      initContainers:
        - name: wait-for-postgres
          image: postgres:16-alpine
          command: ['sh', '-c', 'until pg_isready -h postgres-clients -p 5432; do sleep 1; done']
        - name: wait-for-mysql
          image: mysql:8.0
          command: ['sh', '-c', 'until mysqladmin ping -h mysql-auth --silent; do sleep 1; done']
      containers:
        - name: migrate
          image: ai-persona-system/migrator:latest
          env:
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
          # ... other env vars
          command: ['/app/run-migrations.sh']
      restartPolicy: OnFailure-------------------------------------------------
filepath = ./k8s/backup-cronjob.yaml
# FILE: k8s/backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: database-backup
  namespace: ai-persona-system
  labels:
    app: database-backup
    component: backup
spec:
  # Run daily at 2 AM
  schedule: "0 2 * * *"
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        metadata:
          labels:
            app: database-backup
            component: backup
        spec:
          restartPolicy: OnFailure
          containers:
            - name: backup-postgresql
              image: postgres:16-alpine
              env:
                - name: CLIENTS_DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-secrets
                      key: clients-db-password
                - name: TEMPLATES_DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-secrets
                      key: templates-db-password
                - name: BACKUP_DATE
                  value: "$(date +%Y%m%d_%H%M%S)"
              command:
                - /bin/bash
                - -c
                - |
                  set -e
                  echo "Starting PostgreSQL backup..."
                  
                  # Create backup directory
                  mkdir -p /backup
                  
                  # Backup clients database
                  echo "Backing up clients database..."
                  export PGPASSWORD="$CLIENTS_DB_PASSWORD"
                  pg_dump -h postgres-clients -U clients_user -d clients_db \
                    --verbose --clean --if-exists --create \
                    > /backup/clients_db_$(date +%Y%m%d_%H%M%S).sql
                  
                  # Backup templates database
                  echo "Backing up templates database..."
                  export PGPASSWORD="$TEMPLATES_DB_PASSWORD"
                  pg_dump -h postgres-templates -U templates_user -d templates_db \
                    --verbose --clean --if-exists --create \
                    > /backup/templates_db_$(date +%Y%m%d_%H%M%S).sql
                  
                  echo "PostgreSQL backups completed!"
                  ls -la /backup/

              volumeMounts:
                - name: backup-storage
                  mountPath: /backup
              resources:
                requests:
                  memory: "256Mi"
                  cpu: "200m"
                limits:
                  memory: "512Mi"
                  cpu: "500m"

            - name: backup-mysql
              image: mysql:8.0
              env:
                - name: AUTH_DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-secrets
                      key: auth-db-password
              command:
                - /bin/bash
                - -c
                - |
                  set -e
                  echo "Starting MySQL backup..."
                  
                  # Create backup directory
                  mkdir -p /backup
                  
                  # Backup auth database
                  echo "Backing up auth database..."
                  mysqldump -h mysql-auth -u auth_user -p"$AUTH_DB_PASSWORD" \
                    --routines --triggers --single-transaction \
                    auth_db > /backup/auth_db_$(date +%Y%m%d_%H%M%S).sql
                  
                  echo "MySQL backup completed!"
                  ls -la /backup/

              volumeMounts:
                - name: backup-storage
                  mountPath: /backup
              resources:
                requests:
                  memory: "256Mi"
                  cpu: "200m"
                limits:
                  memory: "512Mi"
                  cpu: "500m"

            # Cleanup old backups (keep last 7 days)
            - name: cleanup-old-backups
              image: alpine:latest
              command:
                - /bin/sh
                - -c
                - |
                  echo "Cleaning up old backups (keeping last 7 days)..."
                  find /backup -name "*.sql" -type f -mtime +7 -delete
                  echo "Cleanup completed!"
                  echo "Current backups:"
                  ls -la /backup/

              volumeMounts:
                - name: backup-storage
                  mountPath: /backup
              resources:
                requests:
                  memory: "64Mi"
                  cpu: "100m"
                limits:
                  memory: "128Mi"
                  cpu: "200m"

          volumes:
            - name: backup-storage
              persistentVolumeClaim:
                claimName: backup-storage-pvc

---
# PVC for backup storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-storage-pvc
  namespace: ai-persona-system
  labels:
    app: database-backup
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  resources:
    requests:
      storage: 50Gi-------------------------------------------------
filepath = ./k8s/kafka-topics-job.yaml
# k8s/kafka-topics-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-topics-init
  namespace: ai-persona-system
spec:
  template:
    spec:
      containers:
        - name: topic-creator
          image: confluentinc/cp-kafka:7.5.0
          command: ['/bin/bash', '-c']
          args:
            - |
              kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --create --topic system.agent.reasoning.process --partitions 3 --replication-factor 1 --if-not-exists
              kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --create --topic system.responses.reasoning --partitions 6 --replication-factor 1 --if-not-exists
              kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --create --topic system.adapter.image.generate --partitions 3 --replication-factor 1 --if-not-exists
              # ... more topics-------------------------------------------------
filepath = ./k8s/image-adapter.yaml
# FILE: configs/image-adapter.yaml
service_info:
  name: "image-generator-adapter"
  version: "1.0.0"
  environment: "development"

server:
  port: "8084"

logging:
  level: "info"

infrastructure:
  kafka_brokers:
    - "kafka-0.kafka-headless:9092"
    - "kafka-1.kafka-headless:9092"
    - "kafka-2.kafka-headless:9092"

  clients_database: {}
  templates_database: {}
  auth_database: {}

  object_storage:
    provider: "s3"
    endpoint: "http://minio:9000"
    bucket: "agent-artifacts"
    access_key_env_var: "MINIO_ACCESS_KEY"
    secret_key_env_var: "MINIO_SECRET_KEY"

custom:
  image_provider: "stability_ai"
  default_model: "stable-diffusion-xl-1024-v1-0"
  timeout_seconds: 90
-------------------------------------------------
filepath = ./internal/adapters/imagegenerator/adapter.go
// FILE: internal/adapters/imagegenerator/adapter.go (updated with circuit breaker)
package imagegenerator

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"time"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/errors"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/gqls/agentchassis/platform/resilience"
	"github.com/gqls/agentchassis/platform/storage"
	"go.uber.org/zap"
)

const (
	requestTopic  = "system.adapter.image.generate"
	responseTopic = "system.responses.image"
	consumerGroup = "image-generator-adapter-group"
)

// RequestPayload defines the expected data for an image generation request
type RequestPayload struct {
	Action string `json:"action"`
	Data   struct {
		Prompt      string  `json:"prompt"`
		AspectRatio string  `json:"aspect_ratio,omitempty"`
		Style       string  `json:"style,omitempty"`
		Seed        float64 `json:"seed,omitempty"`
	} `json:"data"`
}

// ResponsePayload defines the data sent back after successful generation
type ResponsePayload struct {
	ImageURI string `json:"image_uri"`
	Prompt   string `json:"prompt"`
	Seed     int64  `json:"seed"`
}

// Adapter handles the translation between our internal system and an external API
type Adapter struct {
	ctx           context.Context
	logger        *zap.Logger
	consumer      *kafka.Consumer
	producer      kafka.Producer
	storageClient storage.Client
	httpClient    *resilience.HTTPClientWithBreaker
	externalAPI   string
	apiKey        string
}

// NewAdapter initializes all dependencies for the adapter
func NewAdapter(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger) (*Adapter, error) {
	// Initialize Kafka consumer
	consumer, err := kafka.NewConsumer(cfg.Infrastructure.KafkaBrokers, requestTopic, consumerGroup, logger)
	if err != nil {
		return nil, fmt.Errorf("failed to create kafka consumer: %w", err)
	}

	// Initialize Kafka producer
	producer, err := kafka.NewProducer(cfg.Infrastructure.KafkaBrokers, logger)
	if err != nil {
		consumer.Close()
		return nil, fmt.Errorf("failed to create kafka producer: %w", err)
	}

	// Initialize Object Storage client
	storageClient, err := storage.NewS3Client(ctx, cfg.Infrastructure.ObjectStorage)
	if err != nil {
		consumer.Close()
		producer.Close()
		return nil, fmt.Errorf("failed to create storage client: %w", err)
	}

	// Setup HTTP client with circuit breaker
	baseClient := &http.Client{Timeout: 90 * time.Second}
	cbConfig := resilience.DefaultCircuitBreakerConfig("stability-ai")
	cbConfig.ConsecutiveFailures = 3
	cbConfig.FailureRatio = 0.5
	httpClient := resilience.NewHTTPClientWithBreaker(baseClient, cbConfig, logger)

	externalAPIEndpoint := "https://api.stability.ai/v1/generation/stable-diffusion-v1-6/text-to-image"
	apiKey := os.Getenv("STABILITY_API_KEY")

	return &Adapter{
		ctx:           ctx,
		logger:        logger,
		consumer:      consumer,
		producer:      producer,
		storageClient: storageClient,
		httpClient:    httpClient,
		externalAPI:   externalAPIEndpoint,
		apiKey:        apiKey,
	}, nil
}

// Run starts the consumer loop
func (a *Adapter) Run() error {
	for {
		select {
		case <-a.ctx.Done():
			a.consumer.Close()
			a.producer.Close()
			return nil
		default:
			msg, err := a.consumer.FetchMessage(a.ctx)
			if err != nil {
				if err == context.Canceled {
					continue
				}
				a.logger.Error("Failed to fetch message", zap.Error(err))
				continue
			}
			go a.handleMessage(msg)
		}
	}
}

// handleMessage processes a single image generation request
func (a *Adapter) handleMessage(msg kafka.Message) {
	headers := kafka.HeadersToMap(msg.Headers)
	l := a.logger.With(
		zap.String("correlation_id", headers["correlation_id"]),
		zap.String("request_id", headers["request_id"]),
	)

	var req RequestPayload
	if err := json.Unmarshal(msg.Value, &req); err != nil {
		l.Error("Failed to unmarshal request payload", zap.Error(err))
		a.sendErrorResponse(headers, errors.ValidationError("payload", "invalid JSON"))
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}

	// Call the external API with circuit breaker protection
	imageData, err := a.callExternalImageAPI(req.Data.Prompt)
	if err != nil {
		l.Error("External image API call failed", zap.Error(err))

		// Check if it's a circuit breaker error
		if resilience.IsCircuitBreakerError(err) {
			retryAfter := 30 * time.Second
			a.sendErrorResponse(headers, errors.New(errors.ErrExternalService, "Image service temporarily unavailable").
				AsRetryable(&retryAfter).
				Build())
		} else {
			a.sendErrorResponse(headers, errors.New(errors.ErrAIServiceError, "Failed to generate image").
				WithCause(err).
				Build())
		}
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}

	// Upload the resulting image to Object Storage
	fileName := fmt.Sprintf("images/%s/%s.png", headers["client_id"], uuid.NewString())
	imageURI, err := a.storageClient.Upload(a.ctx, fileName, "image/png", bytes.NewReader(imageData))
	if err != nil {
		l.Error("Failed to upload image to object storage", zap.Error(err))
		a.sendErrorResponse(headers, errors.InternalError("Failed to store image", err))
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}
	l.Info("Image successfully uploaded to storage", zap.String("uri", imageURI))

	// Produce a standard response message with the URI
	responsePayload := ResponsePayload{
		ImageURI: imageURI,
		Prompt:   req.Data.Prompt,
	}
	a.sendSuccessResponse(headers, responsePayload)

	// Commit the original message
	a.consumer.CommitMessages(context.Background(), msg)
}

// callExternalImageAPI calls the Stability AI API with proper error handling
func (a *Adapter) callExternalImageAPI(prompt string) ([]byte, error) {
	a.logger.Info("Calling external image API", zap.String("prompt", prompt))

	requestBody := map[string]interface{}{
		"text_prompts": []map[string]interface{}{
			{"text": prompt, "weight": 1},
		},
		"cfg_scale":            7,
		"clip_guidance_preset": "FAST_BLUE",
		"height":               512,
		"width":                512,
		"samples":              1,
		"steps":                30,
	}

	jsonBody, err := json.Marshal(requestBody)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	req, err := http.NewRequestWithContext(a.ctx, "POST", a.externalAPI, bytes.NewBuffer(jsonBody))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", a.apiKey))
	req.Header.Set("Accept", "application/json")

	// Execute through circuit breaker
	resp, err := a.httpClient.Do(req)
	if err != nil {
		return nil, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %w", err)
	}

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("API returned status %d: %s", resp.StatusCode, string(body))
	}

	// Parse the response to extract the image
	var apiResponse struct {
		Artifacts []struct {
			Base64 string `json:"base64"`
		} `json:"artifacts"`
	}

	if err := json.Unmarshal(body, &apiResponse); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if len(apiResponse.Artifacts) == 0 {
		return nil, fmt.Errorf("no images in response")
	}

	// Decode base64 image
	imageData, err := base64.StdEncoding.DecodeString(apiResponse.Artifacts[0].Base64)
	if err != nil {
		return nil, fmt.Errorf("failed to decode image: %w", err)
	}

	return imageData, nil
}

// sendSuccessResponse sends a successful response
func (a *Adapter) sendSuccessResponse(headers map[string]string, payload ResponsePayload) {
	responseBytes, _ := json.Marshal(payload)
	responseHeaders := a.createResponseHeaders(headers)

	if err := a.producer.Produce(a.ctx, responseTopic, responseHeaders,
		[]byte(headers["correlation_id"]), responseBytes); err != nil {
		a.logger.Error("Failed to produce response message", zap.Error(err))
	}
}

// sendErrorResponse sends an error response
func (a *Adapter) sendErrorResponse(headers map[string]string, domainErr *errors.DomainError) {
	responseHeaders := a.createResponseHeaders(headers)
	domainErr.TraceID = headers["correlation_id"]

	errorBytes, _ := json.Marshal(domainErr)

	if err := a.producer.Produce(a.ctx, responseTopic, responseHeaders,
		[]byte(headers["correlation_id"]), errorBytes); err != nil {
		a.logger.Error("Failed to produce error response", zap.Error(err))
	}
}

// createResponseHeaders creates response headers with proper causality tracking
func (a *Adapter) createResponseHeaders(originalHeaders map[string]string) map[string]string {
	return map[string]string{
		"correlation_id": originalHeaders["correlation_id"],
		"causation_id":   originalHeaders["request_id"],
		"request_id":     uuid.NewString(),
		"timestamp":      time.Now().UTC().Format(time.RFC3339),
	}
}

// StartHealthServer starts a simple HTTP server for health checks
func (a *Adapter) StartHealthServer(port string) {
	http.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
		status := map[string]interface{}{
			"status":          "healthy",
			"adapter":         "image-generator",
			"circuit_breaker": a.httpClient.State(),
			"circuit_counts":  a.httpClient.Counts(),
		}

		w.Header().Set("Content-Type", "application/json")
		if a.httpClient.Breaker.IsOpen() {
			w.WriteHeader(http.StatusServiceUnavailable)
			status["status"] = "degraded"
		} else {
			w.WriteHeader(http.StatusOK)
		}
		json.NewEncoder(w).Encode(status)
	})

	go func() {
		a.logger.Info("Starting health server", zap.String("port", port))
		if err := http.ListenAndServe(":"+port, nil); err != nil {
			a.logger.Error("Health server failed", zap.Error(err))
		}
	}()
}
-------------------------------------------------
filepath = ./internal/adapters/websearch/adapter.go
// FILE: internal/adapters/websearch/adapter.go
package websearch

import (
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	"time"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/kafka"
	"go.uber.org/zap"
)

const (
	requestTopic  = "system.adapter.web.search"
	responseTopic = "system.responses.websearch"
	consumerGroup = "web-search-adapter-group"
)

// RequestPayload for web search
type RequestPayload struct {
	Action string `json:"action"`
	Data   struct {
		Query      string `json:"query"`
		NumResults int    `json:"num_results,omitempty"`
		SearchType string `json:"search_type,omitempty"` // web, news, images
	} `json:"data"`
}

// ResponsePayload with search results
type ResponsePayload struct {
	Query   string         `json:"query"`
	Results []SearchResult `json:"results"`
	Total   int            `json:"total"`
}

// SearchResult represents a single search result
type SearchResult struct {
	Title       string `json:"title"`
	URL         string `json:"url"`
	Snippet     string `json:"snippet"`
	PublishedAt string `json:"published_at,omitempty"`
}

// Adapter handles web search requests
type Adapter struct {
	ctx          context.Context
	logger       *zap.Logger
	consumer     *kafka.Consumer
	producer     kafka.Producer
	httpClient   *http.Client
	apiKey       string
	searchAPIURL string
}

// NewAdapter creates a new web search adapter
func NewAdapter(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger) (*Adapter, error) {
	consumer, err := kafka.NewConsumer(cfg.Infrastructure.KafkaBrokers, requestTopic, consumerGroup, logger)
	if err != nil {
		return nil, fmt.Errorf("failed to create consumer: %w", err)
	}

	producer, err := kafka.NewProducer(cfg.Infrastructure.KafkaBrokers, logger)
	if err != nil {
		consumer.Close()
		return nil, fmt.Errorf("failed to create producer: %w", err)
	}

	apiKey := os.Getenv("SERP_API_KEY")
	if apiKey == "" {
		consumer.Close()
		producer.Close()
		return nil, fmt.Errorf("SERP_API_KEY not set")
	}

	return &Adapter{
		ctx:          ctx,
		logger:       logger,
		consumer:     consumer,
		producer:     producer,
		httpClient:   &http.Client{Timeout: 30 * time.Second},
		apiKey:       apiKey,
		searchAPIURL: "https://serpapi.com/search",
	}, nil
}

// Run starts the adapter's main loop
func (a *Adapter) Run() error {
	a.logger.Info("Web search adapter running")

	for {
		select {
		case <-a.ctx.Done():
			a.consumer.Close()
			a.producer.Close()
			return nil
		default:
			msg, err := a.consumer.FetchMessage(a.ctx)
			if err != nil {
				if err == context.Canceled {
					continue
				}
				a.logger.Error("Failed to fetch message", zap.Error(err))
				continue
			}
			go a.handleMessage(msg)
		}
	}
}

// handleMessage processes a search request
func (a *Adapter) handleMessage(msg kafka.Message) {
	headers := kafka.HeadersToMap(msg.Headers)
	l := a.logger.With(zap.String("correlation_id", headers["correlation_id"]))

	var req RequestPayload
	if err := json.Unmarshal(msg.Value, &req); err != nil {
		l.Error("Failed to unmarshal request", zap.Error(err))
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}

	// Perform the search
	results, err := a.performSearch(req.Data.Query, req.Data.NumResults)
	if err != nil {
		l.Error("Search failed", zap.Error(err))
		a.sendErrorResponse(headers, "Search failed: "+err.Error())
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}

	// Send response
	response := ResponsePayload{
		Query:   req.Data.Query,
		Results: results,
		Total:   len(results),
	}

	a.sendResponse(headers, response)
	a.consumer.CommitMessages(context.Background(), msg)
}

// performSearch executes the actual web search
func (a *Adapter) performSearch(query string, numResults int) ([]SearchResult, error) {
	if numResults == 0 {
		numResults = 10
	}

	// Build search URL
	params := url.Values{}
	params.Add("q", query)
	params.Add("api_key", a.apiKey)
	params.Add("num", fmt.Sprintf("%d", numResults))
	params.Add("engine", "google")

	searchURL := fmt.Sprintf("%s?%s", a.searchAPIURL, params.Encode())

	// Execute request
	resp, err := a.httpClient.Get(searchURL)
	if err != nil {
		return nil, fmt.Errorf("failed to execute search request: %w", err)
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %w", err)
	}

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("search API returned status %d: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var apiResponse struct {
		OrganicResults []struct {
			Title   string `json:"title"`
			Link    string `json:"link"`
			Snippet string `json:"snippet"`
			Date    string `json:"date,omitempty"`
		} `json:"organic_results"`
	}

	if err := json.Unmarshal(body, &apiResponse); err != nil {
		return nil, fmt.Errorf("failed to parse search response: %w", err)
	}

	// Convert to our format
	results := make([]SearchResult, 0, len(apiResponse.OrganicResults))
	for _, r := range apiResponse.OrganicResults {
		results = append(results, SearchResult{
			Title:       r.Title,
			URL:         r.Link,
			Snippet:     r.Snippet,
			PublishedAt: r.Date,
		})
	}

	return results, nil
}

// sendResponse sends a successful response
func (a *Adapter) sendResponse(headers map[string]string, payload ResponsePayload) {
	responseBytes, _ := json.Marshal(payload)
	responseHeaders := map[string]string{
		"correlation_id": headers["correlation_id"],
		"causation_id":   headers["request_id"],
		"request_id":     uuid.NewString(),
	}

	if err := a.producer.Produce(a.ctx, responseTopic, responseHeaders,
		[]byte(headers["correlation_id"]), responseBytes); err != nil {
		a.logger.Error("Failed to produce response", zap.Error(err))
	}
}

// sendErrorResponse sends an error response
func (a *Adapter) sendErrorResponse(headers map[string]string, errorMsg string) {
	payload := map[string]interface{}{
		"success": false,
		"error":   errorMsg,
	}
	responseBytes, _ := json.Marshal(payload)
	responseHeaders := map[string]string{
		"correlation_id": headers["correlation_id"],
		"causation_id":   headers["request_id"],
		"request_id":     uuid.NewString(),
	}

	a.producer.Produce(a.ctx, responseTopic, responseHeaders,
		[]byte(headers["correlation_id"]), responseBytes)
}
-------------------------------------------------
filepath = ./internal/auth-service/gateway/handlers.go
// FILE: internal/auth-service/gateway/handlers.go
package gateway

import (
	"io"
	"net/http"
	"net/url"
	"strings"

	"github.com/gin-gonic/gin"
	"go.uber.org/zap"
)

// HTTPHandler handles gateway HTTP requests
type HTTPHandler struct {
	service *Service
	logger  *zap.Logger
}

// NewHTTPHandler creates a new gateway HTTP handler
func NewHTTPHandler(service *Service, logger *zap.Logger) *HTTPHandler {
	return &HTTPHandler{
		service: service,
		logger:  logger,
	}
}

// ProxyToCoreManager proxies requests to the core manager service
func (h *HTTPHandler) ProxyToCoreManager(c *gin.Context) {
	// Build target URL
	targetPath := c.Param("path")
	if targetPath == "" {
		targetPath = strings.TrimPrefix(c.Request.URL.Path, "/api/v1")
	}

	targetURL := h.service.coreManagerURL.ResolveReference(&url.URL{
		Path:     "/api/v1" + targetPath,
		RawQuery: c.Request.URL.RawQuery,
	})

	h.logger.Debug("Proxying request",
		zap.String("method", c.Request.Method),
		zap.String("target", targetURL.String()))

	// Create new request
	req, err := http.NewRequest(c.Request.Method, targetURL.String(), c.Request.Body)
	if err != nil {
		h.logger.Error("Failed to create proxy request", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to create request"})
		return
	}

	// Copy headers
	for key, values := range c.Request.Header {
		for _, value := range values {
			req.Header.Add(key, value)
		}
	}

	// Add user context headers
	req.Header.Set("X-User-ID", c.GetString("user_id"))
	req.Header.Set("X-Client-ID", c.GetString("client_id"))
	req.Header.Set("X-User-Role", c.GetString("user_role"))
	req.Header.Set("X-User-Tier", c.GetString("user_tier"))
	req.Header.Set("X-User-Email", c.GetString("user_email"))

	// Add permissions
	if perms, exists := c.Get("user_permissions"); exists {
		if permissions, ok := perms.([]string); ok {
			req.Header.Set("X-User-Permissions", strings.Join(permissions, ","))
		}
	}

	// Execute request
	resp, err := h.service.httpClient.Do(req)
	if err != nil {
		h.logger.Error("Proxy request failed", zap.Error(err))
		c.JSON(http.StatusBadGateway, gin.H{"error": "Service unavailable"})
		return
	}
	defer resp.Body.Close()

	// Copy response headers
	for key, values := range resp.Header {
		for _, value := range values {
			c.Header(key, value)
		}
	}

	// Set status code
	c.Status(resp.StatusCode)

	// Copy response body
	_, err = io.Copy(c.Writer, resp.Body)
	if err != nil {
		h.logger.Error("Failed to copy response body", zap.Error(err))
	}
}

// HandleTemplateRoutes handles all template-related routes
func (h *HTTPHandler) HandleTemplateRoutes(c *gin.Context) {
	h.ProxyToCoreManager(c)
}

// HandleInstanceRoutes handles all instance-related routes
func (h *HTTPHandler) HandleInstanceRoutes(c *gin.Context) {
	h.ProxyToCoreManager(c)
}

// HandleAdminRoutes provides a generic proxy for all admin routes destined for core-manager
func (h *HTTPHandler) HandleAdminRoutes(c *gin.Context) {
	h.ProxyToCoreManager(c)
}

// HandleWebSocket handles WebSocket connections
func (h *HTTPHandler) HandleWebSocket(c *gin.Context) {
	wsProxy := NewWebSocketProxy(h.service.coreManagerURL, h.logger)
	wsProxy.ProxyWebSocket(c)
}
-------------------------------------------------
filepath = ./internal/auth-service/gateway/proxy.go
package gateway

import (
	"net/http"
	"net/http/httputil"
	"net/url"

	"github.com/gin-gonic/gin"
	"github.com/gorilla/websocket"
	"go.uber.org/zap"
)

// WebSocketProxy handles WebSocket connections
type WebSocketProxy struct {
	targetURL *url.URL
	upgrader  websocket.Upgrader
	logger    *zap.Logger
}

// NewWebSocketProxy creates a new WebSocket proxy
func NewWebSocketProxy(targetURL *url.URL, logger *zap.Logger) *WebSocketProxy {
	return &WebSocketProxy{
		targetURL: targetURL,
		upgrader: websocket.Upgrader{
			CheckOrigin: func(r *http.Request) bool {
				// Configure based on your security requirements
				return true
			},
			ReadBufferSize:  1024,
			WriteBufferSize: 1024,
		},
		logger: logger,
	}
}

// ProxyWebSocket proxies WebSocket connections
func (p *WebSocketProxy) ProxyWebSocket(c *gin.Context) {
	// Build target WebSocket URL
	targetURL := *p.targetURL
	targetURL.Scheme = "ws"
	if p.targetURL.Scheme == "https" {
		targetURL.Scheme = "wss"
	}
	targetURL.Path = c.Request.URL.Path
	targetURL.RawQuery = c.Request.URL.RawQuery

	p.logger.Debug("Proxying WebSocket connection",
		zap.String("target", targetURL.String()))

	// Connect to target
	targetHeader := http.Header{}
	for key, values := range c.Request.Header {
		if key == "Upgrade" || key == "Connection" || key == "Sec-Websocket-Key" ||
			key == "Sec-Websocket-Version" || key == "Sec-Websocket-Extensions" {
			continue
		}
		targetHeader[key] = values
	}

	// Add user context headers
	targetHeader.Set("X-User-ID", c.GetString("user_id"))
	targetHeader.Set("X-Client-ID", c.GetString("client_id"))

	targetConn, resp, err := websocket.DefaultDialer.Dial(targetURL.String(), targetHeader)
	if err != nil {
		p.logger.Error("Failed to connect to target WebSocket", zap.Error(err))
		if resp != nil {
			c.Status(resp.StatusCode)
		} else {
			c.Status(http.StatusBadGateway)
		}
		return
	}
	defer targetConn.Close()

	// Upgrade client connection
	clientConn, err := p.upgrader.Upgrade(c.Writer, c.Request, nil)
	if err != nil {
		p.logger.Error("Failed to upgrade client connection", zap.Error(err))
		return
	}
	defer clientConn.Close()

	// Proxy messages
	errChan := make(chan error, 2)

	// Client to target
	go func() {
		for {
			messageType, data, err := clientConn.ReadMessage()
			if err != nil {
				errChan <- err
				return
			}

			if err := targetConn.WriteMessage(messageType, data); err != nil {
				errChan <- err
				return
			}
		}
	}()

	// Target to client
	go func() {
		for {
			messageType, data, err := targetConn.ReadMessage()
			if err != nil {
				errChan <- err
				return
			}

			if err := clientConn.WriteMessage(messageType, data); err != nil {
				errChan <- err
				return
			}
		}
	}()

	// Wait for error
	err = <-errChan
	p.logger.Debug("WebSocket proxy closed", zap.Error(err))
}

// ReverseProxy creates a standard HTTP reverse proxy
func (s *Service) ReverseProxy() *httputil.ReverseProxy {
	proxy := httputil.NewSingleHostReverseProxy(s.coreManagerURL)

	// Customize the director
	originalDirector := proxy.Director
	proxy.Director = func(req *http.Request) {
		originalDirector(req)

		// Add user context from gin context if available
		if ginCtx, ok := req.Context().Value("gin_context").(*gin.Context); ok {
			req.Header.Set("X-User-ID", ginCtx.GetString("user_id"))
			req.Header.Set("X-Client-ID", ginCtx.GetString("client_id"))
			req.Header.Set("X-User-Role", ginCtx.GetString("user_role"))
			req.Header.Set("X-User-Tier", ginCtx.GetString("user_tier"))
		}
	}

	// Custom error handler
	proxy.ErrorHandler = func(w http.ResponseWriter, r *http.Request, err error) {
		s.logger.Error("Reverse proxy error", zap.Error(err))
		w.WriteHeader(http.StatusBadGateway)
		w.Write([]byte(`{"error": "Service temporarily unavailable"}`))
	}

	return proxy
}
-------------------------------------------------
filepath = ./internal/auth-service/gateway/service.go
package gateway

import (
	"net/http"
	"net/url"
	"time"

	"github.com/gqls/agentchassis/platform/config"
	"go.uber.org/zap"
)

// Service handles API gateway functionality
type Service struct {
	coreManagerURL *url.URL
	httpClient     *http.Client
	logger         *zap.Logger
}

// NewService creates a new gateway service
func NewService(cfg *config.ServiceConfig, logger *zap.Logger) *Service {
	coreManagerURL, _ := url.Parse(cfg.Custom["core_manager_url"].(string))

	return &Service{
		coreManagerURL: coreManagerURL,
		httpClient: &http.Client{
			Timeout: 30 * time.Second,
		},
		logger: logger,
	}
}

// GetCoreManagerURL returns the core manager URL
func (s *Service) GetCoreManagerURL() *url.URL {
	return s.coreManagerURL
}
-------------------------------------------------
filepath = ./internal/auth-service/user/handlers.go
package user

import (
	"net/http"

	"github.com/gin-gonic/gin"
)

// Handlers wraps the user service for HTTP handling
type Handlers struct {
	service *Service
}

// NewHandlers creates new user handlers
func NewHandlers(service *Service) *Handlers {
	return &Handlers{service: service}
}

// HandleGetCurrentUser returns the current user's details
func (h *Handlers) HandleGetCurrentUser(c *gin.Context) {
	userID := c.GetString("user_id")

	user, err := h.service.GetUser(c.Request.Context(), userID)
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": "User not found"})
		return
	}

	c.JSON(http.StatusOK, user)
}

// HandleUpdateCurrentUser updates the current user's details
func (h *Handlers) HandleUpdateCurrentUser(c *gin.Context) {
	userID := c.GetString("user_id")

	var req UpdateUserRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	if err := h.service.UpdateUser(c.Request.Context(), userID, &req); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to update user"})
		return
	}

	// Return updated user
	user, _ := h.service.GetUser(c.Request.Context(), userID)
	c.JSON(http.StatusOK, user)
}

// HandleChangePassword changes the user's password
func (h *Handlers) HandleChangePassword(c *gin.Context) {
	userID := c.GetString("user_id")

	var req ChangePasswordRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	if err := h.service.ChangePassword(c.Request.Context(), userID, &req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	c.JSON(http.StatusOK, gin.H{"message": "Password changed successfully"})
}

// HandleDeleteAccount deletes the user's account
func (h *Handlers) HandleDeleteAccount(c *gin.Context) {
	userID := c.GetString("user_id")

	if err := h.service.DeleteUser(c.Request.Context(), userID); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to delete account"})
		return
	}

	c.JSON(http.StatusOK, gin.H{"message": "Account deleted successfully"})
}
-------------------------------------------------
filepath = ./internal/auth-service/user/repository_admin.go
// FILE: internal/auth-service/user/repository_admin.go
package user

import (
	"context"
	"database/sql"
	"fmt"
	"strings"
	"time"

	"go.uber.org/zap"
)

// ListUsersParams contains parameters for listing users
type ListUsersParams struct {
	Offset    int
	Limit     int
	Email     string
	ClientID  string
	Role      string
	Tier      string
	IsActive  *bool
	SortBy    string
	SortOrder string
}

// UserStats contains statistics about a user
type UserStats struct {
	TotalProjects int        `json:"total_projects"`
	TotalPersonas int        `json:"total_personas"`
	LastLoginAt   *time.Time `json:"last_login_at"`
	AccountAge    string     `json:"account_age"`
	TotalLogins   int        `json:"total_logins"`
}

// UserActivity represents a user activity log entry
type UserActivity struct {
	ID        string    `json:"id"`
	UserID    string    `json:"user_id"`
	Action    string    `json:"action"`
	Details   string    `json:"details"`
	IPAddress string    `json:"ip_address"`
	UserAgent string    `json:"user_agent"`
	CreatedAt time.Time `json:"created_at"`
}

// AdminUpdateRequest contains fields that can be updated by admin
type AdminUpdateRequest struct {
	Role             *string
	SubscriptionTier *string
	IsActive         *bool
	EmailVerified    *bool
}

// ListUsers returns a paginated list of users with optional filtering
func (r *Repository) ListUsers(ctx context.Context, params ListUsersParams) ([]User, int, error) {
	// Build the query dynamically
	query := `
		SELECT u.id, u.email, u.password_hash, u.role, u.client_id, 
		       u.subscription_tier, u.is_active, u.email_verified,
		       u.created_at, u.updated_at, u.last_login_at
		FROM users u
		WHERE 1=1
	`
	countQuery := `SELECT COUNT(*) FROM users u WHERE 1=1`

	args := []interface{}{}
	argCount := 0

	// Add filters
	var conditions []string

	if params.Email != "" {
		argCount++
		conditions = append(conditions, fmt.Sprintf("u.email ILIKE $%d", argCount))
		args = append(args, "%"+params.Email+"%")
	}

	if params.ClientID != "" {
		argCount++
		conditions = append(conditions, fmt.Sprintf("u.client_id = $%d", argCount))
		args = append(args, params.ClientID)
	}

	if params.Role != "" {
		argCount++
		conditions = append(conditions, fmt.Sprintf("u.role = $%d", argCount))
		args = append(args, params.Role)
	}

	if params.Tier != "" {
		argCount++
		conditions = append(conditions, fmt.Sprintf("u.subscription_tier = $%d", argCount))
		args = append(args, params.Tier)
	}

	if params.IsActive != nil {
		argCount++
		conditions = append(conditions, fmt.Sprintf("u.is_active = $%d", argCount))
		args = append(args, *params.IsActive)
	}

	// Add conditions to queries
	if len(conditions) > 0 {
		whereClause := " AND " + strings.Join(conditions, " AND ")
		query += whereClause
		countQuery += whereClause
	}

	// Get total count
	var totalCount int
	err := r.db.QueryRowContext(ctx, countQuery, args...).Scan(&totalCount)
	if err != nil {
		return nil, 0, fmt.Errorf("failed to get user count: %w", err)
	}

	// Add sorting
	validSortColumns := map[string]bool{
		"email": true, "created_at": true, "updated_at": true,
		"last_login_at": true, "role": true, "subscription_tier": true,
	}

	sortBy := "created_at"
	if validSortColumns[params.SortBy] {
		sortBy = params.SortBy
	}

	sortOrder := "DESC"
	if strings.ToUpper(params.SortOrder) == "ASC" {
		sortOrder = "ASC"
	}

	query += fmt.Sprintf(" ORDER BY u.%s %s", sortBy, sortOrder)

	// Add pagination
	argCount++
	query += fmt.Sprintf(" LIMIT $%d", argCount)
	args = append(args, params.Limit)

	argCount++
	query += fmt.Sprintf(" OFFSET $%d", argCount)
	args = append(args, params.Offset)

	// Execute query
	rows, err := r.db.QueryContext(ctx, query, args...)
	if err != nil {
		return nil, 0, fmt.Errorf("failed to list users: %w", err)
	}
	defer rows.Close()

	var users []User
	for rows.Next() {
		var user User
		err := rows.Scan(
			&user.ID, &user.Email, &user.PasswordHash, &user.Role,
			&user.ClientID, &user.SubscriptionTier, &user.IsActive,
			&user.EmailVerified, &user.CreatedAt, &user.UpdatedAt,
			&user.LastLoginAt,
		)
		if err != nil {
			r.logger.Error("Failed to scan user row", zap.Error(err))
			continue
		}

		// Don't load profile and permissions for list view (performance)
		users = append(users, user)
	}

	return users, totalCount, nil
}

// GetUserStats retrieves statistics for a user
func (r *Repository) GetUserStats(ctx context.Context, userID string) (*UserStats, error) {
	stats := &UserStats{}

	// Get basic user info for last login and account age
	var createdAt time.Time
	var lastLogin sql.NullTime
	err := r.db.QueryRowContext(ctx,
		"SELECT created_at, last_login_at FROM users WHERE id = $1",
		userID,
	).Scan(&createdAt, &lastLogin)

	if err != nil {
		return nil, err
	}

	if lastLogin.Valid {
		stats.LastLoginAt = &lastLogin.Time
	}

	// Calculate account age
	age := time.Since(createdAt)
	if age.Hours() < 24 {
		stats.AccountAge = fmt.Sprintf("%d hours", int(age.Hours()))
	} else if age.Hours() < 24*30 {
		stats.AccountAge = fmt.Sprintf("%d days", int(age.Hours()/24))
	} else {
		stats.AccountAge = fmt.Sprintf("%d months", int(age.Hours()/(24*30)))
	}

	// Get project count
	err = r.db.QueryRowContext(ctx,
		"SELECT COUNT(*) FROM projects WHERE owner_id = $1 AND is_active = true",
		userID,
	).Scan(&stats.TotalProjects)

	if err != nil && err != sql.ErrNoRows {
		r.logger.Warn("Failed to get project count", zap.Error(err))
	}

	// Note: Persona count would require access to clients DB
	// For now, we'll leave it at 0

	return stats, nil
}

// AdminUpdateUser updates user fields that only admins can change
func (r *Repository) AdminUpdateUser(ctx context.Context, userID string, req *AdminUpdateRequest) error {
	var setClauses []string
	var args []interface{}
	argCount := 0

	if req.Role != nil {
		argCount++
		setClauses = append(setClauses, fmt.Sprintf("role = $%d", argCount))
		args = append(args, *req.Role)
	}

	if req.SubscriptionTier != nil {
		argCount++
		setClauses = append(setClauses, fmt.Sprintf("subscription_tier = $%d", argCount))
		args = append(args, *req.SubscriptionTier)
	}

	if req.IsActive != nil {
		argCount++
		setClauses = append(setClauses, fmt.Sprintf("is_active = $%d", argCount))
		args = append(args, *req.IsActive)
	}

	if req.EmailVerified != nil {
		argCount++
		setClauses = append(setClauses, fmt.Sprintf("email_verified = $%d", argCount))
		args = append(args, *req.EmailVerified)
	}

	if len(setClauses) == 0 {
		return nil // Nothing to update
	}

	argCount++
	setClauses = append(setClauses, fmt.Sprintf("updated_at = $%d", argCount))
	args = append(args, time.Now())

	argCount++
	args = append(args, userID)

	query := fmt.Sprintf(
		"UPDATE users SET %s WHERE id = $%d",
		strings.Join(setClauses, ", "),
		argCount,
	)

	_, err := r.db.ExecContext(ctx, query, args...)
	if err != nil {
		return fmt.Errorf("failed to update user: %w", err)
	}

	return nil
}

// GetUserActivityLog retrieves activity logs for a user
func (r *Repository) GetUserActivityLog(ctx context.Context, userID string, limit, offset int) ([]UserActivity, error) {
	// First, ensure activity log table exists
	createTableQuery := `
		CREATE TABLE IF NOT EXISTS user_activity_logs (
			id VARCHAR(36) PRIMARY KEY,
			user_id VARCHAR(36) NOT NULL,
			action VARCHAR(100) NOT NULL,
			details TEXT,
			ip_address VARCHAR(45),
			user_agent TEXT,
			created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
			INDEX idx_activity_user_created (user_id, created_at DESC),
			FOREIGN KEY (user_id) REFERENCES users(id)
		)
	`

	_, err := r.db.ExecContext(ctx, createTableQuery)
	if err != nil {
		r.logger.Warn("Failed to ensure activity log table", zap.Error(err))
	}

	// Get activity logs
	query := `
		SELECT id, user_id, action, COALESCE(details, ''), 
		       COALESCE(ip_address, ''), COALESCE(user_agent, ''), created_at
		FROM user_activity_logs
		WHERE user_id = ?
		ORDER BY created_at DESC
		LIMIT ? OFFSET ?
	`

	rows, err := r.db.QueryContext(ctx, query, userID, limit, offset)
	if err != nil {
		return nil, fmt.Errorf("failed to get activity logs: %w", err)
	}
	defer rows.Close()

	var activities []UserActivity
	for rows.Next() {
		var activity UserActivity
		err := rows.Scan(
			&activity.ID, &activity.UserID, &activity.Action,
			&activity.Details, &activity.IPAddress, &activity.UserAgent,
			&activity.CreatedAt,
		)
		if err != nil {
			r.logger.Error("Failed to scan activity row", zap.Error(err))
			continue
		}
		activities = append(activities, activity)
	}

	return activities, nil
}

// GrantPermission grants a permission to a user
func (r *Repository) GrantPermission(ctx context.Context, userID, permissionName string) error {
	// First get the permission ID
	var permissionID string
	err := r.db.QueryRowContext(ctx,
		"SELECT id FROM permissions WHERE name = ?",
		permissionName,
	).Scan(&permissionID)

	if err != nil {
		return fmt.Errorf("permission not found: %w", err)
	}

	// Grant the permission
	query := `
		INSERT INTO user_permissions (user_id, permission_id, granted_at)
		VALUES (?, ?, ?)
		ON DUPLICATE KEY UPDATE granted_at = VALUES(granted_at)
	`

	_, err = r.db.ExecContext(ctx, query, userID, permissionID, time.Now())
	if err != nil {
		return fmt.Errorf("failed to grant permission: %w", err)
	}

	return nil
}

// RevokePermission revokes a permission from a user
func (r *Repository) RevokePermission(ctx context.Context, userID, permissionName string) error {
	query := `
		DELETE up FROM user_permissions up
		JOIN permissions p ON up.permission_id = p.id
		WHERE up.user_id = ? AND p.name = ?
	`

	_, err := r.db.ExecContext(ctx, query, userID, permissionName)
	if err != nil {
		return fmt.Errorf("failed to revoke permission: %w", err)
	}

	return nil
}

// LogUserActivity logs a user action
func (r *Repository) LogUserActivity(ctx context.Context, activity *UserActivity) error {
	query := `
		INSERT INTO user_activity_logs 
		(id, user_id, action, details, ip_address, user_agent, created_at)
		VALUES (?, ?, ?, ?, ?, ?, ?)
	`

	_, err := r.db.ExecContext(ctx, query,
		activity.ID, activity.UserID, activity.Action,
		activity.Details, activity.IPAddress, activity.UserAgent,
		activity.CreatedAt,
	)

	return err
}
-------------------------------------------------
filepath = ./internal/auth-service/user/repository.go
package user

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"strings"
	"time"

	"github.com/google/uuid"
	"go.uber.org/zap"
	"golang.org/x/crypto/bcrypt"
)

// Repository handles user data access
type Repository struct {
	db     *sql.DB
	logger *zap.Logger
}

// NewRepository creates a new user repository
func NewRepository(db *sql.DB, logger *zap.Logger) *Repository {
	return &Repository{
		db:     db,
		logger: logger,
	}
}

// CreateUser creates a new user with profile
func (r *Repository) CreateUser(ctx context.Context, req *CreateUserRequest) (*User, error) {
	// Start transaction
	tx, err := r.db.BeginTx(ctx, nil)
	if err != nil {
		return nil, fmt.Errorf("failed to begin transaction: %w", err)
	}
	defer tx.Rollback()

	// Hash password
	hashedPassword, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost)
	if err != nil {
		return nil, fmt.Errorf("failed to hash password: %w", err)
	}

	user := &User{
		ID:               uuid.New().String(),
		Email:            strings.ToLower(req.Email),
		PasswordHash:     string(hashedPassword),
		Role:             "user", // Default role
		ClientID:         req.ClientID,
		SubscriptionTier: "free", // Default tier
		IsActive:         true,
		EmailVerified:    false,
		CreatedAt:        time.Now(),
		UpdatedAt:        time.Now(),
	}

	// Insert user
	query := `
        INSERT INTO users (id, email, password_hash, role, client_id, subscription_tier, 
                          is_active, email_verified, created_at, updated_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `

	_, err = tx.ExecContext(ctx, query,
		user.ID, user.Email, user.PasswordHash, user.Role, user.ClientID,
		user.SubscriptionTier, user.IsActive, user.EmailVerified,
		user.CreatedAt, user.UpdatedAt)

	if err != nil {
		if strings.Contains(err.Error(), "duplicate") {
			return nil, fmt.Errorf("user with email %s already exists", req.Email)
		}
		return nil, fmt.Errorf("failed to create user: %w", err)
	}

	// Create user profile
	profile := &UserProfile{
		UserID:    user.ID,
		FirstName: req.FirstName,
		LastName:  req.LastName,
		Company:   req.Company,
	}

	profileQuery := `
        INSERT INTO user_profiles (user_id, first_name, last_name, company)
        VALUES (?, ?, ?, ?)
    `

	_, err = tx.ExecContext(ctx, profileQuery,
		profile.UserID, profile.FirstName, profile.LastName, profile.Company)

	if err != nil {
		return nil, fmt.Errorf("failed to create user profile: %w", err)
	}

	// Commit transaction
	if err := tx.Commit(); err != nil {
		return nil, fmt.Errorf("failed to commit transaction: %w", err)
	}

	user.Profile = profile
	r.logger.Info("User created successfully", zap.String("user_id", user.ID))

	return user, nil
}

// GetUserByEmail retrieves a user by email
func (r *Repository) GetUserByEmail(ctx context.Context, email string) (*User, error) {
	email = strings.ToLower(email)

	var user User
	query := `
        SELECT id, email, password_hash, role, client_id, subscription_tier, 
               is_active, email_verified, created_at, updated_at, last_login_at
        FROM users
        WHERE email = ? AND is_active = true
    `

	err := r.db.QueryRowContext(ctx, query, email).Scan(
		&user.ID, &user.Email, &user.PasswordHash, &user.Role, &user.ClientID,
		&user.SubscriptionTier, &user.IsActive, &user.EmailVerified,
		&user.CreatedAt, &user.UpdatedAt, &user.LastLoginAt)

	if err != nil {
		if err == sql.ErrNoRows {
			return nil, fmt.Errorf("user not found")
		}
		return nil, fmt.Errorf("failed to get user: %w", err)
	}

	// Load profile
	profile, err := r.getUserProfile(ctx, user.ID)
	if err == nil {
		user.Profile = profile
	}

	// Load permissions
	permissions, err := r.getUserPermissions(ctx, user.ID)
	if err == nil {
		user.Permissions = permissions
	}

	return &user, nil
}

// GetUserByID retrieves a user by ID
func (r *Repository) GetUserByID(ctx context.Context, userID string) (*User, error) {
	var user User
	query := `
        SELECT id, email, password_hash, role, client_id, subscription_tier, 
               is_active, email_verified, created_at, updated_at, last_login_at
        FROM users
        WHERE id = ? AND is_active = true
    `

	err := r.db.QueryRowContext(ctx, query, userID).Scan(
		&user.ID, &user.Email, &user.PasswordHash, &user.Role, &user.ClientID,
		&user.SubscriptionTier, &user.IsActive, &user.EmailVerified,
		&user.CreatedAt, &user.UpdatedAt, &user.LastLoginAt)

	if err != nil {
		if err == sql.ErrNoRows {
			return nil, fmt.Errorf("user not found")
		}
		return nil, fmt.Errorf("failed to get user: %w", err)
	}

	// Load profile
	profile, err := r.getUserProfile(ctx, user.ID)
	if err == nil {
		user.Profile = profile
	}

	// Load permissions
	permissions, err := r.getUserPermissions(ctx, user.ID)
	if err == nil {
		user.Permissions = permissions
	}

	return &user, nil
}

// getUserProfile loads user profile
func (r *Repository) getUserProfile(ctx context.Context, userID string) (*UserProfile, error) {
	var profile UserProfile
	query := `
        SELECT user_id, first_name, last_name, company, phone, avatar_url, preferences
        FROM user_profiles
        WHERE user_id = ?
    `

	var preferences sql.NullString
	err := r.db.QueryRowContext(ctx, query, userID).Scan(
		&profile.UserID, &profile.FirstName, &profile.LastName,
		&profile.Company, &profile.Phone, &profile.AvatarURL, &preferences)

	if err != nil {
		return nil, err
	}

	if preferences.Valid {
		json.Unmarshal([]byte(preferences.String), &profile.Preferences)
	}

	return &profile, nil
}

// getUserPermissions loads user permissions
func (r *Repository) getUserPermissions(ctx context.Context, userID string) ([]string, error) {
	query := `
        SELECT p.name
        FROM permissions p
        JOIN user_permissions up ON p.id = up.permission_id
        WHERE up.user_id = ?
    `

	rows, err := r.db.QueryContext(ctx, query, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var permissions []string
	for rows.Next() {
		var perm string
		if err := rows.Scan(&perm); err != nil {
			continue
		}
		permissions = append(permissions, perm)
	}

	return permissions, nil
}

// UpdateUser updates user information
func (r *Repository) UpdateUser(ctx context.Context, userID string, req *UpdateUserRequest) error {
	tx, err := r.db.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	defer tx.Rollback()

	// Update user table
	userQuery := `UPDATE users SET updated_at = ? WHERE id = ?`
	_, err = tx.ExecContext(ctx, userQuery, time.Now(), userID)
	if err != nil {
		return fmt.Errorf("failed to update user: %w", err)
	}

	// Update profile
	var setClauses []string
	var args []interface{}

	if req.FirstName != nil {
		setClauses = append(setClauses, "first_name = ?")
		args = append(args, *req.FirstName)
	}
	if req.LastName != nil {
		setClauses = append(setClauses, "last_name = ?")
		args = append(args, *req.LastName)
	}
	if req.Company != nil {
		setClauses = append(setClauses, "company = ?")
		args = append(args, *req.Company)
	}
	if req.Phone != nil {
		setClauses = append(setClauses, "phone = ?")
		args = append(args, *req.Phone)
	}
	if req.Preferences != nil {
		prefsJSON, _ := json.Marshal(req.Preferences)
		setClauses = append(setClauses, "preferences = ?")
		args = append(args, string(prefsJSON))
	}

	if len(setClauses) > 0 {
		args = append(args, userID)
		profileQuery := fmt.Sprintf(
			"UPDATE user_profiles SET %s WHERE user_id = ?",
			strings.Join(setClauses, ", "),
		)

		_, err = tx.ExecContext(ctx, profileQuery, args...)
		if err != nil {
			return fmt.Errorf("failed to update profile: %w", err)
		}
	}

	if err := tx.Commit(); err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}

	return nil
}

// ValidatePassword checks if the provided password matches
func (r *Repository) ValidatePassword(user *User, password string) bool {
	err := bcrypt.CompareHashAndPassword([]byte(user.PasswordHash), []byte(password))
	return err == nil
}

// UpdatePassword updates user password
func (r *Repository) UpdatePassword(ctx context.Context, userID, newPassword string) error {
	hashedPassword, err := bcrypt.GenerateFromPassword([]byte(newPassword), bcrypt.DefaultCost)
	if err != nil {
		return fmt.Errorf("failed to hash password: %w", err)
	}

	query := `UPDATE users SET password_hash = ?, updated_at = ? WHERE id = ?`
	_, err = r.db.ExecContext(ctx, query, string(hashedPassword), time.Now(), userID)
	if err != nil {
		return fmt.Errorf("failed to update password: %w", err)
	}

	return nil
}

// UpdateLastLogin updates the last login timestamp
func (r *Repository) UpdateLastLogin(ctx context.Context, userID string) error {
	query := `UPDATE users SET last_login_at = ? WHERE id = ?`
	_, err := r.db.ExecContext(ctx, query, time.Now(), userID)
	return err
}

// UpdateUserTier updates subscription tier
func (r *Repository) UpdateUserTier(ctx context.Context, userID, tier string) error {
	query := `UPDATE users SET subscription_tier = ?, updated_at = ? WHERE id = ?`
	_, err := r.db.ExecContext(ctx, query, tier, time.Now(), userID)
	if err != nil {
		return fmt.Errorf("failed to update user tier: %w", err)
	}
	return nil
}

// DeleteUser soft deletes a user
func (r *Repository) DeleteUser(ctx context.Context, userID string) error {
	query := `UPDATE users SET is_active = false, updated_at = ? WHERE id = ?`
	_, err := r.db.ExecContext(ctx, query, time.Now(), userID)
	if err != nil {
		return fmt.Errorf("failed to delete user: %w", err)
	}
	return nil
}
-------------------------------------------------
filepath = ./internal/auth-service/user/models.go
package user

import (
	"database/sql/driver"
	"encoding/json"
	"fmt"
	"time"
)

// User represents a user in the system
type User struct {
	ID               string     `json:"id" db:"id"`
	Email            string     `json:"email" db:"email"`
	PasswordHash     string     `json:"-" db:"password_hash"`
	Role             string     `json:"role" db:"role"`
	ClientID         string     `json:"client_id" db:"client_id"`
	SubscriptionTier string     `json:"subscription_tier" db:"subscription_tier"`
	IsActive         bool       `json:"is_active" db:"is_active"`
	EmailVerified    bool       `json:"email_verified" db:"email_verified"`
	CreatedAt        time.Time  `json:"created_at" db:"created_at"`
	UpdatedAt        time.Time  `json:"updated_at" db:"updated_at"`
	LastLoginAt      *time.Time `json:"last_login_at,omitempty" db:"last_login_at"`

	// Additional fields
	Profile     *UserProfile `json:"profile,omitempty"`
	Permissions []string     `json:"permissions,omitempty"`
}

// UserProfile contains additional user information
type UserProfile struct {
	UserID      string `json:"user_id" db:"user_id"`
	FirstName   string `json:"first_name" db:"first_name"`
	LastName    string `json:"last_name" db:"last_name"`
	Company     string `json:"company,omitempty" db:"company"`
	Phone       string `json:"phone,omitempty" db:"phone"`
	AvatarURL   string `json:"avatar_url,omitempty" db:"avatar_url"`
	Preferences JSONB  `json:"preferences,omitempty" db:"preferences"`
}

// JSONB handles JSON data in database
type JSONB map[string]interface{}

// Value implements driver.Valuer interface
func (j JSONB) Value() (driver.Value, error) {
	return json.Marshal(j)
}

// Scan implements sql.Scanner interface
func (j *JSONB) Scan(value interface{}) error {
	bytes, ok := value.([]byte)
	if !ok {
		return fmt.Errorf("failed to scan JSONB")
	}
	return json.Unmarshal(bytes, j)
}

// CreateUserRequest for user registration
type CreateUserRequest struct {
	Email     string `json:"email" binding:"required,email"`
	Password  string `json:"password" binding:"required,min=8"`
	ClientID  string `json:"client_id" binding:"required"`
	FirstName string `json:"first_name"`
	LastName  string `json:"last_name"`
	Company   string `json:"company"`
}

// UpdateUserRequest for user updates
type UpdateUserRequest struct {
	FirstName   *string `json:"first_name"`
	LastName    *string `json:"last_name"`
	Company     *string `json:"company"`
	Phone       *string `json:"phone"`
	Preferences *JSONB  `json:"preferences"`
}

// ChangePasswordRequest for password changes
type ChangePasswordRequest struct {
	CurrentPassword string `json:"current_password" binding:"required"`
	NewPassword     string `json:"new_password" binding:"required,min=8"`
}
-------------------------------------------------
filepath = ./internal/auth-service/user/service.go
package user

import (
	"context"
	"fmt"
	"strings"

	"github.com/gqls/agentchassis/internal/auth-service/jwt"
	"go.uber.org/zap"
)

// Service handles business logic for users
type Service struct {
	repo   *Repository
	logger *zap.Logger
}

// NewService creates a new user service
func NewService(repo *Repository, logger *zap.Logger) *Service {
	return &Service{
		repo:   repo,
		logger: logger,
	}
}

// Register creates a new user account
func (s *Service) Register(ctx context.Context, req *CreateUserRequest) (*User, error) {
	// Validate email format
	req.Email = strings.ToLower(strings.TrimSpace(req.Email))

	// Check if user already exists
	existingUser, _ := s.repo.GetUserByEmail(ctx, req.Email)
	if existingUser != nil {
		return nil, fmt.Errorf("user with email %s already exists", req.Email)
	}

	// Create user
	user, err := s.repo.CreateUser(ctx, req)
	if err != nil {
		s.logger.Error("Failed to create user", zap.Error(err))
		return nil, err
	}

	// TODO: Send verification email

	s.logger.Info("User registered successfully",
		zap.String("user_id", user.ID),
		zap.String("email", user.Email))

	return user, nil
}

// Login validates user credentials
func (s *Service) Login(ctx context.Context, email, password string) (*User, error) {
	email = strings.ToLower(strings.TrimSpace(email))

	user, err := s.repo.GetUserByEmail(ctx, email)
	if err != nil {
		s.logger.Warn("Login attempt for non-existent user", zap.String("email", email))
		return nil, fmt.Errorf("invalid credentials")
	}

	if !s.repo.ValidatePassword(user, password) {
		s.logger.Warn("Invalid password attempt", zap.String("user_id", user.ID))
		return nil, fmt.Errorf("invalid credentials")
	}

	if !user.IsActive {
		return nil, fmt.Errorf("account is disabled")
	}

	// Update last login
	if err := s.repo.UpdateLastLogin(ctx, user.ID); err != nil {
		s.logger.Error("Failed to update last login", zap.Error(err))
	}

	return user, nil
}

// GetUser retrieves user details
func (s *Service) GetUser(ctx context.Context, userID string) (*User, error) {
	return s.repo.GetUserByID(ctx, userID)
}

// GetUserInfo returns user info for JWT token generation
func (s *Service) GetUserInfo(ctx context.Context, userID string) (*jwt.UserInfo, error) {
	user, err := s.repo.GetUserByID(ctx, userID)
	if err != nil {
		return nil, err
	}

	return &jwt.UserInfo{
		UserID:      user.ID,
		Email:       user.Email,
		ClientID:    user.ClientID,
		Role:        user.Role,
		Tier:        user.SubscriptionTier,
		Permissions: user.Permissions,
	}, nil
}

// UpdateUser updates user information
func (s *Service) UpdateUser(ctx context.Context, userID string, req *UpdateUserRequest) error {
	return s.repo.UpdateUser(ctx, userID, req)
}

// ChangePassword changes user password
func (s *Service) ChangePassword(ctx context.Context, userID string, req *ChangePasswordRequest) error {
	user, err := s.repo.GetUserByID(ctx, userID)
	if err != nil {
		return err
	}

	// Validate current password
	if !s.repo.ValidatePassword(user, req.CurrentPassword) {
		return fmt.Errorf("current password is incorrect")
	}

	// Update password
	return s.repo.UpdatePassword(ctx, userID, req.NewPassword)
}

// DeleteUser deletes a user account
func (s *Service) DeleteUser(ctx context.Context, userID string) error {
	return s.repo.DeleteUser(ctx, userID)
}
-------------------------------------------------
filepath = ./internal/auth-service/admin/handlers.go
// FILE: internal/auth-service/admin/handlers.go
package admin

import (
	"net/http"
	"strconv"

	"github.com/gin-gonic/gin"
	u "github.com/gqls/agentchassis/internal/auth-service/user" // Aliased import
	"go.uber.org/zap"
)

// Handlers provides admin endpoints for user management
type Handlers struct {
	userRepo *u.Repository // Use aliased package
	logger   *zap.Logger
}

// NewHandlers creates new admin handlers
func NewHandlers(userRepo *u.Repository, logger *zap.Logger) *Handlers { // Use aliased package
	return &Handlers{
		userRepo: userRepo,
		logger:   logger,
	}
}

// UserListRequest represents query parameters for listing users
type UserListRequest struct {
	Page      int    `form:"page,default=1"`
	PageSize  int    `form:"page_size,default=20"`
	Email     string `form:"email"`
	ClientID  string `form:"client_id"`
	Role      string `form:"role"`
	Tier      string `form:"tier"`
	IsActive  *bool  `form:"is_active"`
	SortBy    string `form:"sort_by,default=created_at"`
	SortOrder string `form:"sort_order,default=desc"`
}

// UserListResponse represents paginated user list
type UserListResponse struct {
	Users      []u.User `json:"users"` // Use aliased package
	TotalCount int      `json:"total_count"`
	Page       int      `json:"page"`
	PageSize   int      `json:"page_size"`
	TotalPages int      `json:"total_pages"`
}

// HandleListUsers returns a paginated list of users with filtering
func (h *Handlers) HandleListUsers(c *gin.Context) {
	var req UserListRequest
	if err := c.ShouldBindQuery(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Validate pagination
	if req.Page < 1 {
		req.Page = 1
	}
	if req.PageSize < 1 || req.PageSize > 100 {
		req.PageSize = 20
	}

	// Get users from repository
	users, totalCount, err := h.userRepo.ListUsers(c.Request.Context(), u.ListUsersParams{ // Use aliased package
		Offset:    (req.Page - 1) * req.PageSize,
		Limit:     req.PageSize,
		Email:     req.Email,
		ClientID:  req.ClientID,
		Role:      req.Role,
		Tier:      req.Tier,
		IsActive:  req.IsActive,
		SortBy:    req.SortBy,
		SortOrder: req.SortOrder,
	})

	if err != nil {
		h.logger.Error("Failed to list users", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to retrieve users"})
		return
	}

	totalPages := (totalCount + req.PageSize - 1) / req.PageSize

	c.JSON(http.StatusOK, UserListResponse{
		Users:      users,
		TotalCount: totalCount,
		Page:       req.Page,
		PageSize:   req.PageSize,
		TotalPages: totalPages,
	})
}

// HandleGetUser returns detailed information about a specific user
func (h *Handlers) HandleGetUser(c *gin.Context) {
	userID := c.Param("user_id")

	user, err := h.userRepo.GetUserByID(c.Request.Context(), userID)
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": "User not found"})
		return
	}

	// Get additional statistics
	stats, err := h.userRepo.GetUserStats(c.Request.Context(), userID)
	if err != nil {
		h.logger.Warn("Failed to get user stats", zap.Error(err))
		stats = &u.UserStats{} // Corrected: Use aliased package 'u' instead of variable 'user'
	}

	c.JSON(http.StatusOK, gin.H{
		"user":  user,
		"stats": stats,
	})
}

// UpdateUserRequest represents admin user update
type UpdateUserRequest struct {
	Role             *string `json:"role"`
	SubscriptionTier *string `json:"subscription_tier"`
	IsActive         *bool   `json:"is_active"`
	EmailVerified    *bool   `json:"email_verified"`
}

// HandleUpdateUser allows admins to update user details
func (h *Handlers) HandleUpdateUser(c *gin.Context) {
	userID := c.Param("user_id")

	var req UpdateUserRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Validate role if provided
	if req.Role != nil {
		validRoles := map[string]bool{"user": true, "admin": true, "moderator": true}
		if !validRoles[*req.Role] {
			c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid role"})
			return
		}
	}

	// Validate tier if provided
	if req.SubscriptionTier != nil {
		validTiers := map[string]bool{"free": true, "basic": true, "premium": true, "enterprise": true}
		if !validTiers[*req.SubscriptionTier] {
			c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid subscription tier"})
			return
		}
	}

	// Update user
	err := h.userRepo.AdminUpdateUser(c.Request.Context(), userID, &u.AdminUpdateRequest{ // Use aliased package
		Role:             req.Role,
		SubscriptionTier: req.SubscriptionTier,
		IsActive:         req.IsActive,
		EmailVerified:    req.EmailVerified,
	})

	if err != nil {
		h.logger.Error("Failed to update user", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to update user"})
		return
	}

	// Return updated user
	updatedUser, _ := h.userRepo.GetUserByID(c.Request.Context(), userID)
	c.JSON(http.StatusOK, updatedUser)
}

// HandleDeleteUser soft deletes a user account
func (h *Handlers) HandleDeleteUser(c *gin.Context) {
	userID := c.Param("user_id")

	// Prevent self-deletion
	currentUserID := c.GetString("user_id")
	if currentUserID == userID {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Cannot delete your own account"})
		return
	}

	err := h.userRepo.DeleteUser(c.Request.Context(), userID)
	if err != nil {
		h.logger.Error("Failed to delete user", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to delete user"})
		return
	}

	c.JSON(http.StatusOK, gin.H{"message": "User deleted successfully"})
}

// HandleGetUserActivity returns user activity logs
func (h *Handlers) HandleGetUserActivity(c *gin.Context) {
	userID := c.Param("user_id")

	// Parse query parameters
	limit, _ := strconv.Atoi(c.DefaultQuery("limit", "50"))
	offset, _ := strconv.Atoi(c.DefaultQuery("offset", "0"))

	activities, err := h.userRepo.GetUserActivityLog(c.Request.Context(), userID, limit, offset)
	if err != nil {
		h.logger.Error("Failed to get user activity", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to retrieve activity log"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"user_id":    userID,
		"activities": activities,
		"count":      len(activities),
	})
}

// HandleGrantPermission grants a permission to a user
func (h *Handlers) HandleGrantPermission(c *gin.Context) {
	userID := c.Param("user_id")

	var req struct {
		PermissionName string `json:"permission_name" binding:"required"`
	}

	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	err := h.userRepo.GrantPermission(c.Request.Context(), userID, req.PermissionName)
	if err != nil {
		h.logger.Error("Failed to grant permission", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to grant permission"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"message":    "Permission granted successfully",
		"user_id":    userID,
		"permission": req.PermissionName,
	})
}

// HandleRevokePermission revokes a permission from a user
func (h *Handlers) HandleRevokePermission(c *gin.Context) {
	userID := c.Param("user_id")
	permissionName := c.Param("permission_name")

	err := h.userRepo.RevokePermission(c.Request.Context(), userID, permissionName)
	if err != nil {
		h.logger.Error("Failed to revoke permission", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to revoke permission"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"message":    "Permission revoked successfully",
		"user_id":    userID,
		"permission": permissionName,
	})
}
-------------------------------------------------
filepath = ./internal/auth-service/admin/user_management.go
// FILE: internal/auth-service/admin/user_management.go
package admin

import (
	"context"
	"crypto/rand"
	"database/sql"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
	"github.com/gqls/agentchassis/internal/auth-service/user"
	"go.uber.org/zap"
)

// UserManagementHandlers provides enhanced user management functionality
type UserManagementHandlers struct {
	userRepo *user.Repository
	db       *sql.DB
	logger   *zap.Logger
}

// NewUserManagementHandlers creates new user management handlers
func NewUserManagementHandlers(userRepo *user.Repository, db *sql.DB, logger *zap.Logger) *UserManagementHandlers {
	return &UserManagementHandlers{
		userRepo: userRepo,
		db:       db,
		logger:   logger,
	}
}

// BulkUserOperation represents a bulk operation on users
type BulkUserOperation struct {
	UserIDs   []string               `json:"user_ids" binding:"required"`
	Operation string                 `json:"operation" binding:"required,oneof=activate deactivate delete upgrade_tier"`
	Params    map[string]interface{} `json:"params,omitempty"`
	Reason    string                 `json:"reason"`
}

// UserExportRequest for exporting user data
type UserExportRequest struct {
	Format  string      `json:"format" binding:"required,oneof=csv json"`
	Filters UserFilters `json:"filters"`
	Fields  []string    `json:"fields"`
}

type UserFilters struct {
	ClientID         string     `json:"client_id"`
	SubscriptionTier string     `json:"subscription_tier"`
	Role             string     `json:"role"`
	IsActive         *bool      `json:"is_active"`
	CreatedAfter     *time.Time `json:"created_after"`
	CreatedBefore    *time.Time `json:"created_before"`
}

// UserImportResult tracks the result of a bulk import
type UserImportResult struct {
	TotalProcessed int      `json:"total_processed"`
	Successful     int      `json:"successful"`
	Failed         int      `json:"failed"`
	Errors         []string `json:"errors,omitempty"`
	UserIDs        []string `json:"created_user_ids"`
}

// HandleBulkUserOperation performs bulk operations on multiple users
func (h *UserManagementHandlers) HandleBulkUserOperation(c *gin.Context) {
	var req BulkUserOperation
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Log the bulk operation
	h.logger.Info("Performing bulk user operation",
		zap.String("operation", req.Operation),
		zap.Int("user_count", len(req.UserIDs)),
		zap.String("admin_id", c.GetString("user_id")))

	results := map[string]interface{}{
		"operation": req.Operation,
		"total":     len(req.UserIDs),
		"succeeded": 0,
		"failed":    0,
		"errors":    []string{},
	}

	for _, userID := range req.UserIDs {
		var err error

		switch req.Operation {
		case "activate":
			err = h.activateUser(c.Request.Context(), userID)
		case "deactivate":
			err = h.deactivateUser(c.Request.Context(), userID, req.Reason)
		case "delete":
			err = h.deleteUser(c.Request.Context(), userID)
		case "upgrade_tier":
			if tier, ok := req.Params["tier"].(string); ok {
				err = h.upgradeTier(c.Request.Context(), userID, tier)
			} else {
				err = fmt.Errorf("tier parameter required for upgrade_tier operation")
			}
		}

		if err != nil {
			results["failed"] = results["failed"].(int) + 1
			errors := results["errors"].([]string)
			results["errors"] = append(errors, fmt.Sprintf("User %s: %v", userID, err))
		} else {
			results["succeeded"] = results["succeeded"].(int) + 1
		}
	}

	// Log activity
	h.logBulkOperation(c.Request.Context(), c.GetString("user_id"), req)

	c.JSON(http.StatusOK, results)
}

// HandleExportUsers exports user data in various formats
func (h *UserManagementHandlers) HandleExportUsers(c *gin.Context) {
	var req UserExportRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Build query with filters
	query, args := h.buildUserExportQuery(req.Filters, req.Fields)

	rows, err := h.db.QueryContext(c.Request.Context(), query, args...)
	if err != nil {
		h.logger.Error("Failed to export users", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to export users"})
		return
	}
	defer rows.Close()

	switch req.Format {
	case "csv":
		h.exportAsCSV(c, rows, req.Fields)
	case "json":
		h.exportAsJSON(c, rows, req.Fields)
	}
}

// HandleImportUsers imports users from CSV
func (h *UserManagementHandlers) HandleImportUsers(c *gin.Context) {
	file, _, err := c.Request.FormFile("file")
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "File required"})
		return
	}
	defer file.Close()

	clientID := c.PostForm("client_id")
	if clientID == "" {
		c.JSON(http.StatusBadRequest, gin.H{"error": "client_id required"})
		return
	}

	// Parse CSV
	reader := csv.NewReader(file)
	records, err := reader.ReadAll()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid CSV file"})
		return
	}

	if len(records) < 2 {
		c.JSON(http.StatusBadRequest, gin.H{"error": "CSV must have header and at least one row"})
		return
	}

	result := UserImportResult{
		TotalProcessed: len(records) - 1, // Excluding header
		Errors:         []string{},
		UserIDs:        []string{},
	}

	// Process each row
	headers := records[0]
	for i, row := range records[1:] {
		userReq := h.parseCSVRow(headers, row, clientID)

		createdUser, err := h.userRepo.CreateUser(c.Request.Context(), userReq)
		if err != nil {
			result.Failed++
			result.Errors = append(result.Errors, fmt.Sprintf("Row %d: %v", i+2, err))
		} else {
			result.Successful++
			result.UserIDs = append(result.UserIDs, createdUser.ID)
		}
	}

	c.JSON(http.StatusOK, result)
}

// HandleGetUserSessions returns active sessions for a user
func (h *UserManagementHandlers) HandleGetUserSessions(c *gin.Context) {
	userID := c.Param("user_id")

	sessions, err := h.getUserSessions(c.Request.Context(), userID)
	if err != nil {
		h.logger.Error("Failed to get user sessions", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to get sessions"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"user_id":  userID,
		"sessions": sessions,
		"count":    len(sessions),
	})
}

// HandleTerminateUserSessions terminates all sessions for a user
func (h *UserManagementHandlers) HandleTerminateUserSessions(c *gin.Context) {
	userID := c.Param("user_id")

	var req struct {
		Reason string `json:"reason"`
	}
	c.ShouldBindJSON(&req)

	// Invalidate all tokens for the user
	query := `DELETE FROM auth_tokens WHERE user_id = ?`
	result, err := h.db.ExecContext(c.Request.Context(), query, userID)
	if err != nil {
		h.logger.Error("Failed to terminate sessions", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to terminate sessions"})
		return
	}

	rowsAffected, _ := result.RowsAffected()

	// Log the action
	h.logUserActivity(c.Request.Context(), userID, "sessions_terminated", map[string]interface{}{
		"terminated_by": c.GetString("user_id"),
		"reason":        req.Reason,
		"count":         rowsAffected,
	})

	c.JSON(http.StatusOK, gin.H{
		"message":           "All sessions terminated",
		"sessions_affected": rowsAffected,
	})
}

// HandleResetUserPassword resets a user's password
func (h *UserManagementHandlers) HandleResetUserPassword(c *gin.Context) {
	userID := c.Param("user_id")

	var req struct {
		NewPassword      string `json:"new_password" binding:"required,min=8"`
		RequireChange    bool   `json:"require_change"`
		NotifyUser       bool   `json:"notify_user"`
		NotificationNote string `json:"notification_note"`
	}

	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Update password
	err := h.userRepo.UpdatePassword(c.Request.Context(), userID, req.NewPassword)
	if err != nil {
		h.logger.Error("Failed to reset password", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to reset password"})
		return
	}

	// Terminate existing sessions
	h.db.ExecContext(c.Request.Context(), "DELETE FROM auth_tokens WHERE user_id = ?", userID)

	// Log the action
	h.logUserActivity(c.Request.Context(), userID, "password_reset", map[string]interface{}{
		"reset_by":       c.GetString("user_id"),
		"require_change": req.RequireChange,
		"notified":       req.NotifyUser,
	})

	// TODO: Send notification if requested

	c.JSON(http.StatusOK, gin.H{
		"message": "Password reset successfully",
		"user_id": userID,
	})
}

// HandleGetUserAuditLog returns audit log for a user
func (h *UserManagementHandlers) HandleGetUserAuditLog(c *gin.Context) {
	userID := c.Param("user_id")

	// Parse query params
	limit := 100
	if l := c.Query("limit"); l != "" {
		fmt.Sscanf(l, "%d", &limit)
	}

	startDate := time.Now().AddDate(0, -1, 0) // Default: last month
	if s := c.Query("start_date"); s != "" {
		startDate, _ = time.Parse(time.RFC3339, s)
	}

	endDate := time.Now()
	if e := c.Query("end_date"); e != "" {
		endDate, _ = time.Parse(time.RFC3339, e)
	}

	// Get audit logs
	logs, err := h.getUserAuditLog(c.Request.Context(), userID, startDate, endDate, limit)
	if err != nil {
		h.logger.Error("Failed to get audit log", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to get audit log"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"user_id": userID,
		"logs":    logs,
		"count":   len(logs),
		"period": gin.H{
			"start": startDate,
			"end":   endDate,
		},
	})
}

// Helper methods

func (h *UserManagementHandlers) activateUser(ctx context.Context, userID string) error {
	query := `UPDATE users SET is_active = true, updated_at = NOW() WHERE id = ?`
	_, err := h.db.ExecContext(ctx, query, userID)
	return err
}

func (h *UserManagementHandlers) deactivateUser(ctx context.Context, userID, reason string) error {
	query := `UPDATE users SET is_active = false, updated_at = NOW() WHERE id = ?`
	_, err := h.db.ExecContext(ctx, query, userID)

	if err == nil {
		// Terminate all sessions
		h.db.ExecContext(ctx, "DELETE FROM auth_tokens WHERE user_id = ?", userID)

		// Log the deactivation
		h.logUserActivity(ctx, userID, "account_deactivated", map[string]interface{}{
			"reason": reason,
		})
	}

	return err
}

func (h *UserManagementHandlers) deleteUser(ctx context.Context, userID string) error {
	// Soft delete
	return h.userRepo.DeleteUser(ctx, userID)
}

func (h *UserManagementHandlers) upgradeTier(ctx context.Context, userID, tier string) error {
	return h.userRepo.UpdateUserTier(ctx, userID, tier)
}

func (h *UserManagementHandlers) buildUserExportQuery(filters UserFilters, fields []string) (string, []interface{}) {
	// Default fields if none specified
	if len(fields) == 0 {
		fields = []string{"id", "email", "role", "client_id", "subscription_tier", "created_at"}
	}

	// Build SELECT clause
	selectFields := []string{}
	for _, field := range fields {
		// Validate field names to prevent SQL injection
		if isValidField(field) {
			selectFields = append(selectFields, field)
		}
	}

	query := fmt.Sprintf("SELECT %s FROM users WHERE 1=1", strings.Join(selectFields, ", "))
	args := []interface{}{}
	argCount := 0

	// Add filters
	if filters.ClientID != "" {
		argCount++
		query += fmt.Sprintf(" AND client_id = ?")
		args = append(args, filters.ClientID)
	}

	if filters.SubscriptionTier != "" {
		argCount++
		query += fmt.Sprintf(" AND subscription_tier = ?")
		args = append(args, filters.SubscriptionTier)
	}

	if filters.Role != "" {
		argCount++
		query += fmt.Sprintf(" AND role = ?")
		args = append(args, filters.Role)
	}

	if filters.IsActive != nil {
		argCount++
		query += fmt.Sprintf(" AND is_active = ?")
		args = append(args, *filters.IsActive)
	}

	if filters.CreatedAfter != nil {
		argCount++
		query += fmt.Sprintf(" AND created_at > ?")
		args = append(args, *filters.CreatedAfter)
	}

	if filters.CreatedBefore != nil {
		argCount++
		query += fmt.Sprintf(" AND created_at < ?")
		args = append(args, *filters.CreatedBefore)
	}

	query += " ORDER BY created_at DESC"

	return query, args
}

func (h *UserManagementHandlers) exportAsCSV(c *gin.Context, rows *sql.Rows, fields []string) {
	c.Header("Content-Type", "text/csv")
	c.Header("Content-Disposition", "attachment; filename=users_export.csv")

	writer := csv.NewWriter(c.Writer)
	defer writer.Flush()

	// Write headers
	writer.Write(fields)

	// Write data
	for rows.Next() {
		values := make([]interface{}, len(fields))
		valuePtrs := make([]interface{}, len(fields))
		for i := range values {
			valuePtrs[i] = &values[i]
		}

		if err := rows.Scan(valuePtrs...); err != nil {
			continue
		}

		record := make([]string, len(fields))
		for i, v := range values {
			record[i] = fmt.Sprintf("%v", v)
		}
		writer.Write(record)
	}
}

func (h *UserManagementHandlers) exportAsJSON(c *gin.Context, rows *sql.Rows, fields []string) {
	users := []map[string]interface{}{}

	for rows.Next() {
		values := make([]interface{}, len(fields))
		valuePtrs := make([]interface{}, len(fields))
		for i := range values {
			valuePtrs[i] = &values[i]
		}

		if err := rows.Scan(valuePtrs...); err != nil {
			continue
		}

		user := make(map[string]interface{})
		for i, field := range fields {
			user[field] = values[i]
		}
		users = append(users, user)
	}

	c.JSON(http.StatusOK, gin.H{
		"users":       users,
		"count":       len(users),
		"exported_at": time.Now(),
	})
}

func (h *UserManagementHandlers) parseCSVRow(headers []string, row []string, clientID string) *user.CreateUserRequest {
	req := &user.CreateUserRequest{
		ClientID: clientID,
	}

	for i, header := range headers {
		if i < len(row) {
			switch strings.ToLower(header) {
			case "email":
				req.Email = row[i]
			case "password":
				req.Password = row[i]
			case "first_name", "firstname":
				req.FirstName = row[i]
			case "last_name", "lastname":
				req.LastName = row[i]
			case "company":
				req.Company = row[i]
			}
		}
	}

	// Generate password if not provided
	if req.Password == "" {
		req.Password = generateRandomPassword()
	}

	return req
}

func (h *UserManagementHandlers) getUserSessions(ctx context.Context, userID string) ([]map[string]interface{}, error) {
	sessions := []map[string]interface{}{}

	query := `
        SELECT id, token_hash, expires_at, created_at
        FROM auth_tokens
        WHERE user_id = ? AND expires_at > NOW()
        ORDER BY created_at DESC
    `

	rows, err := h.db.QueryContext(ctx, query, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	for rows.Next() {
		var session struct {
			ID        string
			TokenHash string
			ExpiresAt time.Time
			CreatedAt time.Time
		}

		if err := rows.Scan(&session.ID, &session.TokenHash, &session.ExpiresAt, &session.CreatedAt); err != nil {
			continue
		}

		sessions = append(sessions, map[string]interface{}{
			"id":         session.ID,
			"expires_at": session.ExpiresAt,
			"created_at": session.CreatedAt,
			"is_active":  true,
		})
	}

	return sessions, nil
}

func (h *UserManagementHandlers) getUserAuditLog(ctx context.Context, userID string, startDate, endDate time.Time, limit int) ([]map[string]interface{}, error) {
	logs := []map[string]interface{}{}

	query := `
        SELECT id, action, details, ip_address, user_agent, created_at
        FROM user_activity_logs
        WHERE user_id = ? AND created_at BETWEEN ? AND ?
        ORDER BY created_at DESC
        LIMIT ?
    `

	rows, err := h.db.QueryContext(ctx, query, userID, startDate, endDate, limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	for rows.Next() {
		var log user.UserActivity
		if err := rows.Scan(&log.ID, &log.Action, &log.Details, &log.IPAddress, &log.UserAgent, &log.CreatedAt); err != nil {
			continue
		}

		logs = append(logs, map[string]interface{}{
			"id":         log.ID,
			"action":     log.Action,
			"details":    log.Details,
			"ip_address": log.IPAddress,
			"user_agent": log.UserAgent,
			"created_at": log.CreatedAt,
		})
	}

	return logs, nil
}

func (h *UserManagementHandlers) logUserActivity(ctx context.Context, userID, action string, details map[string]interface{}) {
	detailsJSON, _ := json.Marshal(details)

	activity := &user.UserActivity{
		ID:        uuid.NewString(),
		UserID:    userID,
		Action:    action,
		Details:   string(detailsJSON),
		CreatedAt: time.Now(),
	}

	h.userRepo.LogUserActivity(ctx, activity)
}

func (h *UserManagementHandlers) logBulkOperation(ctx context.Context, adminID string, operation BulkUserOperation) {
	details := map[string]interface{}{
		"operation":  operation.Operation,
		"user_count": len(operation.UserIDs),
		"reason":     operation.Reason,
		"params":     operation.Params,
	}

	h.logUserActivity(ctx, adminID, "bulk_user_operation", details)
}

func isValidField(field string) bool {
	allowedFields := map[string]bool{
		"id":                true,
		"email":             true,
		"role":              true,
		"client_id":         true,
		"subscription_tier": true,
		"is_active":         true,
		"created_at":        true,
		"updated_at":        true,
		"last_login_at":     true,
	}
	return allowedFields[field]
}

func generateRandomPassword() string {
	const (
		length  = 16
		charset = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
	)
	b := make([]byte, length)
	if _, err := io.ReadFull(rand.Reader, b); err != nil {
		// Fallback to less secure random generator if crypto/rand fails
		return "fallbackPassword123"
	}

	for i := 0; i < length; i++ {
		b[i] = charset[int(b[i])%len(charset)]
	}
	return string(b)
}
-------------------------------------------------
filepath = ./internal/auth-service/admin/repository.go
// FILE: internal/auth-service/admin/repository.go
package admin

import (
	"database/sql"
	"go.uber.org/zap"
)

// Repository handles admin data access
type Repository struct {
	db     *sql.DB
	logger *zap.Logger
	cfg    interface{} // Temporary for compatibility
}

// NewRepository creates a new admin repository
func NewRepository(db *sql.DB, logger *zap.Logger, cfg interface{}) *Repository {
	return &Repository{
		db:     db,
		logger: logger,
		cfg:    cfg,
	}
}

// Additional admin repository methods would go here
// This is a placeholder for admin-specific database operations
-------------------------------------------------
filepath = ./internal/auth-service/middleware/cors.go
// FILE: internal/auth-service/middleware/cors.go
package middleware

import (
	"github.com/gin-gonic/gin"
	"net/http"
)

// CORSMiddleware handles CORS headers
func CORSMiddleware(allowedOrigins []string) gin.HandlerFunc {
	return func(c *gin.Context) {
		origin := c.Request.Header.Get("Origin")

		// Check if origin is allowed
		allowed := false
		for _, allowedOrigin := range allowedOrigins {
			if allowedOrigin == "*" || allowedOrigin == origin {
				allowed = true
				break
			}
		}

		if allowed {
			c.Header("Access-Control-Allow-Origin", origin)
			c.Header("Access-Control-Allow-Credentials", "true")
			c.Header("Access-Control-Allow-Headers", "Content-Type, Content-Length, Accept-Encoding, X-CSRF-Token, Authorization, accept, origin, Cache-Control, X-Requested-With")
			c.Header("Access-Control-Allow-Methods", "POST, OPTIONS, GET, PUT, DELETE, PATCH")
		}

		if c.Request.Method == "OPTIONS" {
			c.AbortWithStatus(http.StatusNoContent)
			return
		}

		c.Next()
	}
}
-------------------------------------------------
filepath = ./internal/auth-service/middleware/logging.go
// FILE: internal/auth-service/middleware/logging.go
package middleware

import (
	"time"

	"github.com/gin-gonic/gin"
	"go.uber.org/zap"
)

// LoggingMiddleware logs HTTP requests
func LoggingMiddleware(logger *zap.Logger) gin.HandlerFunc {
	return func(c *gin.Context) {
		start := time.Now()
		path := c.Request.URL.Path
		raw := c.Request.URL.RawQuery

		// Process request
		c.Next()

		// Log after request is processed
		latency := time.Since(start)

		if raw != "" {
			path = path + "?" + raw
		}

		logger.Info("HTTP Request",
			zap.String("method", c.Request.Method),
			zap.String("path", path),
			zap.Int("status", c.Writer.Status()),
			zap.String("ip", c.ClientIP()),
			zap.Duration("latency", latency),
			zap.String("user_agent", c.Request.UserAgent()),
			zap.String("error", c.Errors.ByType(gin.ErrorTypePrivate).String()),
		)
	}
}
-------------------------------------------------
filepath = ./internal/auth-service/middleware/auth.go
// FILE: internal/auth-service/middleware/auth.go
package middleware

import (
	"net/http"
	"strings"

	"github.com/gin-gonic/gin"
	"github.com/gqls/agentchassis/internal/auth-service/jwt"
	"go.uber.org/zap"
)

// RequireAuth validates JWT tokens
func RequireAuth(jwtService *jwt.Service, logger *zap.Logger) gin.HandlerFunc {
	return func(c *gin.Context) {
		authHeader := c.GetHeader("Authorization")
		if authHeader == "" {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Authorization header required"})
			c.Abort()
			return
		}

		tokenString := strings.Replace(authHeader, "Bearer ", "", 1)

		claims, err := jwtService.ValidateToken(tokenString)
		if err != nil {
			logger.Debug("Token validation failed", zap.Error(err))
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Invalid token"})
			c.Abort()
			return
		}

		// Set user context
		c.Set("user_id", claims.UserID)
		c.Set("client_id", claims.ClientID)
		c.Set("user_email", claims.Email)
		c.Set("user_role", claims.Role)
		c.Set("user_tier", claims.Tier)
		c.Set("user_permissions", claims.Permissions)

		c.Next()
	}
}

// RequireRole checks if user has specific role
func RequireRole(role string) gin.HandlerFunc {
	return func(c *gin.Context) {
		userRole, exists := c.Get("user_role")
		if !exists {
			c.JSON(http.StatusForbidden, gin.H{"error": "No role found"})
			c.Abort()
			return
		}

		roleStr, ok := userRole.(string)
		if !ok {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid role format"})
			c.Abort()
			return
		}

		if roleStr != role {
			c.JSON(http.StatusForbidden, gin.H{"error": "Insufficient role"})
			c.Abort()
			return
		}

		c.Next()
	}
}
-------------------------------------------------
filepath = ./internal/auth-service/project/handlers.go
// FILE: internal/auth-service/project/handlers.go
package project

import (
	"encoding/json"
	"net/http"
	"time"

	"github.com/google/uuid"
	"go.uber.org/zap"
)

// HTTPHandler handles project-related HTTP requests
type HTTPHandler struct {
	repo   *Repository
	logger *zap.Logger
}

// NewHTTPHandler creates a new project HTTP handler
func NewHTTPHandler(repo *Repository, logger *zap.Logger) *HTTPHandler {
	return &HTTPHandler{
		repo:   repo,
		logger: logger,
	}
}

// CreateProject handles project creation
func (h *HTTPHandler) CreateProject(w http.ResponseWriter, r *http.Request) {
	userID := r.Context().Value("user_id").(string)
	clientID := r.Context().Value("client_id").(string)

	var req struct {
		Name        string `json:"name"`
		Description string `json:"description"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	project := &Project{
		ID:          uuid.New().String(),
		ClientID:    clientID,
		Name:        req.Name,
		Description: req.Description,
		OwnerID:     userID,
		IsActive:    true,
		CreatedAt:   time.Now(),
		UpdatedAt:   time.Now(),
	}

	if err := h.repo.Create(r.Context(), project); err != nil {
		h.logger.Error("Failed to create project", zap.Error(err))
		http.Error(w, "Failed to create project", http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(project)
}

// ListProjects returns all projects for a user
func (h *HTTPHandler) ListProjects(w http.ResponseWriter, r *http.Request) {
	userID := r.Context().Value("user_id").(string)
	clientID := r.Context().Value("client_id").(string)

	projects, err := h.repo.ListByUser(r.Context(), clientID, userID)
	if err != nil {
		h.logger.Error("Failed to list projects", zap.Error(err))
		http.Error(w, "Failed to retrieve projects", http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"projects": projects,
		"count":    len(projects),
	})
}

// GetProject returns a specific project
func (h *HTTPHandler) GetProject(w http.ResponseWriter, r *http.Request, projectID string) {
	userID := r.Context().Value("user_id").(string)
	clientID := r.Context().Value("client_id").(string)

	project, err := h.repo.GetByID(r.Context(), projectID)
	if err != nil {
		http.Error(w, "Project not found", http.StatusNotFound)
		return
	}

	// Verify ownership
	if project.ClientID != clientID || project.OwnerID != userID {
		http.Error(w, "Access denied", http.StatusForbidden)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(project)
}

// UpdateProject updates a project
func (h *HTTPHandler) UpdateProject(w http.ResponseWriter, r *http.Request, projectID string) {
	userID := r.Context().Value("user_id").(string)
	clientID := r.Context().Value("client_id").(string)

	// Verify ownership first
	project, err := h.repo.GetByID(r.Context(), projectID)
	if err != nil {
		http.Error(w, "Project not found", http.StatusNotFound)
		return
	}

	if project.ClientID != clientID || project.OwnerID != userID {
		http.Error(w, "Access denied", http.StatusForbidden)
		return
	}

	// Parse update request
	var req struct {
		Name        *string `json:"name"`
		Description *string `json:"description"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	// Apply updates
	if req.Name != nil {
		project.Name = *req.Name
	}
	if req.Description != nil {
		project.Description = *req.Description
	}
	project.UpdatedAt = time.Now()

	if err := h.repo.Update(r.Context(), project); err != nil {
		h.logger.Error("Failed to update project", zap.Error(err))
		http.Error(w, "Failed to update project", http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(project)
}

// DeleteProject deletes a project
func (h *HTTPHandler) DeleteProject(w http.ResponseWriter, r *http.Request, projectID string) {
	userID := r.Context().Value("user_id").(string)
	clientID := r.Context().Value("client_id").(string)

	// Verify ownership
	project, err := h.repo.GetByID(r.Context(), projectID)
	if err != nil {
		http.Error(w, "Project not found", http.StatusNotFound)
		return
	}

	if project.ClientID != clientID || project.OwnerID != userID {
		http.Error(w, "Access denied", http.StatusForbidden)
		return
	}

	if err := h.repo.Delete(r.Context(), projectID); err != nil {
		h.logger.Error("Failed to delete project", zap.Error(err))
		http.Error(w, "Failed to delete project", http.StatusInternalServerError)
		return
	}

	w.WriteHeader(http.StatusNoContent)
}
-------------------------------------------------
filepath = ./internal/auth-service/project/repository.go
// FILE: internal/auth-service/project/repository.go
package project

import (
	"context"
	"database/sql"
	"fmt"
	"time"

	"go.uber.org/zap"
)

// Repository handles project data access
type Repository struct {
	db     *sql.DB
	logger *zap.Logger
}

// NewRepository creates a new project repository
func NewRepository(db *sql.DB, logger *zap.Logger) *Repository {
	return &Repository{
		db:     db,
		logger: logger,
	}
}

// Create creates a new project
func (r *Repository) Create(ctx context.Context, project *Project) error {
	query := `
        INSERT INTO projects (id, client_id, name, description, owner_id, is_active, created_at, updated_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    `

	_, err := r.db.ExecContext(ctx, query,
		project.ID, project.ClientID, project.Name, project.Description,
		project.OwnerID, project.IsActive, project.CreatedAt, project.UpdatedAt,
	)

	if err != nil {
		return fmt.Errorf("failed to create project: %w", err)
	}

	return nil
}

// GetByID retrieves a project by ID
func (r *Repository) GetByID(ctx context.Context, id string) (*Project, error) {
	var p Project
	query := `
        SELECT id, client_id, name, description, owner_id, is_active, created_at, updated_at
        FROM projects
        WHERE id = ? AND is_active = true
    `

	err := r.db.QueryRowContext(ctx, query, id).Scan(
		&p.ID, &p.ClientID, &p.Name, &p.Description,
		&p.OwnerID, &p.IsActive, &p.CreatedAt, &p.UpdatedAt,
	)

	if err != nil {
		if err == sql.ErrNoRows {
			return nil, fmt.Errorf("project not found")
		}
		return nil, err
	}

	return &p, nil
}

// ListByUser returns all projects for a user
func (r *Repository) ListByUser(ctx context.Context, clientID, userID string) ([]Project, error) {
	query := `
        SELECT id, client_id, name, description, owner_id, is_active, created_at, updated_at
        FROM projects
        WHERE client_id = ? AND owner_id = ? AND is_active = true
        ORDER BY created_at DESC
    `

	rows, err := r.db.QueryContext(ctx, query, clientID, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var projects []Project
	for rows.Next() {
		var p Project
		if err := rows.Scan(
			&p.ID, &p.ClientID, &p.Name, &p.Description,
			&p.OwnerID, &p.IsActive, &p.CreatedAt, &p.UpdatedAt,
		); err != nil {
			r.logger.Error("Failed to scan project", zap.Error(err))
			continue
		}
		projects = append(projects, p)
	}

	return projects, nil
}

// Update updates a project
func (r *Repository) Update(ctx context.Context, project *Project) error {
	query := `
        UPDATE projects
        SET name = ?, description = ?, updated_at = ?
        WHERE id = ?
    `

	_, err := r.db.ExecContext(ctx, query,
		project.Name, project.Description, project.UpdatedAt, project.ID,
	)

	return err
}

// Delete soft deletes a project
func (r *Repository) Delete(ctx context.Context, id string) error {
	query := `
        UPDATE projects
        SET is_active = false, updated_at = ?
        WHERE id = ?
    `

	_, err := r.db.ExecContext(ctx, query, time.Now(), id)
	return err
}
-------------------------------------------------
filepath = ./internal/auth-service/project/models.go
// FILE: internal/auth-service/project/models.go
package project

import "time"

// Project represents a project in the system
type Project struct {
	ID          string    `json:"id" db:"id"`
	ClientID    string    `json:"client_id" db:"client_id"`
	Name        string    `json:"name" db:"name"`
	Description string    `json:"description" db:"description"`
	OwnerID     string    `json:"owner_id" db:"owner_id"`
	IsActive    bool      `json:"is_active" db:"is_active"`
	CreatedAt   time.Time `json:"created_at" db:"created_at"`
	UpdatedAt   time.Time `json:"updated_at" db:"updated_at"`
}
-------------------------------------------------
filepath = ./internal/auth-service/jwt/claims.go
package jwt

import (
	"fmt"
	"github.com/golang-jwt/jwt/v5"
	"time"
)

// GetTokenFromString extracts claims without validation (for logging/debugging only)
func GetTokenFromString(tokenString string) (*Claims, error) {
	token, _, err := new(jwt.Parser).ParseUnverified(tokenString, &Claims{})
	if err != nil {
		return nil, err
	}

	if claims, ok := token.Claims.(*Claims); ok {
		return claims, nil
	}

	return nil, fmt.Errorf("invalid claims type")
}

// IsTokenExpired checks if a token is expired without full validation
func IsTokenExpired(claims *Claims) bool {
	return claims.ExpiresAt.Time.Before(time.Now())
}
-------------------------------------------------
filepath = ./internal/auth-service/jwt/service.go
package jwt

import (
	"fmt"
	"time"

	"github.com/golang-jwt/jwt/v5"
	"go.uber.org/zap"
)

// Service handles JWT operations
type Service struct {
	secretKey       []byte
	accessTokenTTL  time.Duration
	refreshTokenTTL time.Duration
	logger          *zap.Logger
}

// Claims represents the JWT claims
type Claims struct {
	UserID      string   `json:"user_id"`
	Email       string   `json:"email"`
	ClientID    string   `json:"client_id"`
	Role        string   `json:"role"`
	Tier        string   `json:"tier"`
	Permissions []string `json:"permissions,omitempty"`
	jwt.RegisteredClaims
}

// RefreshClaims for refresh tokens
type RefreshClaims struct {
	UserID string `json:"user_id"`
	jwt.RegisteredClaims
}

// NewService creates a new JWT service
func NewService(secretKey string, accessMinutes int, logger *zap.Logger) (*Service, error) {
	if secretKey == "" {
		return nil, fmt.Errorf("JWT secret key cannot be empty")
	}

	return &Service{
		secretKey:       []byte(secretKey),
		accessTokenTTL:  time.Duration(accessMinutes) * time.Minute,
		refreshTokenTTL: 7 * 24 * time.Hour, // 7 days
		logger:          logger,
	}, nil
}

// GenerateTokens creates both access and refresh tokens
func (s *Service) GenerateTokens(userID, email, clientID, role, tier string, permissions []string) (string, string, error) {
	accessToken, err := s.generateAccessToken(userID, email, clientID, role, tier, permissions)
	if err != nil {
		return "", "", fmt.Errorf("failed to generate access token: %w", err)
	}

	refreshToken, err := s.generateRefreshToken(userID)
	if err != nil {
		return "", "", fmt.Errorf("failed to generate refresh token: %w", err)
	}

	return accessToken, refreshToken, nil
}

// generateAccessToken creates an access token with full claims
func (s *Service) generateAccessToken(userID, email, clientID, role, tier string, permissions []string) (string, error) {
	now := time.Now()
	claims := Claims{
		UserID:      userID,
		Email:       email,
		ClientID:    clientID,
		Role:        role,
		Tier:        tier,
		Permissions: permissions,
		RegisteredClaims: jwt.RegisteredClaims{
			ExpiresAt: jwt.NewNumericDate(now.Add(s.accessTokenTTL)),
			IssuedAt:  jwt.NewNumericDate(now),
			NotBefore: jwt.NewNumericDate(now),
			Issuer:    "ai-persona-system",
			Subject:   userID,
			ID:        fmt.Sprintf("%d", now.Unix()),
		},
	}

	token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
	return token.SignedString(s.secretKey)
}

// generateRefreshToken creates a refresh token with minimal claims
func (s *Service) generateRefreshToken(userID string) (string, error) {
	now := time.Now()
	claims := RefreshClaims{
		UserID: userID,
		RegisteredClaims: jwt.RegisteredClaims{
			ExpiresAt: jwt.NewNumericDate(now.Add(s.refreshTokenTTL)),
			IssuedAt:  jwt.NewNumericDate(now),
			Subject:   userID,
			ID:        fmt.Sprintf("refresh_%d", now.Unix()),
		},
	}

	token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
	return token.SignedString(s.secretKey)
}

// ValidateToken validates and parses an access token
func (s *Service) ValidateToken(tokenString string) (*Claims, error) {
	token, err := jwt.ParseWithClaims(tokenString, &Claims{}, func(token *jwt.Token) (interface{}, error) {
		if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
			return nil, fmt.Errorf("unexpected signing method: %v", token.Header["alg"])
		}
		return s.secretKey, nil
	})

	if err != nil {
		return nil, fmt.Errorf("failed to parse token: %w", err)
	}

	if claims, ok := token.Claims.(*Claims); ok && token.Valid {
		return claims, nil
	}

	return nil, fmt.Errorf("invalid token claims")
}

// ValidateRefreshToken validates a refresh token
func (s *Service) ValidateRefreshToken(tokenString string) (*RefreshClaims, error) {
	token, err := jwt.ParseWithClaims(tokenString, &RefreshClaims{}, func(token *jwt.Token) (interface{}, error) {
		if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
			return nil, fmt.Errorf("unexpected signing method: %v", token.Header["alg"])
		}
		return s.secretKey, nil
	})

	if err != nil {
		return nil, fmt.Errorf("failed to parse refresh token: %w", err)
	}

	if claims, ok := token.Claims.(*RefreshClaims); ok && token.Valid {
		return claims, nil
	}

	return nil, fmt.Errorf("invalid refresh token")
}

// RefreshAccessToken creates a new access token from a refresh token
func (s *Service) RefreshAccessToken(refreshToken string, getUserFunc func(userID string) (*UserInfo, error)) (string, error) {
	claims, err := s.ValidateRefreshToken(refreshToken)
	if err != nil {
		return "", err
	}

	// Get updated user details
	userInfo, err := getUserFunc(claims.UserID)
	if err != nil {
		return "", fmt.Errorf("failed to get user details: %w", err)
	}

	// Generate new access token with current user info
	return s.generateAccessToken(
		userInfo.UserID,
		userInfo.Email,
		userInfo.ClientID,
		userInfo.Role,
		userInfo.Tier,
		userInfo.Permissions,
	)
}

// UserInfo holds user information for token generation
type UserInfo struct {
	UserID      string
	Email       string
	ClientID    string
	Role        string
	Tier        string
	Permissions []string
}
-------------------------------------------------
filepath = ./internal/auth-service/auth/middleware.go
package auth

import (
	"fmt"
	"net/http"
	"strings"

	"github.com/gin-gonic/gin"
	"github.com/gqls/agentchassis/internal/auth-service/jwt"
	"go.uber.org/zap"
)

// AuthMiddleware validates JWT tokens
func AuthMiddleware(jwtService *jwt.Service, logger *zap.Logger) gin.HandlerFunc {
	return func(c *gin.Context) {
		authHeader := c.GetHeader("Authorization")
		if authHeader == "" {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Authorization header required"})
			c.Abort()
			return
		}

		// Extract token
		parts := strings.Split(authHeader, " ")
		if len(parts) != 2 || parts[0] != "Bearer" {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Invalid authorization header format"})
			c.Abort()
			return
		}

		tokenString := parts[1]

		// Validate token
		claims, err := jwtService.ValidateToken(tokenString)
		if err != nil {
			logger.Debug("Token validation failed", zap.Error(err))
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Invalid token"})
			c.Abort()
			return
		}

		// Set user context
		c.Set("user_id", claims.UserID)
		c.Set("client_id", claims.ClientID)
		c.Set("user_email", claims.Email)
		c.Set("user_role", claims.Role)
		c.Set("user_tier", claims.Tier)
		c.Set("user_permissions", claims.Permissions)
		c.Set("token_id", claims.ID)
		c.Set("claims", claims)

		c.Next()
	}
}

// RequirePermission checks if user has specific permission
func RequirePermission(permission string) gin.HandlerFunc {
	return func(c *gin.Context) {
		permissions, exists := c.Get("user_permissions")
		if !exists {
			c.JSON(http.StatusForbidden, gin.H{"error": "No permissions found"})
			c.Abort()
			return
		}

		userPerms, ok := permissions.([]string)
		if !ok {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid permissions format"})
			c.Abort()
			return
		}

		// Check permission
		hasPermission := false
		for _, p := range userPerms {
			if p == permission || p == "*" { // "*" is superuser
				hasPermission = true
				break
			}
		}

		if !hasPermission {
			c.JSON(http.StatusForbidden, gin.H{"error": "Insufficient permissions"})
			c.Abort()
			return
		}

		c.Next()
	}
}

// RequireRole checks if user has specific role
func RequireRole(roles ...string) gin.HandlerFunc {
	return func(c *gin.Context) {
		userRole, exists := c.Get("user_role")
		if !exists {
			c.JSON(http.StatusForbidden, gin.H{"error": "No role found"})
			c.Abort()
			return
		}

		role, ok := userRole.(string)
		if !ok {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid role format"})
			c.Abort()
			return
		}

		// Check role
		hasRole := false
		for _, r := range roles {
			if r == role {
				hasRole = true
				break
			}
		}

		if !hasRole {
			c.JSON(http.StatusForbidden, gin.H{"error": "Insufficient role"})
			c.Abort()
			return
		}

		c.Next()
	}
}

// RequireTier checks if user has minimum subscription tier
func RequireTier(minTier string) gin.HandlerFunc {
	tierLevels := map[string]int{
		"free":       0,
		"basic":      1,
		"premium":    2,
		"enterprise": 3,
	}

	return func(c *gin.Context) {
		userTier, exists := c.Get("user_tier")
		if !exists {
			c.JSON(http.StatusForbidden, gin.H{"error": "No subscription tier found"})
			c.Abort()
			return
		}

		tier, ok := userTier.(string)
		if !ok {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid tier format"})
			c.Abort()
			return
		}

		userLevel, exists := tierLevels[tier]
		if !exists {
			userLevel = 0 // Default to free
		}

		requiredLevel, exists := tierLevels[minTier]
		if !exists {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid required tier"})
			c.Abort()
			return
		}

		if userLevel < requiredLevel {
			c.JSON(http.StatusForbidden, gin.H{
				"error":         fmt.Sprintf("This feature requires %s tier or higher", minTier),
				"current_tier":  tier,
				"required_tier": minTier,
			})
			c.Abort()
			return
		}

		c.Next()
	}
}
-------------------------------------------------
filepath = ./internal/auth-service/auth/handlers.go
package auth

import (
	"net/http"
	"strings"

	"github.com/gin-gonic/gin"
	"github.com/gqls/agentchassis/internal/auth-service/user"
)

// Handlers wraps the auth service for HTTP handling
type Handlers struct {
	service ServiceInterface
}

// NewHandlers creates new auth handlers
func NewHandlers(service *Service) *Handlers {
	return &Handlers{service: service}
}

// RegisterRequest represents registration data
type RegisterRequest struct {
	Email     string `json:"email" binding:"required,email"`
	Password  string `json:"password" binding:"required,min=8"`
	ClientID  string `json:"client_id" binding:"required"`
	FirstName string `json:"first_name"`
	LastName  string `json:"last_name"`
	Company   string `json:"company"`
}

// LoginRequest represents login data
type LoginRequest struct {
	Email    string `json:"email" binding:"required,email"`
	Password string `json:"password" binding:"required"`
}

// RefreshRequest represents token refresh data
type RefreshRequest struct {
	RefreshToken string `json:"refresh_token" binding:"required"`
}

// HandleRegister handles user registration
func (h *Handlers) HandleRegister(c *gin.Context) {
	var req RegisterRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Convert to user create request
	userReq := &user.CreateUserRequest{
		Email:     req.Email,
		Password:  req.Password,
		ClientID:  req.ClientID,
		FirstName: req.FirstName,
		LastName:  req.LastName,
		Company:   req.Company,
	}

	response, err := h.service.Register(c.Request.Context(), userReq)
	if err != nil {
		if strings.Contains(err.Error(), "already exists") {
			c.JSON(http.StatusConflict, gin.H{"error": err.Error()})
			return
		}
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	c.JSON(http.StatusCreated, response)
}

// HandleLogin handles user login
func (h *Handlers) HandleLogin(c *gin.Context) {
	var req LoginRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	response, err := h.service.Login(c.Request.Context(), req.Email, req.Password)
	if err != nil {
		c.JSON(http.StatusUnauthorized, gin.H{"error": "Invalid credentials"})
		return
	}

	c.JSON(http.StatusOK, response)
}

// HandleRefresh handles token refresh
func (h *Handlers) HandleRefresh(c *gin.Context) {
	var req RefreshRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	response, err := h.service.RefreshToken(c.Request.Context(), req.RefreshToken)
	if err != nil {
		c.JSON(http.StatusUnauthorized, gin.H{"error": err.Error()})
		return
	}

	c.JSON(http.StatusOK, response)
}

// HandleLogout handles user logout
func (h *Handlers) HandleLogout(c *gin.Context) {
	// Get token ID from claims
	tokenID := c.GetString("token_id")

	if err := h.service.Logout(c.Request.Context(), tokenID); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to logout"})
		return
	}

	c.JSON(http.StatusOK, gin.H{"message": "Successfully logged out"})
}

// HandleValidate validates the current token
func (h *Handlers) HandleValidate(c *gin.Context) {
	// Get the token from the Authorization header
	authHeader := c.GetHeader("Authorization")
	if authHeader == "" {
		c.JSON(http.StatusUnauthorized, gin.H{"error": "No token provided"})
		return
	}

	tokenString := strings.Replace(authHeader, "Bearer ", "", 1)

	// Validate the token using the service
	claims, err := h.service.ValidateToken(tokenString)
	if err != nil {
		c.JSON(http.StatusUnauthorized, gin.H{"error": "Invalid token", "details": err.Error()})
		return
	}

	// Return the validation result
	c.JSON(http.StatusOK, gin.H{
		"valid": true,
		"user": gin.H{
			"id":          claims.UserID,
			"email":       claims.Email,
			"role":        claims.Role,
			"tier":        claims.Tier,
			"client_id":   claims.ClientID,
			"permissions": claims.Permissions,
		},
	})
}
-------------------------------------------------
filepath = ./internal/auth-service/auth/handlers_test.go
// FILE: internal/auth-service/auth/handlers_test.go
package auth

import (
	"bytes"
	"context" // Add this import
	"encoding/json"
	"fmt"
	"github.com/gqls/agentchassis/internal/auth-service/jwt"
	"net/http"
	"net/http/httptest"
	"testing"

	"github.com/gin-gonic/gin"
	"github.com/gqls/agentchassis/internal/auth-service/user" // Add this import
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
)

// MockService for testing
type MockService struct {
	mock.Mock
}

func (m *MockService) Register(ctx context.Context, req *user.CreateUserRequest) (*TokenResponse, error) {
	args := m.Called(ctx, req)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*TokenResponse), args.Error(1)
}

func (m *MockService) Login(ctx context.Context, email, password string) (*TokenResponse, error) {
	args := m.Called(ctx, email, password)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*TokenResponse), args.Error(1)
}

func (m *MockService) RefreshToken(ctx context.Context, refreshToken string) (*TokenResponse, error) {
	args := m.Called(ctx, refreshToken)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*TokenResponse), args.Error(1)
}

func (m *MockService) Logout(ctx context.Context, tokenID string) error {
	args := m.Called(ctx, tokenID)
	return args.Error(0)
}

func (m *MockService) ValidateToken(tokenString string) (*jwt.Claims, error) {
	args := m.Called(tokenString)
	if args.Get(0) == nil {
		return nil, args.Error(1)
	}
	return args.Get(0).(*jwt.Claims), args.Error(1)
}

func TestHandleRegister(t *testing.T) {
	gin.SetMode(gin.TestMode)

	tests := []struct {
		name           string
		requestBody    RegisterRequest
		mockResponse   *TokenResponse
		mockError      error
		expectedStatus int
		expectedBody   string
	}{
		{
			name: "successful registration",
			requestBody: RegisterRequest{
				Email:     "newuser@example.com",
				Password:  "password123",
				ClientID:  "client-1",
				FirstName: "Test",
				LastName:  "User",
			},
			mockResponse: &TokenResponse{
				AccessToken: "new_access_token",
				TokenType:   "Bearer",
			},
			expectedStatus: http.StatusCreated,
			expectedBody:   "new_access_token",
		},
		{
			name: "user already exists",
			requestBody: RegisterRequest{
				Email:    "existing@example.com",
				Password: "password123",
				ClientID: "client-1",
			},
			mockError:      fmt.Errorf("user with email existing@example.com already exists"),
			expectedStatus: http.StatusConflict,
			expectedBody:   "already exists",
		},
		{
			name: "bad request - missing password",
			requestBody: RegisterRequest{
				Email: "test@example.com",
			},
			expectedStatus: http.StatusBadRequest,
			expectedBody:   "Password",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			mockService := new(MockService)
			handler := &Handlers{service: mockService}

			// Setup mock expectation
			if tt.mockError != nil || tt.mockResponse != nil {
				// We expect the service's Register method to be called
				userReq := &user.CreateUserRequest{
					Email:     tt.requestBody.Email,
					Password:  tt.requestBody.Password,
					ClientID:  tt.requestBody.ClientID,
					FirstName: tt.requestBody.FirstName,
					LastName:  tt.requestBody.LastName,
				}
				mockService.On("Register", mock.Anything, userReq).Return(tt.mockResponse, tt.mockError).Once()
			}

			w := httptest.NewRecorder()
			c, _ := gin.CreateTestContext(w)

			body, _ := json.Marshal(tt.requestBody)
			c.Request = httptest.NewRequest("POST", "/register", bytes.NewBuffer(body))
			c.Request.Header.Set("Content-Type", "application/json")

			handler.HandleRegister(c)

			assert.Equal(t, tt.expectedStatus, w.Code)
			assert.Contains(t, w.Body.String(), tt.expectedBody)

			// Verify that the mock was called if it was expected
			if tt.mockError != nil || tt.mockResponse != nil {
				mockService.AssertExpectations(t)
			}
		})
	}
}

func TestHandleLogin(t *testing.T) {
	gin.SetMode(gin.TestMode)

	tests := []struct {
		name           string
		requestBody    LoginRequest
		mockResponse   *TokenResponse
		mockError      error
		expectedStatus int
	}{
		{
			name: "successful login",
			requestBody: LoginRequest{
				Email:    "test@example.com",
				Password: "password123",
			},
			mockResponse: &TokenResponse{
				AccessToken:  "test_token",
				RefreshToken: "refresh_token",
				TokenType:    "Bearer",
				ExpiresIn:    3600,
			},
			expectedStatus: http.StatusOK,
		},
		{
			name: "invalid credentials",
			requestBody: LoginRequest{
				Email:    "test@example.com",
				Password: "wrong_password",
			},
			mockError:      fmt.Errorf("invalid credentials"),
			expectedStatus: http.StatusUnauthorized,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			mockService := new(MockService)
			handler := &Handlers{service: mockService}

			if tt.mockResponse != nil {
				mockService.On("Login", mock.Anything, tt.requestBody.Email, tt.requestBody.Password).
					Return(tt.mockResponse, nil)
			} else {
				mockService.On("Login", mock.Anything, tt.requestBody.Email, tt.requestBody.Password).
					Return(nil, tt.mockError)
			}

			w := httptest.NewRecorder()
			c, _ := gin.CreateTestContext(w)

			body, _ := json.Marshal(tt.requestBody)
			c.Request = httptest.NewRequest("POST", "/login", bytes.NewBuffer(body))
			c.Request.Header.Set("Content-Type", "application/json")

			handler.HandleLogin(c)

			assert.Equal(t, tt.expectedStatus, w.Code)
			mockService.AssertExpectations(t)
		})
	}
}

func TestHandleRefresh(t *testing.T) {
	gin.SetMode(gin.TestMode)

	tests := []struct {
		name           string
		requestBody    RefreshRequest
		mockResponse   *TokenResponse
		mockError      error
		expectedStatus int
		expectedBody   string
	}{
		{
			name: "successful refresh",
			requestBody: RefreshRequest{
				RefreshToken: "valid_refresh_token",
			},
			mockResponse: &TokenResponse{
				AccessToken: "new_access_token_from_refresh",
				TokenType:   "Bearer",
			},
			expectedStatus: http.StatusOK,
			expectedBody:   "new_access_token_from_refresh",
		},
		{
			name: "invalid refresh token",
			requestBody: RefreshRequest{
				RefreshToken: "invalid_or_expired_token",
			},
			mockError:      fmt.Errorf("invalid refresh token"),
			expectedStatus: http.StatusUnauthorized,
			expectedBody:   "invalid refresh token",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			mockService := new(MockService)
			handler := &Handlers{service: mockService}

			// Setup mock expectation
			mockService.On("RefreshToken", mock.Anything, tt.requestBody.RefreshToken).
				Return(tt.mockResponse, tt.mockError).
				Once()

			w := httptest.NewRecorder()
			c, _ := gin.CreateTestContext(w)

			body, _ := json.Marshal(tt.requestBody)
			c.Request = httptest.NewRequest("POST", "/refresh", bytes.NewBuffer(body))
			c.Request.Header.Set("Content-Type", "application/json")

			handler.HandleRefresh(c)

			assert.Equal(t, tt.expectedStatus, w.Code)
			assert.Contains(t, w.Body.String(), tt.expectedBody)
			mockService.AssertExpectations(t)
		})
	}
}
-------------------------------------------------
filepath = ./internal/auth-service/auth/service.go
package auth

import (
	"context"
	"fmt"
	"github.com/gqls/agentchassis/internal/auth-service/jwt"
	"github.com/gqls/agentchassis/internal/auth-service/user"
	"go.uber.org/zap"
)

// Service handles authentication logic
type Service struct {
	userService *user.Service
	jwtService  *jwt.Service
	logger      *zap.Logger
}

// NewService creates a new auth service
func NewService(userService *user.Service, jwtService *jwt.Service, logger *zap.Logger) *Service {
	return &Service{
		userService: userService,
		jwtService:  jwtService,
		logger:      logger,
	}
}

// TokenResponse represents the auth response
type TokenResponse struct {
	AccessToken  string    `json:"access_token"`
	RefreshToken string    `json:"refresh_token"`
	TokenType    string    `json:"token_type"`
	ExpiresIn    int       `json:"expires_in"`
	User         *UserInfo `json:"user"`
}

// UserInfo in token response
type UserInfo struct {
	ID            string   `json:"id"`
	Email         string   `json:"email"`
	ClientID      string   `json:"client_id"`
	Role          string   `json:"role"`
	Tier          string   `json:"tier"`
	EmailVerified bool     `json:"email_verified"`
	Permissions   []string `json:"permissions"`
}

// Register handles user registration
func (s *Service) Register(ctx context.Context, req *user.CreateUserRequest) (*TokenResponse, error) {
	// Create user
	newUser, err := s.userService.Register(ctx, req)
	if err != nil {
		return nil, err
	}

	// Generate tokens
	accessToken, refreshToken, err := s.jwtService.GenerateTokens(
		newUser.ID,
		newUser.Email,
		newUser.ClientID,
		newUser.Role,
		newUser.SubscriptionTier,
		newUser.Permissions,
	)
	if err != nil {
		s.logger.Error("Failed to generate tokens", zap.Error(err))
		return nil, fmt.Errorf("failed to generate tokens")
	}

	return &TokenResponse{
		AccessToken:  accessToken,
		RefreshToken: refreshToken,
		TokenType:    "Bearer",
		ExpiresIn:    3600, // 1 hour
		User: &UserInfo{
			ID:            newUser.ID,
			Email:         newUser.Email,
			ClientID:      newUser.ClientID,
			Role:          newUser.Role,
			Tier:          newUser.SubscriptionTier,
			EmailVerified: newUser.EmailVerified,
			Permissions:   newUser.Permissions,
		},
	}, nil
}

// Login handles user login
func (s *Service) Login(ctx context.Context, email, password string) (*TokenResponse, error) {
	// Validate credentials
	user, err := s.userService.Login(ctx, email, password)
	if err != nil {
		return nil, err
	}

	// Generate tokens
	accessToken, refreshToken, err := s.jwtService.GenerateTokens(
		user.ID,
		user.Email,
		user.ClientID,
		user.Role,
		user.SubscriptionTier,
		user.Permissions,
	)
	if err != nil {
		s.logger.Error("Failed to generate tokens", zap.Error(err))
		return nil, fmt.Errorf("failed to generate tokens")
	}

	return &TokenResponse{
		AccessToken:  accessToken,
		RefreshToken: refreshToken,
		TokenType:    "Bearer",
		ExpiresIn:    3600,
		User: &UserInfo{
			ID:            user.ID,
			Email:         user.Email,
			ClientID:      user.ClientID,
			Role:          user.Role,
			Tier:          user.SubscriptionTier,
			EmailVerified: user.EmailVerified,
			Permissions:   user.Permissions,
		},
	}, nil
}

// RefreshToken handles token refresh
func (s *Service) RefreshToken(ctx context.Context, refreshToken string) (*TokenResponse, error) {
	// Validate refresh token and get new access token
	getUserFunc := func(userID string) (*jwt.UserInfo, error) {
		return s.userService.GetUserInfo(ctx, userID)
	}

	accessToken, err := s.jwtService.RefreshAccessToken(refreshToken, getUserFunc)
	if err != nil {
		return nil, fmt.Errorf("invalid refresh token")
	}

	// Get user info for response
	claims, _ := s.jwtService.ValidateToken(accessToken)

	return &TokenResponse{
		AccessToken:  accessToken,
		RefreshToken: refreshToken, // Return same refresh token
		TokenType:    "Bearer",
		ExpiresIn:    3600,
		User: &UserInfo{
			ID:          claims.UserID,
			Email:       claims.Email,
			ClientID:    claims.ClientID,
			Role:        claims.Role,
			Tier:        claims.Tier,
			Permissions: claims.Permissions,
		},
	}, nil
}

// Logout handles user logout (for future implementation with token blacklisting)
func (s *Service) Logout(ctx context.Context, tokenID string) error {
	// In a stateless JWT system, logout is typically handled client-side
	// For added security, you could implement token blacklisting here
	s.logger.Info("User logged out", zap.String("token_id", tokenID))
	return nil
}

// ValidateToken validates an access token
func (s *Service) ValidateToken(tokenString string) (*jwt.Claims, error) {
	return s.jwtService.ValidateToken(tokenString)
}
-------------------------------------------------
filepath = ./internal/auth-service/auth/interfaces.go
// FILE: internal/auth-service/auth/interfaces.go
package auth

import (
	"context"
	"github.com/gqls/agentchassis/internal/auth-service/jwt"
	"github.com/gqls/agentchassis/internal/auth-service/user"
)

// ServiceInterface defines the methods that the auth service must implement
type ServiceInterface interface {
	Register(ctx context.Context, req *user.CreateUserRequest) (*TokenResponse, error)
	Login(ctx context.Context, email, password string) (*TokenResponse, error)
	RefreshToken(ctx context.Context, refreshToken string) (*TokenResponse, error)
	Logout(ctx context.Context, tokenID string) error
	ValidateToken(tokenString string) (*jwt.Claims, error)
}
-------------------------------------------------
filepath = ./internal/auth-service/subscription/handlers.go
// FILE: internal/auth-service/subscription/handlers.go
package subscription

import (
	"go.uber.org/zap"
	"net/http"
	"strconv"

	"github.com/gin-gonic/gin"
)

// Handlers wraps the subscription service for HTTP handling
type Handlers struct {
	service *Service
}

// NewHandlers creates new subscription handlers
func NewHandlers(service *Service) *Handlers {
	return &Handlers{service: service}
}

// HandleGetSubscription returns the current user's subscription
func (h *Handlers) HandleGetSubscription(c *gin.Context) {
	userID := c.GetString("user_id")

	subscription, err := h.service.GetSubscription(c.Request.Context(), userID)
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": "Subscription not found"})
		return
	}

	c.JSON(http.StatusOK, subscription)
}

// HandleGetUsageStats returns usage statistics
func (h *Handlers) HandleGetUsageStats(c *gin.Context) {
	userID := c.GetString("user_id")

	stats, err := h.service.GetUsageStats(c.Request.Context(), userID)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to get usage stats"})
		return
	}

	c.JSON(http.StatusOK, stats)
}

// HandleCheckQuota checks if user has quota for a resource
func (h *Handlers) HandleCheckQuota(c *gin.Context) {
	userID := c.GetString("user_id")
	resource := c.Query("resource")

	if resource == "" {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Resource parameter required"})
		return
	}

	hasQuota, err := h.service.CheckQuota(c.Request.Context(), userID, resource)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"has_quota": hasQuota,
		"resource":  resource,
	})
}

// AdminHandlers for admin operations
type AdminHandlers struct {
	service *Service
	logger  *zap.Logger
}

// NewAdminHandlers creates admin handlers
func NewAdminHandlers(service *Service, logger *zap.Logger) *AdminHandlers {
	return &AdminHandlers{service: service, logger: logger}
}

// HandleCreateSubscription creates a subscription (admin only)
func (h *AdminHandlers) HandleCreateSubscription(c *gin.Context) {
	var req CreateSubscriptionRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	subscription, err := h.service.CreateSubscription(c.Request.Context(), &req)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to create subscription"})
		return
	}

	c.JSON(http.StatusCreated, subscription)
}

// HandleUpdateSubscription updates a subscription (admin only)
func (h *AdminHandlers) HandleUpdateSubscription(c *gin.Context) {
	userID := c.Param("user_id")

	var req UpdateSubscriptionRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	subscription, err := h.service.UpdateSubscription(c.Request.Context(), userID, &req)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to update subscription"})
		return
	}

	c.JSON(http.StatusOK, subscription)
}

// HandleListSubscriptions lists all subscriptions with filtering (admin only)
func (h *AdminHandlers) HandleListSubscriptions(c *gin.Context) {
	limit, _ := strconv.Atoi(c.DefaultQuery("limit", "50"))
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	if limit > 200 {
		limit = 200
	}
	if page < 1 {
		page = 1
	}

	params := ListSubscriptionsParams{
		Limit:  limit,
		Offset: (page - 1) * limit,
		Status: c.Query("status"),
		Tier:   c.Query("tier"),
	}

	subscriptions, total, err := h.service.repo.ListAll(c.Request.Context(), params)
	if err != nil {
		h.logger.Error("Failed to list subscriptions", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to retrieve subscriptions"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"subscriptions": subscriptions,
		"total_count":   total,
		"page":          page,
		"limit":         limit,
	})
}
-------------------------------------------------
filepath = ./internal/auth-service/subscription/repository.go
// FILE: internal/auth-service/subscription/repository.go
package subscription

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"time"

	"go.uber.org/zap"
)

// Repository handles subscription data access
type Repository struct {
	db     *sql.DB
	logger *zap.Logger
}

// NewRepository creates a new subscription repository
func NewRepository(db *sql.DB, logger *zap.Logger) *Repository {
	return &Repository{
		db:     db,
		logger: logger,
	}
}

// GetByUserID retrieves a subscription by user ID
func (r *Repository) GetByUserID(ctx context.Context, userID string) (*Subscription, error) {
	var s Subscription
	query := `
		SELECT id, user_id, tier, status, start_date, end_date, trial_ends_at, 
		       cancelled_at, payment_method, stripe_customer_id, stripe_subscription_id,
		       created_at, updated_at
		FROM subscriptions
		WHERE user_id = ?
	`

	err := r.db.QueryRowContext(ctx, query, userID).Scan(
		&s.ID, &s.UserID, &s.Tier, &s.Status, &s.StartDate, &s.EndDate,
		&s.TrialEndsAt, &s.CancelledAt, &s.PaymentMethod,
		&s.StripeCustomerID, &s.StripeSubscriptionID,
		&s.CreatedAt, &s.UpdatedAt,
	)

	if err != nil {
		if err == sql.ErrNoRows {
			return nil, fmt.Errorf("subscription not found")
		}
		return nil, err
	}

	return &s, nil
}

// Create creates a new subscription
func (r *Repository) Create(ctx context.Context, s *Subscription) error {
	query := `
		INSERT INTO subscriptions (id, user_id, tier, status, start_date, payment_method,
		                          trial_ends_at, created_at, updated_at)
		VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
	`

	_, err := r.db.ExecContext(ctx, query,
		s.ID, s.UserID, s.Tier, s.Status, s.StartDate,
		s.PaymentMethod, s.TrialEndsAt, s.CreatedAt, s.UpdatedAt,
	)

	return err
}

// Update updates an existing subscription
func (r *Repository) Update(ctx context.Context, s *Subscription) error {
	query := `
		UPDATE subscriptions 
		SET tier = ?, status = ?, payment_method = ?, updated_at = ?
		WHERE id = ?
	`

	_, err := r.db.ExecContext(ctx, query,
		s.Tier, s.Status, s.PaymentMethod, s.UpdatedAt, s.ID,
	)

	return err
}

// Cancel cancels a subscription
func (r *Repository) Cancel(ctx context.Context, userID string, cancelledAt time.Time) error {
	query := `
		UPDATE subscriptions 
		SET status = ?, cancelled_at = ?, updated_at = ?
		WHERE user_id = ?
	`

	_, err := r.db.ExecContext(ctx, query,
		StatusCanceled, cancelledAt, time.Now(), userID,
	)

	return err
}

// GetTier retrieves tier information
func (r *Repository) GetTier(ctx context.Context, tierName string) (*SubscriptionTier, error) {
	var t SubscriptionTier
	var featuresJSON string

	query := `
		SELECT id, name, display_name, description, price_monthly, price_yearly,
		       max_personas, max_projects, max_content_items, features, is_active
		FROM subscription_tiers
		WHERE name = ? AND is_active = true
	`

	err := r.db.QueryRowContext(ctx, query, tierName).Scan(
		&t.ID, &t.Name, &t.DisplayName, &t.Description,
		&t.PriceMonthly, &t.PriceYearly,
		&t.MaxPersonas, &t.MaxProjects, &t.MaxContentItems,
		&featuresJSON, &t.IsActive,
	)

	if err != nil {
		return nil, err
	}

	json.Unmarshal([]byte(featuresJSON), &t.Features)

	return &t, nil
}

// GetUsageStats retrieves usage statistics
func (r *Repository) GetUsageStats(ctx context.Context, userID string) (*UsageStats, error) {
	var stats UsageStats
	stats.UserID = userID

	// This would need to query across multiple tables/schemas
	// For now, returning mock data
	stats.PersonasCount = 0
	stats.ProjectsCount = 0
	stats.ContentCount = 0
	stats.LastUpdated = time.Now()

	return &stats, nil
}

// ListSubscriptionsParams contains parameters for listing subscriptions
type ListSubscriptionsParams struct {
	Limit  int
	Offset int
	Status string
	Tier   string
}

// ListAll retrieves a paginated list of all subscriptions
func (r *Repository) ListAll(ctx context.Context, params ListSubscriptionsParams) ([]Subscription, int, error) {
	query := `SELECT id, user_id, tier, status, start_date, end_date, created_at FROM subscriptions WHERE 1=1`
	countQuery := `SELECT COUNT(*) FROM subscriptions WHERE 1=1`

	args := []interface{}{}
	count := 1

	if params.Status != "" {
		query += fmt.Sprintf(" AND status = $%d", count)
		countQuery += fmt.Sprintf(" AND status = $%d", count)
		args = append(args, params.Status)
		count++
	}
	if params.Tier != "" {
		query += fmt.Sprintf(" AND tier = $%d", count)
		countQuery += fmt.Sprintf(" AND tier = $%d", count)
		args = append(args, params.Tier)
		count++
	}

	query += fmt.Sprintf(" ORDER BY created_at DESC LIMIT $%d OFFSET $%d", count, count+1)
	args = append(args, params.Limit, params.Offset)

	// Get total count
	var total int
	if err := r.db.QueryRowContext(ctx, countQuery, args[:count-1]...).Scan(&total); err != nil {
		return nil, 0, fmt.Errorf("failed to get subscription count: %w", err)
	}

	// Get subscriptions
	rows, err := r.db.QueryContext(ctx, query, args...)
	if err != nil {
		return nil, 0, fmt.Errorf("failed to list subscriptions: %w", err)
	}
	defer rows.Close()

	var subscriptions []Subscription
	for rows.Next() {
		var s Subscription
		if err := rows.Scan(&s.ID, &s.UserID, &s.Tier, &s.Status, &s.StartDate, &s.EndDate, &s.CreatedAt); err != nil {
			r.logger.Error("Failed to scan subscription row", zap.Error(err))
			continue
		}
		subscriptions = append(subscriptions, s)
	}

	return subscriptions, total, nil
}
-------------------------------------------------
filepath = ./internal/auth-service/subscription/models.go
package subscription

import (
	"time"
)

// Subscription represents a user's subscription
type Subscription struct {
	ID                   string     `json:"id" db:"id"`
	UserID               string     `json:"user_id" db:"user_id"`
	Tier                 string     `json:"tier" db:"tier"`
	Status               string     `json:"status" db:"status"`
	StartDate            time.Time  `json:"start_date" db:"start_date"`
	EndDate              *time.Time `json:"end_date,omitempty" db:"end_date"`
	TrialEndsAt          *time.Time `json:"trial_ends_at,omitempty" db:"trial_ends_at"`
	CancelledAt          *time.Time `json:"cancelled_at,omitempty" db:"cancelled_at"`
	PaymentMethod        string     `json:"payment_method" db:"payment_method"`
	StripeCustomerID     string     `json:"-" db:"stripe_customer_id"`
	StripeSubscriptionID string     `json:"-" db:"stripe_subscription_id"`
	CreatedAt            time.Time  `json:"created_at" db:"created_at"`
	UpdatedAt            time.Time  `json:"updated_at" db:"updated_at"`
}

// SubscriptionTier defines tier details
type SubscriptionTier struct {
	ID              string   `json:"id" db:"id"`
	Name            string   `json:"name" db:"name"`
	DisplayName     string   `json:"display_name" db:"display_name"`
	Description     string   `json:"description" db:"description"`
	PriceMonthly    float64  `json:"price_monthly" db:"price_monthly"`
	PriceYearly     float64  `json:"price_yearly" db:"price_yearly"`
	MaxPersonas     int      `json:"max_personas" db:"max_personas"`
	MaxProjects     int      `json:"max_projects" db:"max_projects"`
	MaxContentItems int      `json:"max_content_items" db:"max_content_items"`
	Features        []string `json:"features" db:"features"`
	IsActive        bool     `json:"is_active" db:"is_active"`
}

// UsageStats tracks user's resource usage
type UsageStats struct {
	UserID        string    `json:"user_id" db:"user_id"`
	PersonasCount int       `json:"personas_count" db:"personas_count"`
	ProjectsCount int       `json:"projects_count" db:"projects_count"`
	ContentCount  int       `json:"content_count" db:"content_count"`
	LastUpdated   time.Time `json:"last_updated" db:"last_updated"`
}

// SubscriptionStatus constants
const (
	StatusActive   = "active"
	StatusTrialing = "trialing"
	StatusPastDue  = "past_due"
	StatusCanceled = "canceled"
	StatusExpired  = "expired"
)

// Tier constants
const (
	TierFree       = "free"
	TierBasic      = "basic"
	TierPremium    = "premium"
	TierEnterprise = "enterprise"
)

// CreateSubscriptionRequest for new subscriptions
type CreateSubscriptionRequest struct {
	UserID          string `json:"user_id" binding:"required"`
	Tier            string `json:"tier" binding:"required"`
	PaymentMethodID string `json:"payment_method_id"`
	TrialDays       int    `json:"trial_days"`
}

// UpdateSubscriptionRequest for subscription changes
type UpdateSubscriptionRequest struct {
	Tier            *string `json:"tier"`
	PaymentMethodID *string `json:"payment_method_id"`
}

// CheckoutSession for payment processing
type CheckoutSession struct {
	ID         string `json:"id"`
	URL        string `json:"url"`
	SuccessURL string `json:"success_url"`
	CancelURL  string `json:"cancel_url"`
	ExpiresAt  int64  `json:"expires_at"`
}
-------------------------------------------------
filepath = ./internal/auth-service/subscription/service.go
// FILE: internal/auth-service/subscription/service.go
package subscription

import (
	"context"
	"fmt"
	"time"

	"github.com/google/uuid"
	"go.uber.org/zap"
)

// Service handles subscription business logic
type Service struct {
	repo   *Repository
	logger *zap.Logger
}

// NewService creates a new subscription service
func NewService(repo *Repository, logger *zap.Logger) *Service {
	return &Service{
		repo:   repo,
		logger: logger,
	}
}

// GetSubscription retrieves a user's subscription
func (s *Service) GetSubscription(ctx context.Context, userID string) (*Subscription, error) {
	return s.repo.GetByUserID(ctx, userID)
}

// CreateSubscription creates a new subscription
func (s *Service) CreateSubscription(ctx context.Context, req *CreateSubscriptionRequest) (*Subscription, error) {
	subscription := &Subscription{
		ID:            uuid.New().String(),
		UserID:        req.UserID,
		Tier:          req.Tier,
		Status:        StatusActive,
		StartDate:     time.Now(),
		PaymentMethod: req.PaymentMethodID,
		CreatedAt:     time.Now(),
		UpdatedAt:     time.Now(),
	}

	if req.TrialDays > 0 {
		trialEnd := time.Now().AddDate(0, 0, req.TrialDays)
		subscription.TrialEndsAt = &trialEnd
		subscription.Status = StatusTrialing
	}

	if err := s.repo.Create(ctx, subscription); err != nil {
		return nil, fmt.Errorf("failed to create subscription: %w", err)
	}

	return subscription, nil
}

// UpdateSubscription updates an existing subscription
func (s *Service) UpdateSubscription(ctx context.Context, userID string, req *UpdateSubscriptionRequest) (*Subscription, error) {
	subscription, err := s.repo.GetByUserID(ctx, userID)
	if err != nil {
		return nil, err
	}

	if req.Tier != nil {
		subscription.Tier = *req.Tier
	}

	if req.PaymentMethodID != nil {
		subscription.PaymentMethod = *req.PaymentMethodID
	}

	subscription.UpdatedAt = time.Now()

	if err := s.repo.Update(ctx, subscription); err != nil {
		return nil, fmt.Errorf("failed to update subscription: %w", err)
	}

	return subscription, nil
}

// CancelSubscription cancels a subscription
func (s *Service) CancelSubscription(ctx context.Context, userID string) error {
	now := time.Now()
	return s.repo.Cancel(ctx, userID, now)
}

// GetUsageStats retrieves usage statistics for a user
func (s *Service) GetUsageStats(ctx context.Context, userID string) (*UsageStats, error) {
	return s.repo.GetUsageStats(ctx, userID)
}

// CheckQuota checks if a user has quota for a specific resource
func (s *Service) CheckQuota(ctx context.Context, userID string, resource string) (bool, error) {
	subscription, err := s.GetSubscription(ctx, userID)
	if err != nil {
		return false, err
	}

	tier, err := s.repo.GetTier(ctx, subscription.Tier)
	if err != nil {
		return false, err
	}

	usage, err := s.GetUsageStats(ctx, userID)
	if err != nil {
		return false, err
	}

	switch resource {
	case "personas":
		return tier.MaxPersonas == -1 || usage.PersonasCount < tier.MaxPersonas, nil
	case "projects":
		return tier.MaxProjects == -1 || usage.ProjectsCount < tier.MaxProjects, nil
	case "content":
		return tier.MaxContentItems == -1 || usage.ContentCount < tier.MaxContentItems, nil
	default:
		return false, fmt.Errorf("unknown resource type: %s", resource)
	}
}
-------------------------------------------------
filepath = ./internal/agents/reasoning/agent.go
// FILE: internal/agents/reasoning/agent.go
package reasoning

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/platform/aiservice"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/kafka"
	"go.uber.org/zap"
)

const (
	requestTopic  = "system.agent.reasoning.process"
	responseTopic = "system.responses.reasoning"
	consumerGroup = "reasoning-agent-group"
)

// RequestPayload defines the data this agent expects
type RequestPayload struct {
	Action string `json:"action"`
	Data   struct {
		ContentToReview string                 `json:"content_to_review"`
		ReviewCriteria  []string               `json:"review_criteria"`
		BriefContext    map[string]interface{} `json:"brief_context"`
	} `json:"data"`
}

// ResponsePayload defines the response format
type ResponsePayload struct {
	ReviewPassed bool     `json:"review_passed"`
	Score        float64  `json:"score"`
	Suggestions  []string `json:"suggestions"`
	Reasoning    string   `json:"reasoning"`
}

// Agent is the reasoning specialist
type Agent struct {
	ctx      context.Context
	logger   *zap.Logger
	consumer *kafka.Consumer
	producer kafka.Producer
	aiClient aiservice.AIService
}

// NewAgent creates a new reasoning agent
func NewAgent(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger) (*Agent, error) {
	consumer, err := kafka.NewConsumer(cfg.Infrastructure.KafkaBrokers, requestTopic, consumerGroup, logger)
	if err != nil {
		return nil, fmt.Errorf("failed to create consumer: %w", err)
	}

	producer, err := kafka.NewProducer(cfg.Infrastructure.KafkaBrokers, logger)
	if err != nil {
		consumer.Close()
		return nil, fmt.Errorf("failed to create producer: %w", err)
	}

	// Initialize AI client from custom config
	aiConfig := cfg.Custom["ai_service"].(map[string]interface{})
	aiClient, err := aiservice.NewAnthropicClient(ctx, aiConfig)
	if err != nil {
		consumer.Close()
		producer.Close()
		return nil, fmt.Errorf("failed to create AI client: %w", err)
	}

	return &Agent{
		ctx:      ctx,
		logger:   logger,
		consumer: consumer,
		producer: producer,
		aiClient: aiClient,
	}, nil
}

// Run starts the agent's main loop
func (a *Agent) Run() error {
	a.logger.Info("Reasoning Agent is running and waiting for tasks...")

	for {
		select {
		case <-a.ctx.Done():
			a.consumer.Close()
			a.producer.Close()
			return nil
		default:
			msg, err := a.consumer.FetchMessage(a.ctx)
			if err != nil {
				if err == context.Canceled {
					continue
				}
				a.logger.Error("Failed to fetch message", zap.Error(err))
				continue
			}
			go a.handleMessage(msg)
		}
	}
}

// handleMessage processes a single reasoning request
func (a *Agent) handleMessage(msg kafka.Message) {
	headers := kafka.HeadersToMap(msg.Headers)
	l := a.logger.With(zap.String("correlation_id", headers["correlation_id"]))

	var req RequestPayload
	if err := json.Unmarshal(msg.Value, &req); err != nil {
		l.Error("Failed to unmarshal request", zap.Error(err))
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}

	// Build the reasoning prompt
	prompt := a.buildReasoningPrompt(req)

	// Call the AI service
	result, err := a.aiClient.GenerateText(a.ctx, prompt, nil)
	if err != nil {
		l.Error("AI reasoning call failed", zap.Error(err))
		a.sendErrorResponse(headers, "Failed to perform reasoning")
		a.consumer.CommitMessages(context.Background(), msg)
		return
	}

	// Parse the AI response
	var responsePayload ResponsePayload
	if err := json.Unmarshal([]byte(result), &responsePayload); err != nil {
		l.Error("Failed to parse AI response", zap.Error(err))
		// Fallback response
		responsePayload = ResponsePayload{
			ReviewPassed: false,
			Score:        0,
			Suggestions:  []string{"Could not parse AI response"},
			Reasoning:    result,
		}
	}

	// Send response
	a.sendResponse(headers, responsePayload)

	// Commit message
	a.consumer.CommitMessages(context.Background(), msg)
}

// buildReasoningPrompt creates the prompt for the LLM
func (a *Agent) buildReasoningPrompt(req RequestPayload) string {
	return fmt.Sprintf(`You are a logical reasoning engine. Review the following content based on these criteria: %v.

Context: %v

Content to review: "%s"

Provide your analysis as a JSON object with the following structure:
{
    "review_passed": boolean,
    "score": number (0-10),
    "suggestions": ["suggestion1", "suggestion2", ...],
    "reasoning": "detailed explanation of your analysis"
}

Be thorough but concise in your reasoning.`,
		req.Data.ReviewCriteria,
		req.Data.BriefContext,
		req.Data.ContentToReview,
	)
}

// sendResponse sends a successful response
func (a *Agent) sendResponse(headers map[string]string, payload ResponsePayload) {
	responseBytes, _ := json.Marshal(payload)
	responseHeaders := map[string]string{
		"correlation_id": headers["correlation_id"],
		"causation_id":   headers["request_id"],
		"request_id":     uuid.NewString(),
	}

	if err := a.producer.Produce(a.ctx, responseTopic, responseHeaders,
		[]byte(headers["correlation_id"]), responseBytes); err != nil {
		a.logger.Error("Failed to produce response", zap.Error(err))
	}
}

// sendErrorResponse sends an error response
func (a *Agent) sendErrorResponse(headers map[string]string, errorMsg string) {
	payload := map[string]interface{}{
		"success": false,
		"error":   errorMsg,
	}
	responseBytes, _ := json.Marshal(payload)
	responseHeaders := map[string]string{
		"correlation_id": headers["correlation_id"],
		"causation_id":   headers["request_id"],
		"request_id":     uuid.NewString(),
	}

	a.producer.Produce(a.ctx, responseTopic, responseHeaders,
		[]byte(headers["correlation_id"]), responseBytes)
}

// StartHealthServer starts a simple HTTP server for health checks
func (a *Agent) StartHealthServer(port string) {
	http.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		w.WriteHeader(http.StatusOK)
		json.NewEncoder(w).Encode(map[string]string{
			"status": "healthy",
			"agent":  "reasoning-agent",
		})
	})

	go func() {
		a.logger.Info("Starting health server", zap.String("port", port))
		if err := http.ListenAndServe(":"+port, nil); err != nil {
			a.logger.Error("Health server failed", zap.Error(err))
		}
	}()
}
-------------------------------------------------
filepath = ./internal/core-manager/api/server.go
// FILE: internal/core-manager/api/server.go
package api

import (
	"context"
	"fmt"
	"net/http"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
	"github.com/gqls/agentchassis/internal/core-manager/admin" // Import the new admin package
	"github.com/gqls/agentchassis/internal/core-manager/database"
	"github.com/gqls/agentchassis/internal/core-manager/middleware"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/kafka" // Import kafka for admin handlers
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// Server represents the Core Manager API server
type Server struct {
	ctx           context.Context
	cfg           *config.ServiceConfig
	logger        *zap.Logger
	router        *gin.Engine
	httpServer    *http.Server
	personaRepo   models.PersonaRepository
	kafkaProducer kafka.Producer // Add kafka producer
}

// NewServer function updated to include kafkaProducer
func NewServer(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger, templatesDB, clientsDB *pgxpool.Pool) (*Server, error) {
	// Initialize repositories
	personaRepo := database.NewPersonaRepository(templatesDB, clientsDB, logger)

	// Create Gin router
	router := gin.New()
	router.Use(gin.Recovery())

	// Initialize Kafka Producer for admin handlers
	kafkaProducer, err := kafka.NewProducer(cfg.Infrastructure.KafkaBrokers, logger)
	if err != nil {
		return nil, fmt.Errorf("failed to create kafka producer for admin handlers: %w", err)
	}

	// Initialize auth middleware config
	authConfig, err := middleware.NewAuthMiddlewareConfig(cfg, logger)
	if err != nil {
		return nil, fmt.Errorf("failed to initialize auth middleware: %w", err)
	}

	server := &Server{
		ctx:           ctx,
		cfg:           cfg,
		logger:        logger,
		router:        router,
		personaRepo:   personaRepo,
		kafkaProducer: kafkaProducer,
	}

	// Setup routes with configured auth middleware
	server.setupRoutesWithAuth(router, authConfig)

	// Create HTTP server
	server.httpServer = &http.Server{
		Addr:    ":" + cfg.Server.Port,
		Handler: router,
	}

	return server, nil
}

// setupRoutesWithAuth configures all API routes with auth config
func (s *Server) setupRoutesWithAuth(router *gin.Engine, authConfig *middleware.AuthMiddlewareConfig) {
	// Health check (no auth)
	router.GET("/health", s.handleHealth)

	// API v1 group with authentication
	apiV1 := router.Group("/api/v1")
	apiV1.Use(middleware.AuthMiddleware(authConfig))
	{
		// Template Management (Admin Only)
		templates := apiV1.Group("/templates")
		templates.Use(middleware.AdminOnly())
		{
			templates.POST("", s.handleCreateTemplate)
			templates.GET("", s.handleListTemplates)
			templates.GET("/:id", s.handleGetTemplate)
			templates.PUT("/:id", s.handleUpdateTemplate)
			templates.DELETE("/:id", s.handleDeleteTemplate)
		}

		// Persona Instance Management (Tenant-scoped)
		instances := apiV1.Group("/personas/instances")
		instances.Use(middleware.TenantMiddleware(s.logger))
		{
			instances.POST("", s.handleCreateInstance)
			instances.GET("", s.handleListInstances)
			instances.GET("/:id", s.handleGetInstance)
			instances.PATCH("/:id", s.handleUpdateInstance)
			instances.DELETE("/:id", s.handleDeleteInstance)
		}

		// Admin Management (Admin Only)
		adminGroup := apiV1.Group("/admin")
		adminGroup.Use(middleware.AdminOnly())
		{
			// Initialize admin handlers
			clientHandlers := admin.NewClientHandlers(s.personaRepo.(*database.PersonaRepository).ClientsDB(), s.logger)
			systemHandlers := admin.NewSystemHandlers(s.personaRepo.(*database.PersonaRepository).ClientsDB(), s.personaRepo.(*database.PersonaRepository).TemplatesDB(), s.kafkaProducer, s.logger)

			// Initialize AgentHandlers with the personaRepo dependency
			agentAdminHandlers := admin.NewAgentHandlers(s.personaRepo.(*database.PersonaRepository).ClientsDB(), s.personaRepo.(*database.PersonaRepository).TemplatesDB(), s.kafkaProducer, s.logger, s.personaRepo)

			// Client Management
			adminGroup.POST("/clients", clientHandlers.HandleCreateClient)
			adminGroup.GET("/clients", clientHandlers.HandleListClients)
			adminGroup.GET("/clients/:client_id/usage", clientHandlers.HandleGetClientUsage)

			// System & Workflow Management
			adminGroup.GET("/system/status", systemHandlers.HandleGetSystemStatus)
			adminGroup.GET("/system/kafka/topics", systemHandlers.HandleListKafkaTopics)
			adminGroup.GET("/workflows", systemHandlers.HandleListWorkflows)
			adminGroup.GET("/workflows/:correlation_id", systemHandlers.HandleGetWorkflow)
			adminGroup.POST("/workflows/:correlation_id/resume", systemHandlers.HandleResumeWorkflow)

			// Agent Definition Management
			adminGroup.GET("/agent-definitions", systemHandlers.HandleListAgentDefinitions)
			adminGroup.PUT("/agent-definitions/:type_name", systemHandlers.HandleUpdateAgentDefinition)

			// Agent Instance Management
			adminGroup.PUT("/clients/:client_id/instances/:instance_id/config", agentAdminHandlers.HandleUpdateInstanceConfig)

		}
	}
}

// Start starts the HTTP server
func (s *Server) Start() error {
	s.logger.Info("Starting Core Manager API server", zap.String("address", s.httpServer.Addr))
	return s.httpServer.ListenAndServe()
}

// Shutdown gracefully shuts down the server
func (s *Server) Shutdown(ctx context.Context) error {
	s.kafkaProducer.Close() // Close the producer on shutdown
	return s.httpServer.Shutdown(ctx)
}

// Address returns the server's address
func (s *Server) Address() string {
	return s.httpServer.Addr
}

// Health check handler
func (s *Server) handleHealth(c *gin.Context) {
	c.JSON(http.StatusOK, gin.H{
		"status":  "healthy",
		"service": s.cfg.ServiceInfo.Name,
		"version": s.cfg.ServiceInfo.Version,
	})
}

// Template Handlers

func (s *Server) handleCreateTemplate(c *gin.Context) {
	var req models.Persona
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request body: " + err.Error()})
		return
	}

	req.ID = uuid.New()
	req.IsTemplate = true
	req.CreatedAt = time.Now()
	req.UpdatedAt = time.Now()

	createdTemplate, err := s.personaRepo.CreateTemplate(c.Request.Context(), &req)
	if err != nil {
		s.logger.Error("Failed to create template", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to create template"})
		return
	}

	c.JSON(http.StatusCreated, createdTemplate)
}

func (s *Server) handleListTemplates(c *gin.Context) {
	templates, err := s.personaRepo.ListTemplates(c.Request.Context())
	if err != nil {
		s.logger.Error("Failed to list templates", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to retrieve templates"})
		return
	}
	c.JSON(http.StatusOK, gin.H{"templates": templates})
}

func (s *Server) handleGetTemplate(c *gin.Context) {
	templateID := c.Param("id")
	template, err := s.personaRepo.GetTemplateByID(c.Request.Context(), templateID)
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": "Template not found"})
		return
	}
	c.JSON(http.StatusOK, template)
}

func (s *Server) handleUpdateTemplate(c *gin.Context) {
	templateID := c.Param("id")
	var req models.Persona
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request body"})
		return
	}

	req.ID, _ = uuid.Parse(templateID)
	req.UpdatedAt = time.Now()

	updatedTemplate, err := s.personaRepo.UpdateTemplate(c.Request.Context(), &req)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to update template"})
		return
	}
	c.JSON(http.StatusOK, updatedTemplate)
}

func (s *Server) handleDeleteTemplate(c *gin.Context) {
	templateID := c.Param("id")
	if err := s.personaRepo.DeleteTemplate(c.Request.Context(), templateID); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to delete template"})
		return
	}
	c.Status(http.StatusNoContent)
}

// Instance Handlers

// handleCreateInstance with proper context
func (s *Server) handleCreateInstance(c *gin.Context) {
	claims := c.MustGet("user_claims").(*middleware.AuthClaims)

	var req struct {
		TemplateID   string `json:"template_id" binding:"required,uuid"`
		InstanceName string `json:"instance_name" binding:"required"`
	}
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request: " + err.Error()})
		return
	}

	instance, err := s.personaRepo.CreateInstanceFromTemplate(c.Request.Context(),
		req.TemplateID, claims.UserID, req.InstanceName)
	if err != nil {
		s.logger.Error("Failed to create instance", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Could not create instance"})
		return
	}
	c.JSON(http.StatusCreated, instance)
}

func (s *Server) handleGetInstance(c *gin.Context) {
	instanceID := c.Param("id")
	instance, err := s.personaRepo.GetInstanceByID(c.Request.Context(), instanceID)
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": "Instance not found"})
		return
	}
	c.JSON(http.StatusOK, instance)
}

func (s *Server) handleListInstances(c *gin.Context) {
	claims := c.MustGet("user_claims").(*middleware.AuthClaims)
	instances, err := s.personaRepo.ListInstances(c.Request.Context(), claims.UserID)
	if err != nil {
		s.logger.Error("Failed to list instances", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Could not retrieve instances"})
		return
	}
	c.JSON(http.StatusOK, gin.H{"instances": instances})
}

func (s *Server) handleUpdateInstance(c *gin.Context) {
	instanceID := c.Param("id")
	var req struct {
		Name   *string                `json:"name"`
		Config map[string]interface{} `json:"config"`
	}
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request body"})
		return
	}

	updatedInstance, err := s.personaRepo.UpdateInstance(c.Request.Context(), instanceID, req.Name, req.Config)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to update instance"})
		return
	}
	c.JSON(http.StatusOK, updatedInstance)
}

func (s *Server) handleDeleteInstance(c *gin.Context) {
	instanceID := c.Param("id")
	if err := s.personaRepo.DeleteInstance(c.Request.Context(), instanceID); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to delete instance"})
		return
	}
	c.Status(http.StatusNoContent)
}
-------------------------------------------------
filepath = ./internal/core-manager/admin/client_handlers.go
// FILE: internal/core-manager/admin/client_handlers.go
package admin

import (
	"context"
	"database/sql"
	"fmt"
	"net/http"
	"strings"

	"github.com/gin-gonic/gin"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// ClientHandlers handles admin operations for client management
type ClientHandlers struct {
	clientsDB *pgxpool.Pool
	logger    *zap.Logger
}

// NewClientHandlers creates new client admin handlers
func NewClientHandlers(clientsDB *pgxpool.Pool, logger *zap.Logger) *ClientHandlers {
	return &ClientHandlers{
		clientsDB: clientsDB,
		logger:    logger,
	}
}

// CreateClientRequest represents a request to create a new client
type CreateClientRequest struct {
	ClientID    string                 `json:"client_id" binding:"required,alphanum,min=3,max=50"`
	DisplayName string                 `json:"display_name" binding:"required"`
	Settings    map[string]interface{} `json:"settings,omitempty"`
}

// ClientInfo represents client information
type ClientInfo struct {
	ClientID    string                 `json:"client_id"`
	DisplayName string                 `json:"display_name"`
	Settings    map[string]interface{} `json:"settings,omitempty"`
	CreatedAt   string                 `json:"created_at"`
	IsActive    bool                   `json:"is_active"`
}

// HandleCreateClient creates a new client with schema
func (h *ClientHandlers) HandleCreateClient(c *gin.Context) {
	var req CreateClientRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Validate client_id format (alphanumeric, underscores allowed)
	if !isValidClientID(req.ClientID) {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid client_id format. Use only alphanumeric characters and underscores"})
		return
	}

	// Check if client already exists
	exists, err := h.clientExists(c.Request.Context(), req.ClientID)
	if err != nil {
		h.logger.Error("Failed to check client existence", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to check client existence"})
		return
	}

	if exists {
		c.JSON(http.StatusConflict, gin.H{"error": "Client already exists"})
		return
	}

	// Create client schema
	if err := h.createClientSchema(c.Request.Context(), req.ClientID); err != nil {
		h.logger.Error("Failed to create client schema", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to create client schema"})
		return
	}

	// Store client info in a clients table (we'll need to create this)
	if err := h.storeClientInfo(c.Request.Context(), &req); err != nil {
		h.logger.Error("Failed to store client info", zap.Error(err))
		// Note: Schema is already created, this is a partial failure
	}

	h.logger.Info("Client created successfully", zap.String("client_id", req.ClientID))
	c.JSON(http.StatusCreated, gin.H{
		"message":   "Client created successfully",
		"client_id": req.ClientID,
	})
}

// HandleListClients lists all clients
func (h *ClientHandlers) HandleListClients(c *gin.Context) {
	clients, err := h.listClients(c.Request.Context())
	if err != nil {
		h.logger.Error("Failed to list clients", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to list clients"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"clients": clients,
		"count":   len(clients),
	})
}

// HandleGetClientUsage gets usage statistics for a client
func (h *ClientHandlers) HandleGetClientUsage(c *gin.Context) {
	clientID := c.Param("client_id")

	if !isValidClientID(clientID) {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid client_id format"})
		return
	}

	usage, err := h.getClientUsage(c.Request.Context(), clientID)
	if err != nil {
		if err == sql.ErrNoRows {
			c.JSON(http.StatusNotFound, gin.H{"error": "Client not found"})
			return
		}
		h.logger.Error("Failed to get client usage", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to get client usage"})
		return
	}

	c.JSON(http.StatusOK, usage)
}

// Helper functions

func isValidClientID(clientID string) bool {
	// Only allow alphanumeric and underscores
	for _, char := range clientID {
		if !((char >= 'a' && char <= 'z') || (char >= 'A' && char <= 'Z') || (char >= '0' && char <= '9') || char == '_') {
			return false
		}
	}
	return len(clientID) >= 3 && len(clientID) <= 50
}

func (h *ClientHandlers) clientExists(ctx context.Context, clientID string) (bool, error) {
	// Check if schema exists
	query := `
		SELECT EXISTS (
			SELECT 1 FROM information_schema.schemata 
			WHERE schema_name = $1
		)
	`
	schemaName := fmt.Sprintf("client_%s", clientID)

	var exists bool
	err := h.clientsDB.QueryRow(ctx, query, schemaName).Scan(&exists)
	return exists, err
}

func (h *ClientHandlers) createClientSchema(ctx context.Context, clientID string) error {
	// Call the stored procedure to create client schema
	query := `SELECT create_client_schema($1)`
	_, err := h.clientsDB.Exec(ctx, query, clientID)
	return err
}

func (h *ClientHandlers) storeClientInfo(ctx context.Context, req *CreateClientRequest) error {
	// First, ensure we have a clients info table
	createTableQuery := `
		CREATE TABLE IF NOT EXISTS clients_info (
			client_id VARCHAR(50) PRIMARY KEY,
			display_name VARCHAR(255) NOT NULL,
			settings JSONB DEFAULT '{}',
			is_active BOOLEAN DEFAULT true,
			created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
			updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
		)
	`

	if _, err := h.clientsDB.Exec(ctx, createTableQuery); err != nil {
		return fmt.Errorf("failed to create clients_info table: %w", err)
	}

	// Insert client info
	insertQuery := `
		INSERT INTO clients_info (client_id, display_name, settings)
		VALUES ($1, $2, $3)
		ON CONFLICT (client_id) DO NOTHING
	`

	_, err := h.clientsDB.Exec(ctx, insertQuery, req.ClientID, req.DisplayName, req.Settings)
	return err
}

func (h *ClientHandlers) listClients(ctx context.Context) ([]ClientInfo, error) {
	// First ensure the table exists
	h.storeClientInfo(ctx, &CreateClientRequest{}) // This will create table if needed

	query := `
		SELECT client_id, display_name, settings, is_active, created_at
		FROM clients_info
		WHERE is_active = true
		ORDER BY created_at DESC
	`

	rows, err := h.clientsDB.Query(ctx, query)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var clients []ClientInfo
	for rows.Next() {
		var client ClientInfo
		var createdAt sql.NullTime
		var settings sql.NullString

		err := rows.Scan(&client.ClientID, &client.DisplayName, &settings, &client.IsActive, &createdAt)
		if err != nil {
			h.logger.Error("Failed to scan client row", zap.Error(err))
			continue
		}

		if createdAt.Valid {
			client.CreatedAt = createdAt.Time.Format("2006-01-02T15:04:05Z")
		}

		clients = append(clients, client)
	}

	// Also check for schemas without entries in clients_info
	schemaQuery := `
		SELECT schema_name 
		FROM information_schema.schemata 
		WHERE schema_name LIKE 'client_%'
		AND schema_name NOT IN (
			SELECT 'client_' || client_id FROM clients_info
		)
	`

	schemaRows, err := h.clientsDB.Query(ctx, schemaQuery)
	if err == nil {
		defer schemaRows.Close()
		for schemaRows.Next() {
			var schemaName string
			if err := schemaRows.Scan(&schemaName); err == nil {
				clientID := strings.TrimPrefix(schemaName, "client_")
				clients = append(clients, ClientInfo{
					ClientID:    clientID,
					DisplayName: clientID + " (Legacy)",
					IsActive:    true,
					CreatedAt:   "Unknown",
				})
			}
		}
	}

	return clients, nil
}

// ClientUsageStats represents usage statistics for a client
type ClientUsageStats struct {
	ClientID            string `json:"client_id"`
	TotalUsers          int    `json:"total_users"`
	ActiveUsers         int    `json:"active_users"`
	TotalInstances      int    `json:"total_instances"`
	ActiveInstances     int    `json:"active_instances"`
	TotalWorkflows      int    `json:"total_workflows"`
	WorkflowsLast30Days int    `json:"workflows_last_30_days"`
	TotalMemoryEntries  int    `json:"total_memory_entries"`
	TotalFuelConsumed   int64  `json:"total_fuel_consumed"`
}

func (h *ClientHandlers) getClientUsage(ctx context.Context, clientID string) (*ClientUsageStats, error) {
	stats := &ClientUsageStats{ClientID: clientID}

	// Get user counts from auth database (would need access to auth DB)
	// For now, we'll focus on what we can get from clients DB

	// Count agent instances
	instanceQuery := fmt.Sprintf(`
		SELECT 
			COUNT(*) as total,
			COUNT(*) FILTER (WHERE is_active = true) as active
		FROM client_%s.agent_instances
	`, clientID)

	err := h.clientsDB.QueryRow(ctx, instanceQuery).Scan(&stats.TotalInstances, &stats.ActiveInstances)
	if err != nil && !strings.Contains(err.Error(), "does not exist") {
		return nil, err
	}

	// Count workflows
	workflowQuery := fmt.Sprintf(`
		SELECT 
			COUNT(*) as total,
			COUNT(*) FILTER (WHERE started_at > NOW() - INTERVAL '30 days') as recent
		FROM client_%s.workflow_executions
	`, clientID)

	err = h.clientsDB.QueryRow(ctx, workflowQuery).Scan(&stats.TotalWorkflows, &stats.WorkflowsLast30Days)
	if err != nil && !strings.Contains(err.Error(), "does not exist") {
		h.logger.Warn("Failed to get workflow stats", zap.Error(err))
	}

	// Count memory entries
	memoryQuery := fmt.Sprintf(`
		SELECT COUNT(*) FROM client_%s.agent_memory
	`, clientID)

	err = h.clientsDB.QueryRow(ctx, memoryQuery).Scan(&stats.TotalMemoryEntries)
	if err != nil && !strings.Contains(err.Error(), "does not exist") {
		h.logger.Warn("Failed to get memory stats", zap.Error(err))
	}

	// Get fuel consumption
	fuelQuery := fmt.Sprintf(`
		SELECT COALESCE(SUM(fuel_consumed), 0)
		FROM client_%s.usage_analytics
	`, clientID)

	err = h.clientsDB.QueryRow(ctx, fuelQuery).Scan(&stats.TotalFuelConsumed)
	if err != nil && !strings.Contains(err.Error(), "does not exist") {
		h.logger.Warn("Failed to get fuel stats", zap.Error(err))
	}

	return stats, nil
}
-------------------------------------------------
filepath = ./internal/core-manager/admin/dashboard_handlers.go
// FILE: internal/core-manager/admin/dashboard_handlers.go
package admin

import (
	"context"
	"database/sql"
	"fmt"
	"net/http"
	"sort"
	"strconv"
	"strings"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// DashboardHandlers provides admin dashboard endpoints
type DashboardHandlers struct {
	clientsDB   *pgxpool.Pool
	templatesDB *pgxpool.Pool
	authDB      *sql.DB // For accessing auth database
	logger      *zap.Logger
}

// NewDashboardHandlers creates new dashboard handlers
func NewDashboardHandlers(clientsDB, templatesDB *pgxpool.Pool, authDB *sql.DB, logger *zap.Logger) *DashboardHandlers {
	return &DashboardHandlers{
		clientsDB:   clientsDB,
		templatesDB: templatesDB,
		authDB:      authDB,
		logger:      logger,
	}
}

// DashboardMetrics represents overall system metrics
type DashboardMetrics struct {
	Overview       OverviewMetrics     `json:"overview"`
	UserMetrics    UserMetrics         `json:"user_metrics"`
	AgentMetrics   AgentMetrics        `json:"agent_metrics"`
	UsageMetrics   UsageMetrics        `json:"usage_metrics"`
	SystemHealth   SystemHealthMetrics `json:"system_health"`
	RecentActivity []ActivityEntry     `json:"recent_activity"`
}

type OverviewMetrics struct {
	TotalClients        int     `json:"total_clients"`
	TotalUsers          int     `json:"total_users"`
	ActiveUsers30Days   int     `json:"active_users_30_days"`
	TotalAgentInstances int     `json:"total_agent_instances"`
	TotalWorkflows      int     `json:"total_workflows"`
	SuccessRate         float64 `json:"success_rate"`
	TotalRevenue        float64 `json:"total_revenue_mtd"`
}

type UserMetrics struct {
	UsersByTier      map[string]int `json:"users_by_tier"`
	NewUsersToday    int            `json:"new_users_today"`
	NewUsersThisWeek int            `json:"new_users_this_week"`
	ChurnRate        float64        `json:"churn_rate_monthly"`
}

type AgentMetrics struct {
	AgentsByType        map[string]int `json:"agents_by_type"`
	MostUsedAgents      []AgentUsage   `json:"most_used_agents"`
	AverageResponseTime float64        `json:"avg_response_time_ms"`
}

type AgentUsage struct {
	AgentType  string `json:"agent_type"`
	UsageCount int    `json:"usage_count"`
}

type UsageMetrics struct {
	TotalFuelConsumed int64            `json:"total_fuel_consumed"`
	FuelByAgentType   map[string]int64 `json:"fuel_by_agent_type"`
	APICallsToday     int              `json:"api_calls_today"`
	StorageUsedGB     float64          `json:"storage_used_gb"`
}

type SystemHealthMetrics struct {
	DatabaseStatus  string  `json:"database_status"`
	KafkaStatus     string  `json:"kafka_status"`
	AverageLatency  float64 `json:"average_latency_ms"`
	ErrorRate       float64 `json:"error_rate_percent"`
	ActiveWorkflows int     `json:"active_workflows"`
	QueueDepth      int     `json:"queue_depth"`
}

type ActivityEntry struct {
	Timestamp   time.Time `json:"timestamp"`
	Type        string    `json:"type"`
	Description string    `json:"description"`
	UserID      string    `json:"user_id,omitempty"`
	ClientID    string    `json:"client_id,omitempty"`
}

// HandleGetDashboard returns comprehensive dashboard metrics
func (h *DashboardHandlers) HandleGetDashboard(c *gin.Context) {
	ctx := c.Request.Context()

	metrics := &DashboardMetrics{
		Overview:       h.getOverviewMetrics(ctx),
		UserMetrics:    h.getUserMetrics(ctx),
		AgentMetrics:   h.getAgentMetrics(ctx),
		UsageMetrics:   h.getUsageMetrics(ctx),
		SystemHealth:   h.getSystemHealth(ctx),
		RecentActivity: h.getRecentActivity(ctx),
	}

	c.JSON(http.StatusOK, metrics)
}

// getOverviewMetrics collects high-level system metrics
func (h *DashboardHandlers) getOverviewMetrics(ctx context.Context) OverviewMetrics {
	metrics := OverviewMetrics{}

	// Count total clients
	var totalClients int
	err := h.clientsDB.QueryRow(ctx, `
		SELECT COUNT(DISTINCT schema_name) 
		FROM information_schema.schemata 
		WHERE schema_name LIKE 'client_%'
	`).Scan(&totalClients)
	if err != nil {
		h.logger.Error("Failed to count clients", zap.Error(err))
	}
	metrics.TotalClients = totalClients

	// Count total users from auth DB
	err = h.authDB.QueryRowContext(ctx, `
		SELECT COUNT(*) FROM users WHERE is_active = true
	`).Scan(&metrics.TotalUsers)
	if err != nil {
		h.logger.Error("Failed to count users", zap.Error(err))
	}

	// Count active users in last 30 days
	err = h.authDB.QueryRowContext(ctx, `
		SELECT COUNT(DISTINCT user_id) 
		FROM user_activity_logs 
		WHERE created_at > NOW() - INTERVAL 30 DAY
	`).Scan(&metrics.ActiveUsers30Days)
	if err != nil {
		h.logger.Error("Failed to count active users", zap.Error(err))
	}

	// Count total agent instances across all clients
	rows, err := h.clientsDB.Query(ctx, `
		SELECT schema_name 
		FROM information_schema.schemata 
		WHERE schema_name LIKE 'client_%'
	`)
	if err == nil {
		defer rows.Close()
		for rows.Next() {
			var schemaName string
			if err := rows.Scan(&schemaName); err == nil {
				var count int
				query := fmt.Sprintf("SELECT COUNT(*) FROM %s.agent_instances WHERE is_active = true", schemaName)
				h.clientsDB.QueryRow(ctx, query).Scan(&count)
				metrics.TotalAgentInstances += count
			}
		}
	}

	// Count total workflows and calculate success rate
	var successCount int
	err = h.clientsDB.QueryRow(ctx, `
		SELECT 
			COUNT(*) as total,
			COUNT(*) FILTER (WHERE status = 'COMPLETED') as success
		FROM orchestrator_state
		WHERE created_at > NOW() - INTERVAL '30 days'
	`).Scan(&metrics.TotalWorkflows, &successCount)
	if err == nil && metrics.TotalWorkflows > 0 {
		metrics.SuccessRate = float64(successCount) / float64(metrics.TotalWorkflows) * 100
	}

	// Calculate total revenue (simplified - would need proper billing integration)
	err = h.authDB.QueryRowContext(ctx, `
		SELECT COALESCE(SUM(
			CASE 
				WHEN st.name = 'basic' THEN st.price_monthly
				WHEN st.name = 'premium' THEN st.price_monthly
				WHEN st.name = 'enterprise' THEN st.price_monthly
				ELSE 0
			END
		), 0) as total_revenue
		FROM users u
		JOIN subscriptions s ON u.id = s.user_id
		JOIN subscription_tiers st ON s.tier = st.name
		WHERE u.is_active = true AND s.status = 'active'
	`).Scan(&metrics.TotalRevenue)
	if err != nil {
		h.logger.Error("Failed to calculate revenue", zap.Error(err))
	}

	return metrics
}

// getUserMetrics collects user-related metrics
func (h *DashboardHandlers) getUserMetrics(ctx context.Context) UserMetrics {
	metrics := UserMetrics{
		UsersByTier: make(map[string]int),
	}

	// Count users by subscription tier
	rows, err := h.authDB.QueryContext(ctx, `
		SELECT subscription_tier, COUNT(*) 
		FROM users 
		WHERE is_active = true 
		GROUP BY subscription_tier
	`)
	if err == nil {
		defer rows.Close()
		for rows.Next() {
			var tier string
			var count int
			if err := rows.Scan(&tier, &count); err == nil {
				metrics.UsersByTier[tier] = count
			}
		}
	}

	// Count new users today
	err = h.authDB.QueryRowContext(ctx, `
		SELECT COUNT(*) FROM users 
		WHERE DATE(created_at) = CURDATE()
	`).Scan(&metrics.NewUsersToday)

	// Count new users this week
	err = h.authDB.QueryRowContext(ctx, `
		SELECT COUNT(*) FROM users 
		WHERE created_at > NOW() - INTERVAL 7 DAY
	`).Scan(&metrics.NewUsersThisWeek)

	// Calculate churn rate (simplified)
	var totalLastMonth, churnedThisMonth int
	h.authDB.QueryRowContext(ctx, `
		SELECT 
			(SELECT COUNT(*) FROM users WHERE created_at < DATE_SUB(NOW(), INTERVAL 1 MONTH)) as total_last_month,
			(SELECT COUNT(*) FROM users WHERE is_active = false AND updated_at > DATE_SUB(NOW(), INTERVAL 1 MONTH)) as churned
	`).Scan(&totalLastMonth, &churnedThisMonth)

	if totalLastMonth > 0 {
		metrics.ChurnRate = float64(churnedThisMonth) / float64(totalLastMonth) * 100
	}

	return metrics
}

// getAgentMetrics collects agent-related metrics
func (h *DashboardHandlers) getAgentMetrics(ctx context.Context) AgentMetrics {
	metrics := AgentMetrics{
		AgentsByType: make(map[string]int),
	}

	// Count agents by type
	rows, err := h.clientsDB.Query(ctx, `
		SELECT type, COUNT(*) 
		FROM agent_definitions 
		WHERE is_active = true 
		GROUP BY type
	`)
	if err == nil {
		defer rows.Close()
		for rows.Next() {
			var agentType string
			var count int
			if err := rows.Scan(&agentType, &count); err == nil {
				metrics.AgentsByType[agentType] = count
			}
		}
	}

	// Get most used agents (from all client schemas)
	// This is simplified - in production you'd aggregate across all client schemas
	mostUsedQuery := `
		SELECT 
			ad.type,
			COUNT(*) as usage_count
		FROM orchestrator_state os
		JOIN agent_definitions ad ON true
		WHERE os.created_at > NOW() - INTERVAL '7 days'
		GROUP BY ad.type
		ORDER BY usage_count DESC
		LIMIT 5
	`

	rows, err = h.clientsDB.Query(ctx, mostUsedQuery)
	if err == nil {
		defer rows.Close()
		for rows.Next() {
			var usage AgentUsage
			if err := rows.Scan(&usage.AgentType, &usage.UsageCount); err == nil {
				metrics.MostUsedAgents = append(metrics.MostUsedAgents, usage)
			}
		}
	}

	// Calculate average response time (simplified)
	// In production, this would come from your metrics system
	metrics.AverageResponseTime = 245.7 // Mock value

	return metrics
}

// getUsageMetrics collects resource usage metrics
func (h *DashboardHandlers) getUsageMetrics(ctx context.Context) UsageMetrics {
	metrics := UsageMetrics{
		FuelByAgentType: make(map[string]int64),
	}

	// Get total fuel consumed across all clients
	// This would need to aggregate from all client schemas
	metrics.TotalFuelConsumed = 125000 // Mock value

	// Fuel by agent type (mock data)
	metrics.FuelByAgentType = map[string]int64{
		"copywriter":      45000,
		"researcher":      35000,
		"reasoning":       25000,
		"image-generator": 20000,
	}

	// API calls today (mock)
	metrics.APICallsToday = 8543

	// Storage used (would query actual storage metrics)
	metrics.StorageUsedGB = 45.7

	return metrics
}

// getSystemHealth checks current system health
func (h *DashboardHandlers) getSystemHealth(ctx context.Context) SystemHealthMetrics {
	metrics := SystemHealthMetrics{}

	// Check database status
	if err := h.clientsDB.Ping(ctx); err != nil {
		metrics.DatabaseStatus = "unhealthy"
	} else {
		metrics.DatabaseStatus = "healthy"
	}

	// Kafka status (simplified)
	metrics.KafkaStatus = "healthy"

	// Get active workflows count
	h.clientsDB.QueryRow(ctx, `
		SELECT COUNT(*) FROM orchestrator_state 
		WHERE status IN ('RUNNING', 'AWAITING_RESPONSES', 'PAUSED_FOR_HUMAN_INPUT')
	`).Scan(&metrics.ActiveWorkflows)

	// Mock metrics - in production these would come from Prometheus
	metrics.AverageLatency = 123.5
	metrics.ErrorRate = 0.02
	metrics.QueueDepth = 42

	return metrics
}

// getRecentActivity returns recent system activity
func (h *DashboardHandlers) getRecentActivity(ctx context.Context) []ActivityEntry {
	activities := []ActivityEntry{}

	// Get recent user registrations
	rows, err := h.authDB.QueryContext(ctx, `
		SELECT created_at, 'user_registration' as type, email, client_id
		FROM users
		ORDER BY created_at DESC
		LIMIT 10
	`)
	if err == nil {
		defer rows.Close()
		for rows.Next() {
			var activity ActivityEntry
			var email string
			rows.Scan(&activity.Timestamp, &activity.Type, &email, &activity.ClientID)
			activity.Description = fmt.Sprintf("New user registered: %s", email)
			activities = append(activities, activity)
		}
	}

	// Get recent workflow completions
	workflowRows, err := h.clientsDB.Query(ctx, `
		SELECT updated_at, status, correlation_id, client_id
		FROM orchestrator_state
		WHERE status IN ('COMPLETED', 'FAILED')
		ORDER BY updated_at DESC
		LIMIT 10
	`)
	if err == nil {
		defer workflowRows.Close()
		for workflowRows.Next() {
			var activity ActivityEntry
			var status, correlationID string
			workflowRows.Scan(&activity.Timestamp, &status, &correlationID, &activity.ClientID)
			activity.Type = "workflow_" + strings.ToLower(status)
			activity.Description = fmt.Sprintf("Workflow %s: %s", status, correlationID[:8])
			activities = append(activities, activity)
		}
	}

	// Sort by timestamp
	sort.Slice(activities, func(i, j int) bool {
		return activities[i].Timestamp.After(activities[j].Timestamp)
	})

	// Return top 20
	if len(activities) > 20 {
		activities = activities[:20]
	}

	return activities
}

// HandleGetSystemLogs returns recent system logs
func (h *DashboardHandlers) HandleGetSystemLogs(c *gin.Context) {
	service := c.Query("service")
	level := c.Query("level")
	limit := 100

	if limitStr := c.Query("limit"); limitStr != "" {
		if l, err := strconv.Atoi(limitStr); err == nil && l > 0 && l <= 1000 {
			limit = l
		}
	}

	// In production, this would query your centralized logging system
	// For now, return mock data
	logs := []LogEntry{
		{
			Timestamp: time.Now(),
			Service:   "auth-service",
			Level:     "info",
			Message:   "User login successful",
			Metadata: map[string]interface{}{
				"user_id": "123",
				"ip":      "192.168.1.1",
			},
		},
	}

	c.JSON(http.StatusOK, gin.H{
		"logs":  logs,
		"total": len(logs),
		"query": gin.H{
			"service": service,
			"level":   level,
			"limit":   limit,
		},
	})
}

type LogEntry struct {
	Timestamp time.Time              `json:"timestamp"`
	Service   string                 `json:"service"`
	Level     string                 `json:"level"`
	Message   string                 `json:"message"`
	Metadata  map[string]interface{} `json:"metadata,omitempty"`
}
-------------------------------------------------
filepath = ./internal/core-manager/admin/system_handlers.go
// FILE: internal/core-manager/admin/system_handlers.go
package admin

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"net/http"
	"strings"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/gqls/agentchassis/platform/orchestration"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// SystemHandlers handles admin system monitoring operations
type SystemHandlers struct {
	clientsDB     *pgxpool.Pool
	templatesDB   *pgxpool.Pool
	kafkaProducer kafka.Producer
	logger        *zap.Logger
}

// NewSystemHandlers creates new system monitoring handlers
func NewSystemHandlers(clientsDB, templatesDB *pgxpool.Pool, kafkaProducer kafka.Producer, logger *zap.Logger) *SystemHandlers {
	return &SystemHandlers{
		clientsDB:     clientsDB,
		templatesDB:   templatesDB,
		kafkaProducer: kafkaProducer,
		logger:        logger,
	}
}

// SystemStatus represents overall system health
type SystemStatus struct {
	Status      string                    `json:"status"`
	Timestamp   time.Time                 `json:"timestamp"`
	Services    map[string]ServiceStatus  `json:"services"`
	Databases   map[string]DatabaseStatus `json:"databases"`
	KafkaStatus KafkaStatus               `json:"kafka"`
}

// ServiceStatus represents a service health status
type ServiceStatus struct {
	Name    string `json:"name"`
	Status  string `json:"status"`
	Version string `json:"version,omitempty"`
	Uptime  string `json:"uptime,omitempty"`
}

// DatabaseStatus represents database health
type DatabaseStatus struct {
	Name        string `json:"name"`
	Status      string `json:"status"`
	Connections int    `json:"active_connections"`
	Size        string `json:"size,omitempty"`
}

// KafkaStatus represents Kafka cluster status
type KafkaStatus struct {
	Status      string `json:"status"`
	BrokerCount int    `json:"broker_count"`
	TopicCount  int    `json:"topic_count"`
	ConsumerLag int64  `json:"total_consumer_lag,omitempty"`
}

// HandleGetSystemStatus returns aggregated system status
func (h *SystemHandlers) HandleGetSystemStatus(c *gin.Context) {
	status := SystemStatus{
		Status:    "healthy",
		Timestamp: time.Now(),
		Services:  make(map[string]ServiceStatus),
		Databases: make(map[string]DatabaseStatus),
	}

	// Check database connections
	// Clients DB
	if err := h.clientsDB.Ping(c.Request.Context()); err != nil {
		status.Status = "degraded"
		status.Databases["clients_db"] = DatabaseStatus{
			Name:   "clients_db",
			Status: "unhealthy",
		}
	} else {
		stats := h.clientsDB.Stat()
		status.Databases["clients_db"] = DatabaseStatus{
			Name:        "clients_db",
			Status:      "healthy",
			Connections: int(stats.AcquiredConns()),
		}
	}

	// Templates DB
	if err := h.templatesDB.Ping(c.Request.Context()); err != nil {
		status.Status = "degraded"
		status.Databases["templates_db"] = DatabaseStatus{
			Name:   "templates_db",
			Status: "unhealthy",
		}
	} else {
		stats := h.templatesDB.Stat()
		status.Databases["templates_db"] = DatabaseStatus{
			Name:        "templates_db",
			Status:      "healthy",
			Connections: int(stats.AcquiredConns()),
		}
	}

	// Get database sizes
	h.getDatabaseSizes(c.Request.Context(), &status)

	// Check Kafka (simplified - in production you'd want more detailed checks)
	status.KafkaStatus = h.getKafkaStatus(c.Request.Context())

	c.JSON(http.StatusOK, status)
}

// HandleListKafkaTopics lists all Kafka topics
func (h *SystemHandlers) HandleListKafkaTopics(c *gin.Context) {
	// This would require a Kafka admin client
	// For now, return known topics
	knownTopics := []string{
		"orchestrator.state-changes",
		"human.approvals",
		"system.events",
		"system.notifications.ui",
		"system.commands.workflow.resume",
		"system.agent.reasoning.process",
		"system.responses.reasoning",
		"system.adapter.image.generate",
		"system.responses.image",
		"system.adapter.web.search",
		"system.responses.websearch",
		"system.agent.generic.process",
		"system.tasks.copywriter",
		"system.tasks.researcher",
		"system.tasks.content-creator",
		"system.tasks.multimedia-creator",
	}

	c.JSON(http.StatusOK, gin.H{
		"topics": knownTopics,
		"count":  len(knownTopics),
	})
}

// WorkflowListRequest represents workflow filtering parameters
type WorkflowListRequest struct {
	Status    string `form:"status"`
	ClientID  string `form:"client_id"`
	StartDate string `form:"start_date"`
	EndDate   string `form:"end_date"`
	Limit     int    `form:"limit,default=50"`
	Offset    int    `form:"offset,default=0"`
}

// HandleListWorkflows lists workflow executions
func (h *SystemHandlers) HandleListWorkflows(c *gin.Context) {
	var req WorkflowListRequest
	if err := c.ShouldBindQuery(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	workflows, err := h.listWorkflows(c.Request.Context(), req)
	if err != nil {
		h.logger.Error("Failed to list workflows", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to list workflows"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"workflows": workflows,
		"count":     len(workflows),
	})
}

// HandleGetWorkflow returns detailed workflow state
func (h *SystemHandlers) HandleGetWorkflow(c *gin.Context) {
	correlationID := c.Param("correlation_id")

	workflow, err := h.getWorkflowState(c.Request.Context(), correlationID)
	if err != nil {
		if err == sql.ErrNoRows {
			c.JSON(http.StatusNotFound, gin.H{"error": "Workflow not found"})
			return
		}
		h.logger.Error("Failed to get workflow", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to get workflow"})
		return
	}

	c.JSON(http.StatusOK, workflow)
}

// HandleResumeWorkflow manually resumes or terminates a workflow
func (h *SystemHandlers) HandleResumeWorkflow(c *gin.Context) {
	correlationID := c.Param("correlation_id")

	var req struct {
		Action   string                 `json:"action" binding:"required,oneof=resume terminate"`
		Feedback map[string]interface{} `json:"feedback,omitempty"`
	}

	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Get workflow state to find client_id
	workflow, err := h.getWorkflowState(c.Request.Context(), correlationID)
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": "Workflow not found"})
		return
	}

	if req.Action == "terminate" {
		// Update workflow status to FAILED
		err = h.updateWorkflowStatus(c.Request.Context(), correlationID, "FAILED", "Manually terminated by admin")
		if err != nil {
			h.logger.Error("Failed to terminate workflow", zap.Error(err))
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to terminate workflow"})
			return
		}
	} else if req.Action == "resume" {
		// Send resume command via Kafka
		resumePayload := map[string]interface{}{
			"approved": true,
			"feedback": req.Feedback,
		}

		payloadBytes, _ := json.Marshal(resumePayload)
		headers := map[string]string{
			"correlation_id": correlationID,
			"client_id":      workflow["client_id"].(string),
			"admin_action":   "true",
		}

		err = h.kafkaProducer.Produce(c.Request.Context(),
			orchestration.ResumeWorkflowTopic,
			headers,
			[]byte(correlationID),
			payloadBytes,
		)

		if err != nil {
			h.logger.Error("Failed to send resume command", zap.Error(err))
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to resume workflow"})
			return
		}
	}

	c.JSON(http.StatusOK, gin.H{
		"message":        fmt.Sprintf("Workflow %s successfully", req.Action+"d"),
		"correlation_id": correlationID,
	})
}

// HandleListAgentDefinitions lists all agent types
func (h *SystemHandlers) HandleListAgentDefinitions(c *gin.Context) {
	query := `
		SELECT id, type, display_name, description, category, default_config, is_active
		FROM agent_definitions
		WHERE deleted_at IS NULL
		ORDER BY category, type
	`

	rows, err := h.clientsDB.Query(c.Request.Context(), query)
	if err != nil {
		h.logger.Error("Failed to list agent definitions", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to list agent definitions"})
		return
	}
	defer rows.Close()

	var definitions []map[string]interface{}
	for rows.Next() {
		var def struct {
			ID            string
			Type          string
			DisplayName   string
			Description   string
			Category      string
			DefaultConfig json.RawMessage
			IsActive      bool
		}

		err := rows.Scan(&def.ID, &def.Type, &def.DisplayName,
			&def.Description, &def.Category, &def.DefaultConfig, &def.IsActive)
		if err != nil {
			h.logger.Error("Failed to scan agent definition", zap.Error(err))
			continue
		}

		var config map[string]interface{}
		json.Unmarshal(def.DefaultConfig, &config)

		definitions = append(definitions, map[string]interface{}{
			"id":             def.ID,
			"type":           def.Type,
			"display_name":   def.DisplayName,
			"description":    def.Description,
			"category":       def.Category,
			"default_config": config,
			"is_active":      def.IsActive,
		})
	}

	c.JSON(http.StatusOK, gin.H{
		"definitions": definitions,
		"count":       len(definitions),
	})
}

// HandleUpdateAgentDefinition updates an agent type configuration
func (h *SystemHandlers) HandleUpdateAgentDefinition(c *gin.Context) {
	typeName := c.Param("type_name")

	var req struct {
		DisplayName   *string                `json:"display_name"`
		Description   *string                `json:"description"`
		DefaultConfig map[string]interface{} `json:"default_config"`
		IsActive      *bool                  `json:"is_active"`
	}

	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Build update query
	updates := []string{"updated_at = NOW()"}
	args := []interface{}{}
	argCount := 0

	if req.DisplayName != nil {
		argCount++
		updates = append(updates, fmt.Sprintf("display_name = $%d", argCount))
		args = append(args, *req.DisplayName)
	}

	if req.Description != nil {
		argCount++
		updates = append(updates, fmt.Sprintf("description = $%d", argCount))
		args = append(args, *req.Description)
	}

	if req.DefaultConfig != nil {
		argCount++
		configBytes, _ := json.Marshal(req.DefaultConfig)
		updates = append(updates, fmt.Sprintf("default_config = $%d", argCount))
		args = append(args, configBytes)
	}

	if req.IsActive != nil {
		argCount++
		updates = append(updates, fmt.Sprintf("is_active = $%d", argCount))
		args = append(args, *req.IsActive)
	}

	argCount++
	args = append(args, typeName)

	query := fmt.Sprintf(
		"UPDATE agent_definitions SET %s WHERE type = $%d AND deleted_at IS NULL",
		strings.Join(updates, ", "),
		argCount,
	)

	result, err := h.clientsDB.Exec(c.Request.Context(), query, args...)
	if err != nil {
		h.logger.Error("Failed to update agent definition", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to update agent definition"})
		return
	}

	if result.RowsAffected() == 0 {
		c.JSON(http.StatusNotFound, gin.H{"error": "Agent definition not found"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"message": "Agent definition updated successfully",
		"type":    typeName,
	})
}

// Helper methods

func (h *SystemHandlers) getDatabaseSizes(ctx context.Context, status *SystemStatus) {
	// Get clients DB size
	var clientsSize string
	err := h.clientsDB.QueryRow(ctx,
		"SELECT pg_size_pretty(pg_database_size(current_database()))").Scan(&clientsSize)
	if err == nil {
		dbStatus := status.Databases["clients_db"]
		dbStatus.Size = clientsSize
		status.Databases["clients_db"] = dbStatus
	}

	// Get templates DB size
	var templatesSize string
	err = h.templatesDB.QueryRow(ctx,
		"SELECT pg_size_pretty(pg_database_size(current_database()))").Scan(&templatesSize)
	if err == nil {
		dbStatus := status.Databases["templates_db"]
		dbStatus.Size = templatesSize
		status.Databases["templates_db"] = dbStatus
	}
}

func (h *SystemHandlers) getKafkaStatus(ctx context.Context) KafkaStatus {
	// In a real implementation, you'd use a Kafka admin client
	// For now, return a simplified status
	return KafkaStatus{
		Status:      "healthy",
		BrokerCount: 3,  // Based on the k8s config
		TopicCount:  20, // Approximate
	}
}

func (h *SystemHandlers) listWorkflows(ctx context.Context, req WorkflowListRequest) ([]map[string]interface{}, error) {
	query := `
		SELECT correlation_id, client_id, status, current_step, 
		       created_at, updated_at, error
		FROM orchestrator_state
		WHERE 1=1
	`

	args := []interface{}{}
	argCount := 0

	if req.Status != "" {
		argCount++
		query += fmt.Sprintf(" AND status = $%d", argCount)
		args = append(args, req.Status)
	}

	if req.ClientID != "" {
		argCount++
		query += fmt.Sprintf(" AND client_id = $%d", argCount)
		args = append(args, req.ClientID)
	}

	if req.StartDate != "" {
		argCount++
		query += fmt.Sprintf(" AND created_at >= $%d", argCount)
		args = append(args, req.StartDate)
	}

	if req.EndDate != "" {
		argCount++
		query += fmt.Sprintf(" AND created_at <= $%d", argCount)
		args = append(args, req.EndDate)
	}

	query += " ORDER BY created_at DESC"

	argCount++
	query += fmt.Sprintf(" LIMIT $%d", argCount)
	args = append(args, req.Limit)

	argCount++
	query += fmt.Sprintf(" OFFSET $%d", argCount)
	args = append(args, req.Offset)

	rows, err := h.clientsDB.Query(ctx, query, args...)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var workflows []map[string]interface{}
	for rows.Next() {
		var w struct {
			CorrelationID string
			ClientID      string
			Status        string
			CurrentStep   string
			CreatedAt     time.Time
			UpdatedAt     time.Time
			Error         sql.NullString
		}

		err := rows.Scan(&w.CorrelationID, &w.ClientID, &w.Status,
			&w.CurrentStep, &w.CreatedAt, &w.UpdatedAt, &w.Error)
		if err != nil {
			continue
		}

		workflow := map[string]interface{}{
			"correlation_id": w.CorrelationID,
			"client_id":      w.ClientID,
			"status":         w.Status,
			"current_step":   w.CurrentStep,
			"created_at":     w.CreatedAt,
			"updated_at":     w.UpdatedAt,
		}

		if w.Error.Valid {
			workflow["error"] = w.Error.String
		}

		workflows = append(workflows, workflow)
	}

	return workflows, nil
}

func (h *SystemHandlers) getWorkflowState(ctx context.Context, correlationID string) (map[string]interface{}, error) {
	var state orchestration.OrchestrationState
	var awaitedStepsJSON, collectedDataJSON, initialRequestDataJSON, finalResultJSON []byte
	var errorNull sql.NullString

	query := `
		SELECT correlation_id, client_id, status, current_step, awaited_steps, 
		       collected_data, initial_request_data, final_result, error, 
		       created_at, updated_at
		FROM orchestrator_state
		WHERE correlation_id = $1
	`

	err := h.clientsDB.QueryRow(ctx, query, correlationID).Scan(
		&state.CorrelationID,
		&state.ClientID,
		&state.Status,
		&state.CurrentStep,
		&awaitedStepsJSON,
		&collectedDataJSON,
		&initialRequestDataJSON,
		&finalResultJSON,
		&errorNull,
		&state.CreatedAt,
		&state.UpdatedAt,
	)

	if err != nil {
		return nil, err
	}

	// Parse JSON fields
	json.Unmarshal(awaitedStepsJSON, &state.AwaitedSteps)
	json.Unmarshal(collectedDataJSON, &state.CollectedData)
	state.InitialRequestData = initialRequestDataJSON
	state.FinalResult = finalResultJSON

	if errorNull.Valid {
		state.Error = errorNull.String
	}

	// Convert to map for response
	result := map[string]interface{}{
		"correlation_id":       state.CorrelationID,
		"client_id":            state.ClientID,
		"status":               state.Status,
		"current_step":         state.CurrentStep,
		"awaited_steps":        state.AwaitedSteps,
		"collected_data":       state.CollectedData,
		"initial_request_data": json.RawMessage(state.InitialRequestData),
		"final_result":         json.RawMessage(state.FinalResult),
		"error":                state.Error,
		"created_at":           state.CreatedAt,
		"updated_at":           state.UpdatedAt,
	}

	return result, nil
}

func (h *SystemHandlers) updateWorkflowStatus(ctx context.Context, correlationID, status, errorMsg string) error {
	query := `
		UPDATE orchestrator_state 
		SET status = $2, error = $3, updated_at = NOW()
		WHERE correlation_id = $1
	`

	_, err := h.clientsDB.Exec(ctx, query, correlationID, status, errorMsg)
	return err
}
-------------------------------------------------
filepath = ./internal/core-manager/admin/agent_handlers.go
// FILE: internal/core-manager/admin/agent_handlers.go
package admin

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"net/http"
	"strings"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// AgentHandlers manages agent-related admin operations
type AgentHandlers struct {
	clientsDB     *pgxpool.Pool
	templatesDB   *pgxpool.Pool
	kafkaProducer kafka.Producer
	logger        *zap.Logger
	personaRepo   models.PersonaRepository
}

// NewAgentHandlers creates new agent management handlers
func NewAgentHandlers(clientsDB, templatesDB *pgxpool.Pool, kafkaProducer kafka.Producer, logger *zap.Logger, personaRepo models.PersonaRepository) *AgentHandlers {
	return &AgentHandlers{
		clientsDB:     clientsDB,
		templatesDB:   templatesDB,
		kafkaProducer: kafkaProducer,
		logger:        logger,
		personaRepo:   personaRepo,
	}
}

// AgentDefinitionRequest for creating/updating agent definitions
type AgentDefinitionRequest struct {
	Type          string                 `json:"type" binding:"required"`
	DisplayName   string                 `json:"display_name" binding:"required"`
	Description   string                 `json:"description"`
	Category      string                 `json:"category" binding:"required,oneof=data-driven code-driven adapter"`
	DefaultConfig map[string]interface{} `json:"default_config"`
}

// AgentInstanceDetails provides detailed info about an agent instance
type AgentInstanceDetails struct {
	ID           string                 `json:"id"`
	TemplateID   string                 `json:"template_id"`
	TemplateName string                 `json:"template_name"`
	ClientID     string                 `json:"client_id"`
	OwnerUserID  string                 `json:"owner_user_id"`
	Name         string                 `json:"name"`
	Config       map[string]interface{} `json:"config"`
	IsActive     bool                   `json:"is_active"`
	CreatedAt    time.Time              `json:"created_at"`
	UpdatedAt    time.Time              `json:"updated_at"`
	Usage        AgentUsageStats        `json:"usage"`
	Health       AgentHealthStatus      `json:"health"`
}

// AgentUsageStats tracks usage metrics for an agent
type AgentUsageStats struct {
	TotalExecutions   int        `json:"total_executions"`
	SuccessfulRuns    int        `json:"successful_runs"`
	FailedRuns        int        `json:"failed_runs"`
	AverageRunTime    float64    `json:"average_run_time_ms"`
	LastExecutionTime *time.Time `json:"last_execution_time,omitempty"`
	FuelConsumed      int64      `json:"fuel_consumed"`
}

// AgentHealthStatus represents current health of an agent
type AgentHealthStatus struct {
	Status        string    `json:"status"` // healthy, degraded, unhealthy
	LastCheckTime time.Time `json:"last_check_time"`
	ErrorRate     float64   `json:"error_rate_percent"`
	ResponseTime  float64   `json:"avg_response_time_ms"`
	QueueDepth    int       `json:"queue_depth"`
}

// HandleCreateAgentDefinition creates a new agent type
func (h *AgentHandlers) HandleCreateAgentDefinition(c *gin.Context) {
	var req AgentDefinitionRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	// Check if type already exists
	var exists bool
	err := h.clientsDB.QueryRow(c.Request.Context(),
		"SELECT EXISTS(SELECT 1 FROM agent_definitions WHERE type = $1 AND deleted_at IS NULL)",
		req.Type,
	).Scan(&exists)

	if err != nil {
		h.logger.Error("Failed to check agent existence", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to check agent existence"})
		return
	}

	if exists {
		c.JSON(http.StatusConflict, gin.H{"error": "Agent type already exists"})
		return
	}

	// Insert new agent definition
	id := uuid.New()
	configBytes, _ := json.Marshal(req.DefaultConfig)

	query := `
		INSERT INTO agent_definitions 
		(id, type, display_name, description, category, default_config, is_active)
		VALUES ($1, $2, $3, $4, $5, $6, true)
	`

	_, err = h.clientsDB.Exec(c.Request.Context(), query,
		id, req.Type, req.DisplayName, req.Description, req.Category, configBytes,
	)

	if err != nil {
		h.logger.Error("Failed to create agent definition", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to create agent definition"})
		return
	}

	// Create Kafka topics for the new agent type
	h.createAgentTopics(c.Request.Context(), req.Type)

	h.logger.Info("Agent definition created",
		zap.String("id", id.String()),
		zap.String("type", req.Type))

	c.JSON(http.StatusCreated, gin.H{
		"id":      id.String(),
		"type":    req.Type,
		"message": "Agent definition created successfully",
	})
}

// HandleListAgentInstances lists all agent instances with filtering
func (h *AgentHandlers) HandleListAgentInstances(c *gin.Context) {
	clientID := c.Query("client_id")
	agentType := c.Query("agent_type")
	isActive := c.Query("is_active")

	instances := []AgentInstanceDetails{}

	// Get all client schemas
	schemaRows, err := h.clientsDB.Query(c.Request.Context(), `
		SELECT schema_name 
		FROM information_schema.schemata 
		WHERE schema_name LIKE 'client_%'
	`)
	if err != nil {
		h.logger.Error("Failed to list schemas", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to list instances"})
		return
	}
	defer schemaRows.Close()

	for schemaRows.Next() {
		var schemaName string
		if err := schemaRows.Scan(&schemaName); err != nil {
			continue
		}

		currentClientID := strings.TrimPrefix(schemaName, "client_")
		if clientID != "" && currentClientID != clientID {
			continue
		}

		// Query instances from this client schema
		query := fmt.Sprintf(`
			SELECT 
				ai.id, ai.template_id, ai.owner_user_id, ai.name, 
				ai.config, ai.is_active, ai.created_at, ai.updated_at,
				pt.name as template_name
			FROM %s.agent_instances ai
			LEFT JOIN persona_templates pt ON ai.template_id = pt.id
			WHERE 1=1
		`, schemaName)

		args := []interface{}{}
		argCount := 0

		if agentType != "" {
			argCount++
			query += fmt.Sprintf(" AND pt.category = $%d", argCount)
			args = append(args, agentType)
		}

		if isActive != "" {
			argCount++
			query += fmt.Sprintf(" AND ai.is_active = $%d", argCount)
			args = append(args, isActive == "true")
		}

		rows, err := h.clientsDB.Query(c.Request.Context(), query, args...)
		if err != nil {
			h.logger.Error("Failed to query instances", zap.Error(err))
			continue
		}

		for rows.Next() {
			var instance AgentInstanceDetails
			var configJSON []byte
			var templateName sql.NullString

			err := rows.Scan(
				&instance.ID, &instance.TemplateID, &instance.OwnerUserID,
				&instance.Name, &configJSON, &instance.IsActive,
				&instance.CreatedAt, &instance.UpdatedAt, &templateName,
			)
			if err != nil {
				continue
			}

			instance.ClientID = currentClientID
			if templateName.Valid {
				instance.TemplateName = templateName.String
			}
			json.Unmarshal(configJSON, &instance.Config)

			// Get usage stats
			instance.Usage = h.getAgentUsageStats(c.Request.Context(), currentClientID, instance.ID)
			instance.Health = h.getAgentHealth(c.Request.Context(), instance.ID)

			instances = append(instances, instance)
		}
		rows.Close()
	}

	c.JSON(http.StatusOK, gin.H{
		"instances": instances,
		"count":     len(instances),
	})
}

// HandleGetAgentInstance returns detailed information about a specific agent
func (h *AgentHandlers) HandleGetAgentInstance(c *gin.Context) {
	agentID := c.Param("agent_id")
	clientID := c.Param("client_id")

	if clientID == "" {
		// Need to find which client owns this agent
		clientID = h.findClientForAgent(c.Request.Context(), agentID)
		if clientID == "" {
			c.JSON(http.StatusNotFound, gin.H{"error": "Agent not found"})
			return
		}
	}

	query := fmt.Sprintf(`
		SELECT 
			ai.id, ai.template_id, ai.owner_user_id, ai.name, 
			ai.config, ai.is_active, ai.created_at, ai.updated_at,
			pt.name as template_name
		FROM client_%s.agent_instances ai
		LEFT JOIN persona_templates pt ON ai.template_id = pt.id
		WHERE ai.id = $1
	`, clientID)

	var instance AgentInstanceDetails
	var configJSON []byte
	var templateName sql.NullString

	err := h.clientsDB.QueryRow(c.Request.Context(), query, agentID).Scan(
		&instance.ID, &instance.TemplateID, &instance.OwnerUserID,
		&instance.Name, &configJSON, &instance.IsActive,
		&instance.CreatedAt, &instance.UpdatedAt, &templateName,
	)

	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": "Agent not found"})
		return
	}

	instance.ClientID = clientID
	if templateName.Valid {
		instance.TemplateName = templateName.String
	}
	json.Unmarshal(configJSON, &instance.Config)

	// Get detailed usage and health
	instance.Usage = h.getAgentUsageStats(c.Request.Context(), clientID, agentID)
	instance.Health = h.getAgentHealth(c.Request.Context(), agentID)

	// Get recent executions
	executions := h.getRecentExecutions(c.Request.Context(), clientID, agentID, 10)

	c.JSON(http.StatusOK, gin.H{
		"agent":      instance,
		"executions": executions,
	})
}

// HandleToggleAgentStatus enables/disables an agent instance
func (h *AgentHandlers) HandleToggleAgentStatus(c *gin.Context) {
	agentID := c.Param("agent_id")
	clientID := c.Param("client_id")

	if clientID == "" {
		clientID = h.findClientForAgent(c.Request.Context(), agentID)
		if clientID == "" {
			c.JSON(http.StatusNotFound, gin.H{"error": "Agent not found"})
			return
		}
	}

	var req struct {
		IsActive bool   `json:"is_active"`
		Reason   string `json:"reason"`
	}

	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
		return
	}

	query := fmt.Sprintf(`
		UPDATE client_%s.agent_instances 
		SET is_active = $2, updated_at = NOW()
		WHERE id = $1
	`, clientID)

	result, err := h.clientsDB.Exec(c.Request.Context(), query, agentID, req.IsActive)
	if err != nil {
		h.logger.Error("Failed to toggle agent status", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to update agent status"})
		return
	}

	if result.RowsAffected() == 0 {
		c.JSON(http.StatusNotFound, gin.H{"error": "Agent not found"})
		return
	}

	// Log the action
	h.logger.Info("Agent status toggled",
		zap.String("agent_id", agentID),
		zap.String("client_id", clientID),
		zap.Bool("is_active", req.IsActive),
		zap.String("reason", req.Reason))

	// Send notification if disabling
	if !req.IsActive {
		h.notifyAgentDisabled(c.Request.Context(), clientID, agentID, req.Reason)
	}

	c.JSON(http.StatusOK, gin.H{
		"message":   fmt.Sprintf("Agent %s successfully", map[bool]string{true: "enabled", false: "disabled"}[req.IsActive]),
		"agent_id":  agentID,
		"is_active": req.IsActive,
	})
}

// HandleRestartAgent sends a restart command to an agent
func (h *AgentHandlers) HandleRestartAgent(c *gin.Context) {
	agentID := c.Param("agent_id")

	// Send restart command via Kafka
	command := map[string]interface{}{
		"command":   "restart",
		"agent_id":  agentID,
		"timestamp": time.Now().UTC(),
	}

	commandBytes, _ := json.Marshal(command)
	headers := map[string]string{
		"correlation_id": uuid.NewString(),
		"command_type":   "agent_restart",
		"agent_id":       agentID,
	}

	err := h.kafkaProducer.Produce(c.Request.Context(),
		"system.agent.commands",
		headers,
		[]byte(agentID),
		commandBytes,
	)

	if err != nil {
		h.logger.Error("Failed to send restart command", zap.Error(err))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to send restart command"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"message":  "Restart command sent",
		"agent_id": agentID,
	})
}

// Helper methods

func (h *AgentHandlers) createAgentTopics(ctx context.Context, agentType string) {
	topics := []string{
		fmt.Sprintf("tasks.high.%s", agentType),
		fmt.Sprintf("tasks.normal.%s", agentType),
		fmt.Sprintf("tasks.low.%s", agentType),
		fmt.Sprintf("responses.%s", agentType),
		fmt.Sprintf("dlq.%s", agentType),
	}

	for _, topic := range topics {
		// In production, you'd use Kafka admin client
		h.logger.Info("Would create Kafka topic", zap.String("topic", topic))
	}
}

func (h *AgentHandlers) findClientForAgent(ctx context.Context, agentID string) string {
	// Search through all client schemas
	rows, err := h.clientsDB.Query(ctx, `
		SELECT schema_name 
		FROM information_schema.schemata 
		WHERE schema_name LIKE 'client_%'
	`)
	if err != nil {
		return ""
	}
	defer rows.Close()

	for rows.Next() {
		var schemaName string
		if err := rows.Scan(&schemaName); err != nil {
			continue
		}

		var exists bool
		query := fmt.Sprintf("SELECT EXISTS(SELECT 1 FROM %s.agent_instances WHERE id = $1)", schemaName)
		if err := h.clientsDB.QueryRow(ctx, query, agentID).Scan(&exists); err == nil && exists {
			return strings.TrimPrefix(schemaName, "client_")
		}
	}

	return ""
}

func (h *AgentHandlers) getAgentUsageStats(ctx context.Context, clientID, agentID string) AgentUsageStats {
	stats := AgentUsageStats{}

	// Get execution stats from workflow_executions
	query := fmt.Sprintf(`
		SELECT 
			COUNT(*) as total,
			COUNT(*) FILTER (WHERE status = 'COMPLETED') as successful,
			COUNT(*) FILTER (WHERE status = 'FAILED') as failed,
			AVG(EXTRACT(EPOCH FROM (completed_at - started_at)) * 1000) as avg_runtime,
			MAX(completed_at) as last_execution
		FROM client_%s.workflow_executions
		WHERE agent_instance_id = $1
	`, clientID)

	var lastExecution sql.NullTime
	h.clientsDB.QueryRow(ctx, query, agentID).Scan(
		&stats.TotalExecutions,
		&stats.SuccessfulRuns,
		&stats.FailedRuns,
		&stats.AverageRunTime,
		&lastExecution,
	)

	if lastExecution.Valid {
		stats.LastExecutionTime = &lastExecution.Time
	}

	// Get fuel consumption
	fuelQuery := fmt.Sprintf(`
		SELECT COALESCE(SUM(fuel_consumed), 0)
		FROM client_%s.usage_analytics
		WHERE metadata->>'agent_id' = $1
	`, clientID)
	h.clientsDB.QueryRow(ctx, fuelQuery, agentID).Scan(&stats.FuelConsumed)

	return stats
}

func (h *AgentHandlers) getAgentHealth(ctx context.Context, agentID string) AgentHealthStatus {
	// In production, this would query your metrics system
	return AgentHealthStatus{
		Status:        "healthy",
		LastCheckTime: time.Now(),
		ErrorRate:     2.5,
		ResponseTime:  145.3,
		QueueDepth:    3,
	}
}

func (h *AgentHandlers) getRecentExecutions(ctx context.Context, clientID, agentID string, limit int) []map[string]interface{} {
	executions := []map[string]interface{}{}

	query := fmt.Sprintf(`
		SELECT 
			id, correlation_id, status, started_at, completed_at,
			input_data, output_data, error_message
		FROM client_%s.workflow_executions
		WHERE agent_instance_id = $1
		ORDER BY started_at DESC
		LIMIT $2
	`, clientID)

	rows, err := h.clientsDB.Query(ctx, query, agentID, limit)
	if err != nil {
		return executions
	}
	defer rows.Close()

	for rows.Next() {
		var exec struct {
			ID            string
			CorrelationID string
			Status        string
			StartedAt     time.Time
			CompletedAt   sql.NullTime
			InputData     json.RawMessage
			OutputData    json.RawMessage
			ErrorMessage  sql.NullString
		}

		if err := rows.Scan(&exec.ID, &exec.CorrelationID, &exec.Status,
			&exec.StartedAt, &exec.CompletedAt, &exec.InputData,
			&exec.OutputData, &exec.ErrorMessage); err != nil {
			continue
		}

		execution := map[string]interface{}{
			"id":             exec.ID,
			"correlation_id": exec.CorrelationID,
			"status":         exec.Status,
			"started_at":     exec.StartedAt,
			"duration_ms":    nil,
		}

		if exec.CompletedAt.Valid {
			duration := exec.CompletedAt.Time.Sub(exec.StartedAt).Milliseconds()
			execution["completed_at"] = exec.CompletedAt.Time
			execution["duration_ms"] = duration
		}

		if exec.ErrorMessage.Valid {
			execution["error"] = exec.ErrorMessage.String
		}

		executions = append(executions, execution)
	}

	return executions
}

func (h *AgentHandlers) notifyAgentDisabled(ctx context.Context, clientID, agentID, reason string) {
	notification := map[string]interface{}{
		"event_type": "AGENT_DISABLED",
		"client_id":  clientID,
		"agent_id":   agentID,
		"reason":     reason,
		"timestamp":  time.Now().UTC(),
	}

	notificationBytes, _ := json.Marshal(notification)
	headers := map[string]string{
		"correlation_id": uuid.NewString(),
		"event_type":     "agent_disabled",
	}

	h.kafkaProducer.Produce(ctx, "system.notifications.admin", headers,
		[]byte(agentID), notificationBytes)
}

// HandleUpdateInstanceConfig updates the configuration of a specific agent instance.
func (h *AgentHandlers) HandleUpdateInstanceConfig(c *gin.Context) {
	agentID := c.Param("agent_id")
	clientID := c.Param("client_id")

	var req struct {
		Config map[string]interface{} `json:"config" binding:"required"`
	}
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request body: " + err.Error()})
		return
	}

	// Now you can call the repository method directly from the handler
	err := h.personaRepo.AdminUpdateInstanceConfig(c.Request.Context(), clientID, agentID, req.Config)
	if err != nil {
		h.logger.Error("Failed to update instance config", zap.Error(err), zap.String("agent_id", agentID))
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to update instance config"})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"message":   "Configuration updated successfully.",
		"agent_id":  agentID,
		"client_id": clientID,
	})
}
-------------------------------------------------
filepath = ./internal/core-manager/middleware/auth.go
// FILE: internal/core-manager/middleware/auth.go
package middleware

import (
	"encoding/json"
	"fmt"
	"net/http"
	"os"
	"strings"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/golang-jwt/jwt/v5"
	"github.com/gqls/agentchassis/platform/config"
	"go.uber.org/zap"
)

// AuthClaims represents the JWT claims
type AuthClaims struct {
	UserID      string   `json:"user_id"`
	Email       string   `json:"email"`
	ClientID    string   `json:"client_id"`
	Role        string   `json:"role"`
	Tier        string   `json:"tier"`
	Permissions []string `json:"permissions,omitempty"`
	jwt.RegisteredClaims
}

// AuthMiddlewareConfig holds configuration for auth middleware
type AuthMiddlewareConfig struct {
	JWTSecret      []byte
	AuthServiceURL string
	Logger         *zap.Logger
}

// NewAuthMiddlewareConfig creates auth middleware configuration from service config
func NewAuthMiddlewareConfig(cfg *config.ServiceConfig, logger *zap.Logger) (*AuthMiddlewareConfig, error) {
	// Get JWT secret from environment
	jwtSecretEnvVar := "JWT_SECRET_KEY"
	if cfg.Custom != nil {
		if envVar, ok := cfg.Custom["jwt_secret_env_var"].(string); ok {
			jwtSecretEnvVar = envVar
		}
	}

	jwtSecret := os.Getenv(jwtSecretEnvVar)
	if jwtSecret == "" {
		return nil, fmt.Errorf("JWT secret not found in environment variable %s", jwtSecretEnvVar)
	}

	// Get auth service URL
	authServiceURL := "http://auth-service:8081"
	if cfg.Custom != nil {
		if url, ok := cfg.Custom["auth_service_url"].(string); ok {
			authServiceURL = url
		}
	}

	return &AuthMiddlewareConfig{
		JWTSecret:      []byte(jwtSecret),
		AuthServiceURL: authServiceURL,
		Logger:         logger,
	}, nil
}

// AuthMiddleware validates JWT tokens
func AuthMiddleware(config *AuthMiddlewareConfig) gin.HandlerFunc {
	return func(c *gin.Context) {
		authHeader := c.GetHeader("Authorization")
		if authHeader == "" {
			// Also check X-Authorization header (for proxied requests)
			authHeader = c.GetHeader("X-Authorization")
		}

		if authHeader == "" {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Authorization header required"})
			c.Abort()
			return
		}

		tokenString := strings.Replace(authHeader, "Bearer ", "", 1)

		// Parse and validate token
		token, err := jwt.ParseWithClaims(tokenString, &AuthClaims{}, func(token *jwt.Token) (interface{}, error) {
			// Validate signing method
			if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
				return nil, fmt.Errorf("unexpected signing method: %v", token.Header["alg"])
			}
			return config.JWTSecret, nil
		})

		if err != nil {
			config.Logger.Debug("Token validation failed", zap.Error(err))

			// Optionally validate with auth service
			if isValid := validateWithAuthService(config, tokenString); !isValid {
				c.JSON(http.StatusUnauthorized, gin.H{"error": "Invalid token"})
				c.Abort()
				return
			}
		}

		if claims, ok := token.Claims.(*AuthClaims); ok && token.Valid {
			// Set user context
			c.Set("user_claims", claims)
			c.Set("user_id", claims.UserID)
			c.Set("client_id", claims.ClientID)
			c.Set("user_email", claims.Email)
			c.Set("user_role", claims.Role)
			c.Set("user_tier", claims.Tier)
			c.Set("user_permissions", claims.Permissions)
			c.Next()
		} else {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "Invalid token claims"})
			c.Abort()
			return
		}
	}
}

// validateWithAuthService validates token with auth service (optional fallback)
func validateWithAuthService(config *AuthMiddlewareConfig, token string) bool {
	client := &http.Client{Timeout: 5 * time.Second}

	req, err := http.NewRequest("POST", config.AuthServiceURL+"/api/v1/auth/validate", nil)
	if err != nil {
		config.Logger.Error("Failed to create validation request", zap.Error(err))
		return false
	}

	req.Header.Set("Authorization", "Bearer "+token)

	resp, err := client.Do(req)
	if err != nil {
		config.Logger.Error("Failed to validate with auth service", zap.Error(err))
		return false
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return false
	}

	var result struct {
		Valid bool `json:"valid"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return false
	}

	return result.Valid
}

// TenantMiddleware sets up tenant context
func TenantMiddleware(logger *zap.Logger) gin.HandlerFunc {
	return func(c *gin.Context) {
		claims, exists := c.Get("user_claims")
		if !exists {
			c.JSON(http.StatusUnauthorized, gin.H{"error": "No user claims found"})
			c.Abort()
			return
		}

		authClaims, ok := claims.(*AuthClaims)
		if !ok {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Invalid claims type"})
			c.Abort()
			return
		}

		// Set tenant context for database operations
		c.Set("client_id", authClaims.ClientID)
		c.Set("user_id", authClaims.UserID)

		c.Next()
	}
}

// AdminOnly restricts access to admin users
func AdminOnly() gin.HandlerFunc {
	return func(c *gin.Context) {
		role, exists := c.Get("user_role")
		if !exists {
			c.JSON(http.StatusForbidden, gin.H{"error": "No role found"})
			c.Abort()
			return
		}

		if role != "admin" {
			c.JSON(http.StatusForbidden, gin.H{"error": "Admin access required"})
			c.Abort()
			return
		}

		c.Next()
	}
}
-------------------------------------------------
filepath = ./internal/core-manager/database/personas.go
// FILE: internal/core-manager/database/personas.go

package database

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

type PersonaRepository struct {
	templatesDB *pgxpool.Pool
	clientsDB   *pgxpool.Pool
	logger      *zap.Logger
}

// NewPersonaRepository creates a new repository instance
func NewPersonaRepository(templatesDB, clientsDB *pgxpool.Pool, logger *zap.Logger) *PersonaRepository {
	return &PersonaRepository{
		templatesDB: templatesDB,
		clientsDB:   clientsDB,
		logger:      logger,
	}
}

// ClientsDB returns the clients database pool
func (r *PersonaRepository) ClientsDB() *pgxpool.Pool {
	return r.clientsDB
}

// TemplatesDB returns the templates database pool
func (r *PersonaRepository) TemplatesDB() *pgxpool.Pool {
	return r.templatesDB
}

// Template Methods

func (r *PersonaRepository) CreateTemplate(ctx context.Context, template *models.Persona) (*models.Persona, error) {
	r.logger.Info("Creating new persona template", zap.String("name", template.Name))

	configJSON, _ := json.Marshal(template.Config)

	query := `
        INSERT INTO persona_templates (id, name, description, category, config, is_active, created_at, updated_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
        RETURNING id
    `

	err := r.templatesDB.QueryRow(ctx, query,
		template.ID,
		template.Name,
		template.Description,
		template.Category,
		configJSON,
		true,
		template.CreatedAt,
		template.UpdatedAt,
	).Scan(&template.ID)

	if err != nil {
		r.logger.Error("Failed to create template", zap.Error(err))
		return nil, fmt.Errorf("failed to create template: %w", err)
	}

	return template, nil
}

func (r *PersonaRepository) GetTemplateByID(ctx context.Context, id string) (*models.Persona, error) {
	r.logger.Info("Getting template by ID", zap.String("id", id))

	var template models.Persona
	var configJSON []byte

	query := `
        SELECT id, name, description, category, config, is_active, created_at, updated_at
        FROM persona_templates
        WHERE id = $1 AND is_active = true
    `

	err := r.templatesDB.QueryRow(ctx, query, id).Scan(
		&template.ID,
		&template.Name,
		&template.Description,
		&template.Category,
		&configJSON,
		&template.IsActive,
		&template.CreatedAt,
		&template.UpdatedAt,
	)

	if err != nil {
		if err == pgx.ErrNoRows {
			return nil, fmt.Errorf("template not found")
		}
		return nil, fmt.Errorf("failed to get template: %w", err)
	}

	json.Unmarshal(configJSON, &template.Config)
	template.IsTemplate = true

	return &template, nil
}

func (r *PersonaRepository) ListTemplates(ctx context.Context) ([]models.Persona, error) {
	r.logger.Info("Listing all templates")

	query := `
        SELECT id, name, description, category, config, is_active, created_at, updated_at
        FROM persona_templates
        WHERE is_active = true
        ORDER BY category, name
    `

	rows, err := r.templatesDB.Query(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("failed to list templates: %w", err)
	}
	defer rows.Close()

	var templates []models.Persona
	for rows.Next() {
		var template models.Persona
		var configJSON []byte

		err := rows.Scan(
			&template.ID,
			&template.Name,
			&template.Description,
			&template.Category,
			&configJSON,
			&template.IsActive,
			&template.CreatedAt,
			&template.UpdatedAt,
		)
		if err != nil {
			r.logger.Error("Failed to scan template row", zap.Error(err))
			continue
		}

		json.Unmarshal(configJSON, &template.Config)
		template.IsTemplate = true
		templates = append(templates, template)
	}

	return templates, nil
}

func (r *PersonaRepository) UpdateTemplate(ctx context.Context, template *models.Persona) (*models.Persona, error) {
	r.logger.Info("Updating template", zap.String("id", template.ID.String()))

	configJSON, _ := json.Marshal(template.Config)

	query := `
        UPDATE persona_templates
        SET name = $2, description = $3, category = $4, config = $5, updated_at = $6
        WHERE id = $1
        RETURNING updated_at
    `

	err := r.templatesDB.QueryRow(ctx, query,
		template.ID,
		template.Name,
		template.Description,
		template.Category,
		configJSON,
		time.Now(),
	).Scan(&template.UpdatedAt)

	if err != nil {
		return nil, fmt.Errorf("failed to update template: %w", err)
	}

	return template, nil
}

func (r *PersonaRepository) DeleteTemplate(ctx context.Context, id string) error {
	r.logger.Info("Deleting template", zap.String("id", id))

	// Soft delete
	query := `UPDATE persona_templates SET is_active = false, updated_at = $2 WHERE id = $1`

	_, err := r.templatesDB.Exec(ctx, query, id, time.Now())
	if err != nil {
		return fmt.Errorf("failed to delete template: %w", err)
	}

	return nil
}

// Instance Methods

func (r *PersonaRepository) CreateInstanceFromTemplate(ctx context.Context, templateID string, userID string, instanceName string) (*models.Persona, error) {
	r.logger.Info("Creating instance from template",
		zap.String("templateID", templateID),
		zap.String("userID", userID))

	// Get client ID from context
	clientID, ok := ctx.Value("client_id").(string)
	if !ok {
		return nil, fmt.Errorf("client_id not found in context")
	}

	// First, fetch the template
	template, err := r.GetTemplateByID(ctx, templateID)
	if err != nil {
		return nil, fmt.Errorf("failed to fetch template: %w", err)
	}

	// Create the instance
	instance := &models.Persona{
		ID:          uuid.New(),
		Name:        instanceName,
		Description: template.Description,
		Category:    template.Category,
		Config:      template.Config,
		IsTemplate:  false,
		IsActive:    true,
		CreatedAt:   time.Now(),
		UpdatedAt:   time.Now(),
	}

	configJSON, _ := json.Marshal(instance.Config)

	// Use client-specific schema
	query := fmt.Sprintf(`
        INSERT INTO client_%s.agent_instances 
        (id, template_id, owner_user_id, name, config, is_active, created_at, updated_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
    `, clientID)

	_, err = r.clientsDB.Exec(ctx, query,
		instance.ID,
		templateID,
		userID,
		instance.Name,
		configJSON,
		true,
		instance.CreatedAt,
		instance.UpdatedAt,
	)

	if err != nil {
		return nil, fmt.Errorf("failed to create instance: %w", err)
	}

	return instance, nil
}

func (r *PersonaRepository) GetInstanceByID(ctx context.Context, id string) (*models.Persona, error) {
	clientID, _ := ctx.Value("client_id").(string)

	var instance models.Persona
	var configJSON []byte

	query := fmt.Sprintf(`
        SELECT id, name, config, is_active, created_at, updated_at
        FROM client_%s.agent_instances
        WHERE id = $1 AND is_active = true
    `, clientID)

	err := r.clientsDB.QueryRow(ctx, query, id).Scan(
		&instance.ID,
		&instance.Name,
		&configJSON,
		&instance.IsActive,
		&instance.CreatedAt,
		&instance.UpdatedAt,
	)

	if err != nil {
		return nil, fmt.Errorf("failed to get instance: %w", err)
	}

	json.Unmarshal(configJSON, &instance.Config)
	return &instance, nil
}

func (r *PersonaRepository) ListInstances(ctx context.Context, userID string) ([]models.Persona, error) {
	clientID, _ := ctx.Value("client_id").(string)

	query := fmt.Sprintf(`
        SELECT id, name, config, is_active, created_at, updated_at
        FROM client_%s.agent_instances
        WHERE owner_user_id = $1 AND is_active = true
        ORDER BY created_at DESC
    `, clientID)

	rows, err := r.clientsDB.Query(ctx, query, userID)
	if err != nil {
		return nil, fmt.Errorf("failed to list instances: %w", err)
	}
	defer rows.Close()

	var instances []models.Persona
	for rows.Next() {
		var instance models.Persona
		var configJSON []byte

		err := rows.Scan(
			&instance.ID,
			&instance.Name,
			&configJSON,
			&instance.IsActive,
			&instance.CreatedAt,
			&instance.UpdatedAt,
		)
		if err != nil {
			continue
		}

		json.Unmarshal(configJSON, &instance.Config)
		instances = append(instances, instance)
	}

	return instances, nil
}

func (r *PersonaRepository) UpdateInstance(ctx context.Context, id string, name *string, config map[string]interface{}) (*models.Persona, error) {
	clientID, _ := ctx.Value("client_id").(string)

	// First get the current instance
	instance, err := r.GetInstanceByID(ctx, id)
	if err != nil {
		return nil, err
	}

	// Update fields
	if name != nil {
		instance.Name = *name
	}
	if config != nil {
		// Merge configs
		for k, v := range config {
			instance.Config[k] = v
		}
	}

	configJSON, _ := json.Marshal(instance.Config)

	query := fmt.Sprintf(`
        UPDATE client_%s.agent_instances
        SET name = $2, config = $3, updated_at = $4
        WHERE id = $1
    `, clientID)

	_, err = r.clientsDB.Exec(ctx, query, id, instance.Name, configJSON, time.Now())
	if err != nil {
		return nil, fmt.Errorf("failed to update instance: %w", err)
	}

	return instance, nil
}

func (r *PersonaRepository) DeleteInstance(ctx context.Context, id string) error {
	clientID, _ := ctx.Value("client_id").(string)

	query := fmt.Sprintf(`
        UPDATE client_%s.agent_instances
        SET is_active = false, updated_at = $2
        WHERE id = $1
    `, clientID)

	_, err := r.clientsDB.Exec(ctx, query, id, time.Now())
	if err != nil {
		return fmt.Errorf("failed to delete instance: %w", err)
	}

	return nil
}

// AdminUpdateInstanceConfig allows an admin to update an instance's config
func (r *PersonaRepository) AdminUpdateInstanceConfig(ctx context.Context, clientID, instanceID string, config map[string]interface{}) error {
	r.logger.Info("Admin updating instance config", zap.String("instance_id", instanceID), zap.String("client_id", clientID))

	configJSON, err := json.Marshal(config)
	if err != nil {
		return fmt.Errorf("failed to marshal config: %w", err)
	}

	query := fmt.Sprintf(`
        UPDATE client_%s.agent_instances
        SET config = $2, updated_at = NOW()
        WHERE id = $1
    `, clientID)

	res, err := r.clientsDB.Exec(ctx, query, instanceID, configJSON)
	if err != nil {
		return fmt.Errorf("failed to execute update: %w", err)
	}

	if res.RowsAffected() == 0 {
		return fmt.Errorf("instance not found")
	}

	return nil
}
-------------------------------------------------
filepath = ./build/scripts/build-images.sh
-------------------------------------------------
filepath = ./build/scripts/push-images.sh
-------------------------------------------------
filepath = ./build/docker/frontend/react-nginx.dockerfile
-------------------------------------------------
filepath = ./build/docker/frontend/react-dev.dockerfile
-------------------------------------------------
filepath = ./build/docker/backend/seeder.dockerfile
# FILE: docker/Dockerfile.seeder
FROM alpine:latest

# Install PostgreSQL client and MySQL client
RUN apk add --no-cache \
    postgresql16-client \
    mysql-client \
    bash \
    curl \
    jq

# Create app directory
WORKDIR /app

# Copy seeding scripts and data files
COPY docker/scripts/seed-data.sh /app/
COPY docker/data/ /app/data/
COPY docker/scripts/wait-for-services.sh /app/

# Make scripts executable
RUN chmod +x /app/seed-data.sh /app/wait-for-services.sh

# Set default command
CMD ["/app/seed-data.sh"]-------------------------------------------------
filepath = ./build/docker/backend/web-search-adapter.dockerfile
// FILE: Dockerfile.web-search-adapter
FROM golang:1.21-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o web-search-adapter ./cmd/web-search-adapter

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/web-search-adapter /app/
COPY configs/web-search-adapter.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./web-search-adapter", "-config", "configs/web-search-adapter.yaml"]
-------------------------------------------------
filepath = ./build/docker/backend/reasoning-agent.dockerfile
-------------------------------------------------
filepath = ./build/docker/backend/image-generator-adapter.dockerfile
/ FILE: Dockerfile.image-generator-adapter
FROM golang:1.21-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o image-generator-adapter ./cmd/image-generator-adapter

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/image-generator-adapter /app/
COPY configs/image-adapter.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./image-generator-adapter", "-config", "configs/image-adapter.yaml"]-------------------------------------------------
filepath = ./build/docker/backend/migrator.dockerfile
# FILE: docker/Dockerfile.migrator
FROM alpine:latest

# Install PostgreSQL client and MySQL client
RUN apk add --no-cache \
    postgresql16-client \
    mysql-client \
    bash \
    curl

# Create app directory
WORKDIR /app

# Copy migration scripts and SQL files
COPY platform/database/migrations/ /app/migrations/
COPY docker/scripts/run-migrations.sh /app/
COPY docker/scripts/wait-for-services.sh /app/

# Make scripts executable
RUN chmod +x /app/run-migrations.sh /app/wait-for-services.sh

# Set default command
CMD ["/app/run-migrations.sh"]-------------------------------------------------
filepath = ./build/docker/backend/core-manager.dockerfile
// FILE: Dockerfile.core-manager
FROM golang:1.21-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o core-manager ./cmd/core-manager

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/core-manager /app/
COPY configs/core-manager.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./core-manager", "-config", "configs/core-manager.yaml"]-------------------------------------------------
filepath = ./build/docker/backend/platform.dockerfile
# FILE: Dockerfile.platform
# Base image with common dependencies
FROM golang:1.21-alpine AS base
RUN apk add --no-cache git ca-certificates
WORKDIR /app
-------------------------------------------------
filepath = ./build/docker/backend/auth-service.dockerfile
// FILE: Dockerfile.auth-service
FROM golang:1.21-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o auth-service ./cmd/auth-service

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/auth-service /app/
COPY configs/auth-service.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./auth-service", "-config", "configs/auth-service.yaml"]-------------------------------------------------
filepath = ./build/docker/backend/agent-chassis.dockerfile
# FILE: Dockerfile.agent-chassis
FROM golang:1.21-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o agent-chassis ./cmd/agent-chassis

FROM alpine:latest
RUN apk --no-cache add ca-certificates
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
WORKDIR /app
COPY --from=builder /app/agent-chassis /app/
COPY configs/agent-chassis.yaml /app/configs/
RUN chown -R appuser:appgroup /app
USER appuser
CMD ["./agent-chassis", "-config", "configs/agent-chassis.yaml"]-------------------------------------------------
filepath = ./.gitignore
-------------------------------------------------
filepath = ./.env.example
-------------------------------------------------
filepath = ./README.makefile.md


## Usage Examples:

```bash
# Development workflow
make dev-up                              # Start local environment
make test                                # Run tests
make build-auth-service                  # Build single service
make dev-logs                           # Check logs

# Full deployment
make full-deploy                        # Build, push, and deploy everything

# Deploy infrastructure only
make deploy-infrastructure              # Deploy all infrastructure components

# Deploy applications only
make quick-deploy                       # Deploy apps using existing images

# Individual service workflow
make auth-service                       # Build, push, and deploy auth-service
make logs-auth                         # Check auth-service logs
make rollback-auth-service             # Rollback if needed

# Frontend deployment
make build-frontends                   # Build all frontends
make deploy-admin-dashboard           # Deploy just admin dashboard

# Monitoring
make status                           # Check deployment status
make port-forward-grafana            # Access Grafana locally

# Different environments
make deploy-all ENVIRONMENT=staging REGION=us001
make deploy-all ENVIRONMENT=production REGION=uk001 IMAGE_TAG=v1.2.3


This Makefile provides:
1. **Organized sections** for different types of operations
2. **Individual targets** for each service/component
3. **Composite targets** for common workflows
4. **Environment flexibility** through variables
5. **Colored output** for better readability
6. **Help system** showing all available commands
7. **Safety features** for destructive operations-------------------------------------------------
filepath = ./Dockerfile
FROM ubuntu:latest
LABEL authors="ant"

ENTRYPOINT ["top", "-b"]-------------------------------------------------
filepath = ./deployments/terraform/modules/postgres-instance/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/postgres-instance/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/postgres-instance/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/mysql-instance/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/s3-buckets/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/variables.tf
variable "kustomize_path" {
  description = "The path to the Kustomize overlay to apply."
  type        = string
}

variable "service_name" {
  description = "The name of the Kubernetes deployment resource."
  type        = string
}

variable "namespace" {
  description = "The Kubernetes namespace to deploy into."
  type        = string
}

variable "image_tag" {
  description = "The Docker image tag to apply to the deployment."
  type        = string
  default     = "latest"
}

variable "image_repository" {
  description = "The Docker image repository (e.g., 'aqls/personae-auth-service')."
  type        = string
}

variable "config_sha" {
  description = "A hash of the service's config file to trigger updates."
  type        = string
  default     = ""
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/main.tf
resource "null_resource" "apply_kustomization" {
  triggers = {
    image_tag_trigger = var.image_tag
    config_sha_trigger = var.config_sha
  }

  provisioner "local-exec" {
    command = <<-EOT
      set -e
      echo "Applying Kustomize overlay at ${var.kustomize_path}"
      kubectl apply -k ${var.kustomize_path}

      echo "Setting image for deployment/${var.service_name} to ${var.image_repository}:${var.image_tag}"
      kubectl set image deployment/${var.service_name} ${var.service_name}=${var.image_repository}:${var.image_tag} -n ${var.namespace}

      echo "Waiting for rollout of deployment/${var.service_name}..."
      kubectl rollout status deployment/${var.service_name} -n ${var.namespace} --timeout=5m
    EOT
  }
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/kustomize-apply/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/variables.tf
variable "ingress_namespace" {
  description = "Namespace to deploy the NGINX Ingress controller into."
  type        = string
  default     = "ingress-nginx"
}

variable "helm_chart_version" {
  description = "Version of the ingress-nginx Helm chart to deploy."
  type        = string
  default     = "4.10.1" # Use a specific, known-good version
}

variable "helm_values_content" {
  description = "YAML content string for Helm values. Pass using file() function from root module."
  type        = string
  default     = ""
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/main.tf
resource "kubernetes_namespace" "ns" {
  metadata {
    name = var.ingress_namespace
  }
}

resource "helm_release" "ingress_nginx" {
  name       = "ingress-nginx"
  repository = "https://kubernetes.github.io/ingress-nginx"
  chart      = "ingress-nginx"
  namespace  = kubernetes_namespace.ns.metadata[0].name
  version    = var.helm_chart_version

  # Pass the entire values file content. This is cleaner than many 'set' blocks.
  values = [var.helm_values_content]

  depends_on = [kubernetes_namespace.ns]
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/nginx-ingress/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/variables.tf
variable "operator_namespace" {
  description = "Namespace to deploy the Strimzi Kafka operator into."
  type        = string
}

variable "watched_namespaces_list" {
  description = "List of namespaces for the Strimzi operator to watch."
  type        = list(string)
}

variable "strimzi_yaml_source_path" {
  description = "Path to the directory containing the Strimzi YAML files to apply (e.g., ./strimzi-yaml-0.45.0/install/cluster-operator/)."
  type        = string
}

variable "operator_deployment_yaml_filename" {
  description = "Filename of the main operator deployment YAML within the strimzi_yaml_source_path (used for trigger)."
  type        = string
  default     = "060-Deployment-strimzi-cluster-operator.yaml"
}

variable "cluster_kubeconfig_path" {
  description = "Path to the kubeconfig file for the target Kubernetes cluster."
  type        = string
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/providers.tf
/*
provider "kubernetes" {
  config_path = "~/.kube/config_production_sydney"
}

provider "helm" {
  kubernetes {
    config_path = "~/.kube/config_production_sydney"
  }
}*/-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/main.tf
# This module assumes the namespaces (operator_namespace and watched_namespaces_list)
# are created by a separate configuration or exist.
# The main.tf in the instance directory (e.g., 030-strimzi-operator) will create them.

resource "null_resource" "apply_strimzi_operator_yaml" {
  triggers = {
    operator_deployment_sha1 = fileexists("${var.strimzi_yaml_source_path}/${var.operator_deployment_yaml_filename}") ? filesha1("${var.strimzi_yaml_source_path}/${var.operator_deployment_yaml_filename}") : ""
    watched_namespaces_trigger = join(",", var.watched_namespaces_list)
  }

  provisioner "local-exec" {
    command = "kubectl apply --namespace ${var.operator_namespace} --filename ${var.strimzi_yaml_source_path}/"
    environment = {
      KUBECONFIG = var.cluster_kubeconfig_path
    }
  }
}

-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/outputs.tf
output "operator_namespace_used" {
  description = "Namespace where the Strimzi operator was deployed."
  value       = var.operator_namespace
}

output "watched_namespaces_configured" {
  description = "Namespaces the Strimzi operator is configured to watch."
  value       = var.watched_namespaces_list
}-------------------------------------------------
filepath = ./deployments/terraform/modules/strimzi-operator/versions.tf

terraform {
  required_version = ">=1.0"
  required_providers {
    helm = { source = "hashicorp/helm", version = "~> 2.17.0" }
    kubernetes = { source = "hashicorp/kubernetes", version = "~> 2.36.0"}
    null = { source = "hashicorp/null", version = "~> 3.2.4" }
  }
}-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/main.tf
# This module applies the Kafka cluster custom resource YAML.
resource "null_resource" "apply_kafka_cluster_cr" {
  triggers = {
    yaml_file_sha1 = fileexists(var.kafka_cr_yaml_file_path) ? filesha1(var.kafka_cr_yaml_file_path) : ""
  }

  provisioner "local-exec" {
    command = "kubectl apply --namespace ${var.kafka_cr_namespace} -f ${var.kafka_cr_yaml_file_path}"
  }
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/kafka-cluster/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/k8s-job-runner/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/variables.tf
# The name for the Kubernetes cluster.
variable "cluster_name" {
  description = "Name for the Kubernetes cluster (Spot Cloudspace)."
  type        = string
}

# The Rackspace region for the cluster.
variable "rackspace_region" {
  description = "Rackspace region for the cluster (e.g., uk-lon-1)."
  type        = string
}

# The Kubernetes version.
variable "kubernetes_version" {
  description = "Kubernetes version for the cluster."
  type        = string
  default     = "1.28.8"
}

# A map defining multiple spot node pools. This is the key enhancement.
variable "spot_node_pools" {
  description = "A map of spot node pool configurations. Each key is a pool name."
  type = map(object({
    min_nodes = number
    max_nodes = number
    flavor    = string
    max_price = number
  }))
  default = {}
}

# Optional on-demand node configuration.
variable "ondemand_node_count" {
  description = "Number of on-demand worker nodes."
  type        = number
  default     = 0
}

variable "ondemand_node_flavor" {
  description = "Flavor for on-demand worker nodes."
  type        = string
  default     = null
}

# Slack webhook for preemption notices.
variable "preemption_webhook_url" {
  description = "Slack webhook URL for preemption notices."
  type        = string
  sensitive   = true
  default     = null
}

variable "cni" {
  description = "CNI plugin for the cluster."
  type        = string
  default     = "calico"
}

variable "hacontrol_plane" {
  description = "Enable HA control plane."
  type        = bool
  default     = false
}


variable "spot_min_nodes" {
  description = "Minimum number of spot worker nodes."
  type        = number
  default     = 4
}

variable "spot_max_nodes" {
  description = "Maximum number of spot worker nodes for autoscaling."
  type        = number
  default     = 5
}

variable "spot_node_flavor" {
  description = "Flavor (server class) for spot worker nodes."
  type        = string
}

variable "spot_max_price" {
  description = "Maximum bid price for spot instances."
  type        = number
  default     = 0.01
}

variable "ondemand_node_labels" {
  description = "Labels for on-demand worker nodes."
  type        = map(string)
  default = {
    "role"       = "general",
    "app.type"   = "stateful",
    "managed-by" = "terraform"
  }
}

variable "spot_node_labels" {
  description = "Labels for spot worker nodes."
  type        = map(string)
  default = {
    "role"       = "spot-instance",
    "app.type"   = "stateless",
    "managed-by" = "terraform"
  }
}

variable "ondemand_node_taints" {
  description = "Taints to apply to on-demand worker nodes. E.g., [{key=\"dedicated\", value=\"database\", effect=\"NoSchedule\"}]."
  type = list(object({
    key    = string
    value  = string
    effect = string
  }))
  default = []
}
-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/main.tf
# Main resource to create the Spot cloudspace (Kubernetes cluster).
resource "spot_cloudspace" "cluster" {
  cloudspace_name    = var.cluster_name
  region             = var.rackspace_region
  kubernetes_version = var.kubernetes_version
  preemption_webhook = var.preemption_webhook_url
  wait_until_ready   = true
}

# Creates an on-demand node pool if the count is greater than 0.
resource "spot_ondemandnodepool" "ondemand_pool" {
  count = var.ondemand_node_count > 0 ? 1 : 0

  cloudspace_name      = spot_cloudspace.cluster.cloudspace_name
  server_class         = var.ondemand_node_flavor
  desired_server_count = var.ondemand_node_count
  depends_on           = [spot_cloudspace.cluster]
}

# Creates multiple spot node pools based on the input map.
resource "spot_spotnodepool" "spot_pools" {
  for_each = var.spot_node_pools

  cloudspace_name = spot_cloudspace.cluster.cloudspace_name
  server_class    = each.value.flavor
  bid_price       = each.value.max_price
  autoscaling = {
    min_nodes = each.value.min_nodes
    max_nodes = each.value.max_nodes
  }
  labels = {
    "role"       = "spot-instance"
    "pool-name"  = each.key
    "managed-by" = "terraform"
  }
  depends_on = [spot_cloudspace.cluster]
}

# Data source to retrieve the kubeconfig after the cluster is ready.
data "spot_kubeconfig" "cluster_kubeconfig" {
  cloudspace_name = spot_cloudspace.cluster.cloudspace_name
  depends_on = [
    spot_cloudspace.cluster,
    spot_spotnodepool.spot_pools
  ]
}

-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/outputs.tf
output "kubeconfig_raw" {
  description = "Raw kubeconfig content for the cluster."
  value       = data.spot_kubeconfig.cluster_kubeconfig.raw
  sensitive   = true
}

output "cluster_name" {
  description = "Name of the created Kubernetes cluster."
  value       = spot_cloudspace.cluster.cloudspace_name
}

output "cluster_endpoint_actual" {
  description = "API endpoint for the Kubernetes cluster."
  value       = data.spot_kubeconfig.cluster_kubeconfig.kubeconfigs[0].host
  sensitive   = true
}-------------------------------------------------
filepath = ./deployments/terraform/modules/rackspace-kubernetes/versions.tf
terraform {
  required_providers {
    spot = {
      source  = "rackerlabs/spot"
      version = "~> 0.1.4"
    }
  }
  required_version = ">= 1.0"
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/local/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/development/local/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/020-ingress-nginx/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/070-database-schemas/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/070-database-schemas/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/070-database-schemas/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/080-kafka-topics/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/080-kafka-topics/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/080-kafka-topics/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/backend.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/main.tf
provider "spot" {
  token = var.rackspace_spot_token
}

module "kubernetes_cluster" {
  source = "../../../../modules/rackspace-kubernetes"

  cluster_name     = "personae-prod-uk001"
  rackspace_region = "uk-lon-1"
  preemption_webhook_url = var.slack_webhook_url

  # Define the mix of spot instance pools from your old .tfvars
  spot_node_pools = {
    "gp-large" = {
      min_nodes = 3
      max_nodes = 6
      flavor    = "gp.vs1.large-lon"
      max_price = 0.035
    },
    "mh-medium" = {
      min_nodes = 2
      max_nodes = 4
      flavor    = "mh.vs1.medium-lon"
      max_price = 0.030
    }
  }
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/010-infrastructure/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/090-monitoring/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/090-monitoring/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/090-monitoring/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/050-storage/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/050-storage/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/050-storage/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/050-storage/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/060-databases/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/060-databases/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/060-databases/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/060-databases/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/030-strimzi-operator/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/040-kafka-cluster/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2230-web-search-adapter/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2230-web-search-adapter/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2230-web-search-adapter/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2210-agent-chassis/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2210-agent-chassis/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2210-agent-chassis/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2240-image-generator-adapter/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2240-image-generator-adapter/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2240-image-generator-adapter/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2220-reasoning-agent/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2220-reasoning-agent/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/agents/2220-reasoning-agent/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3330-agent-playground/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3330-agent-playground/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3330-agent-playground/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3330-agent-playground/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3320-user-portal/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3320-user-portal/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3320-user-portal/main.tf
# This instance deploys the main user-facing React application.
module "user_frontend_app" {
  source = "../../../../../../modules/kustomize-apply"

  service_name     = "user-frontend"
  namespace        = "personae-system"
  image_repository = "aqls/personae-web-interface" # Your frontend image
  image_tag        = var.image_tag

  # Point to the production Kustomize overlay for the frontend.
  # This directory would contain the deployment.yaml, service.yaml, ingress.yaml, etc.
  kustomize_path = "../../../../../../../deployments/kustomize/frontends/user-frontend/overlays/production"
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3320-user-portal/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3310-admin-dashboard/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3310-admin-dashboard/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3310-admin-dashboard/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/frontends/3310-admin-dashboard/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1120-core-manager/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1120-core-manager/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1120-core-manager/main.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1120-core-manager/outputs.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1110-auth-service/terraform.tfvars
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1110-auth-service/variables.tf
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1110-auth-service/main.tf
# Create the ConfigMap for the auth-service, populating it from the root configs file.
resource "kubernetes_config_map_v1" "auth_service_config" {
  metadata {
    name      = "auth-service-config"
    namespace = "personae-system"
  }
  data = {
    "auth-service.yaml" = file("../../../../../../../configs/auth-service.yaml")
  }
}

# Create the secret for the auth-service.
resource "kubernetes_secret_v1" "auth_service_secrets" {
  metadata {
    name      = "auth-service-secrets"
    namespace = "personae-system"
  }
  data = {
    "AUTH_DB_PASSWORD"    = var.db_password
    "JWT_SECRET_KEY" = var.jwt_secret
  }
}

# Deploy the application using the generic kustomize-apply module.
module "auth_service_app" {
  source = "../../../../../../modules/kustomize-apply"

  service_name     = "auth-service"
  namespace        = "personae-system"
  image_repository = "aqls/personae-auth-service" # Your image repo
  image_tag        = var.image_tag

  # Point to the production Kustomize overlay for this service.
  kustomize_path = "../../../../../../../deployments/kustomize/services/auth-service/overlays/production"

  # Trigger a redeploy if the config file changes.
  config_sha = filesha1("../../../../../../../configs/auth-service.yaml")

  depends_on = [
    kubernetes_config_map_v1.auth_service_config,
    kubernetes_secret_v1.auth_service_secrets
  ]
}
-------------------------------------------------
filepath = ./deployments/terraform/environments/production/uk001/services/core-platform/1110-auth-service/outputs.tf
-------------------------------------------------
filepath = ./deployments/kustomize/base/configmap-common.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: common-config
  namespace: ai-persona-system
data:
  # Kafka configuration
  kafka_brokers: "kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092"

  # Database hosts
  clients_db_host: "postgres-clients"
  clients_db_port: "5432"
  clients_db_name: "clients_db"
  clients_db_user: "clients_user"

  templates_db_host: "postgres-templates"
  templates_db_port: "5432"
  templates_db_name: "templates_db"
  templates_db_user: "templates_user"

  auth_db_host: "mysql-auth"
  auth_db_port: "3306"
  auth_db_name: "auth_db"
  auth_db_user: "auth_user"

  # Object storage
  minio_endpoint: "http://minio:9000"
  minio_bucket: "agent-artifacts"

  # Service URLs
  core_manager_url: "http://core-manager:8088"
  auth_service_url: "http://auth-service:8081"

  # Observability
  tracing_endpoint: "otel-collector.monitoring.svc.cluster.local:4317"-------------------------------------------------
filepath = ./deployments/kustomize/base/network-policies.yaml
# Default deny all ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress

---
# Allow ingress from same namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector: {}

---
# Allow ingress from ingress controller
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-controller
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: auth-service
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8081

---
# Allow Prometheus scraping
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-prometheus
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090-------------------------------------------------
filepath = ./deployments/kustomize/base/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ai-persona-system
  labels:
    name: ai-persona-system
    monitoring: enabled-------------------------------------------------
filepath = ./deployments/kustomize/base/rbac-security.yaml
# FILE: k8s/rbac-security.yaml
# Service Account for applications
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ai-persona-app
  namespace: ai-persona-system
  labels:
    app: ai-persona-system

---
# Role for application access
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: ai-persona-system
  name: ai-persona-app-role
rules:
  # Allow reading secrets for configuration
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  # Allow reading configmaps
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list"]
  # Allow pod operations for health checks
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]

---
# Bind the role to the service account
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ai-persona-app-binding
  namespace: ai-persona-system
subjects:
  - kind: ServiceAccount
    name: ai-persona-app
    namespace: ai-persona-system
roleRef:
  kind: Role
  name: ai-persona-app-role
  apiGroup: rbac.authorization.k8s.io

---
# Pod Security Policy (if using older Kubernetes versions)
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: ai-persona-restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  runAsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'

---
# Network Policy - Default deny all ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress

---
# Network Policy - Allow same namespace communication
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector: {}

---
# Network Policy - Allow ingress controller access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-controller
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: auth-service
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8081

---
# Network Policy - Allow monitoring
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: ai-persona-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090

---
# Network Policy - Database access (only from specific apps)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: postgres-clients
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: core-manager
        - podSelector:
            matchLabels:
              app: agent-chassis
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 5432

---
# Network Policy - MySQL access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mysql-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: mysql-auth
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: auth-service
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 3306

---
# Network Policy - Kafka access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kafka-access-policy
  namespace: ai-persona-system
spec:
  podSelector:
    matchLabels:
      app: kafka
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: agent-chassis
        - podSelector:
            matchLabels:
              app: reasoning-agent
        - podSelector:
            matchLabels:
              app: image-generator-adapter
        - podSelector:
            matchLabels:
              app: web-search-adapter
        - podSelector:
            matchLabels:
              component: initialization
      ports:
        - protocol: TCP
          port: 9092-------------------------------------------------
filepath = ./deployments/kustomize/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/security/network-policy.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/security/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/monitoring/service-monitor.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/monitoring/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/components/monitoring/pod-monitor.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/kafka/kafka.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
  namespace: ai-persona-system
  labels:
    app: kafka
spec:
  ports:
    - port: 9092
      name: broker
    - port: 9093
      name: controller
  clusterIP: None
  selector:
    app: kafka

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-ui
  namespace: ai-persona-system
spec:
  ports:
    - port: 8080
      targetPort: 8080
  selector:
    app: kafka-ui
  type: ClusterIP

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: ai-persona-system
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:7.5.0
          ports:
            - containerPort: 9092
              name: broker
            - containerPort: 9093
              name: controller
          env:
            - name: KAFKA_NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_PROCESS_ROLES
              value: "broker,controller"
            - name: KAFKA_LISTENERS
              value: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
            - name: KAFKA_ADVERTISED_LISTENERS
              value: "PLAINTEXT://$(KAFKA_NODE_ID).kafka-headless:9092"
            - name: KAFKA_CONTROLLER_LISTENER_NAMES
              value: "CONTROLLER"
            - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
              value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
            - name: KAFKA_CONTROLLER_QUORUM_VOTERS
              value: "kafka-0@kafka-0.kafka-headless:9093,kafka-1@kafka-1.kafka-headless:9093,kafka-2@kafka-2.kafka-headless:9093"
            - name: KAFKA_LOG_DIRS
              value: "/var/lib/kafka/data"
            - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
              value: "false"
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "3"
            - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "3"
            - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
              value: "2"
            - name: KAFKA_DEFAULT_REPLICATION_FACTOR
              value: "3"
            - name: KAFKA_MIN_INSYNC_REPLICAS
              value: "2"
          volumeMounts:
            - name: kafka-storage
              mountPath: /var/lib/kafka/data
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
  volumeClaimTemplates:
    - metadata:
        name: kafka-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 10Gi

---
# Kafka UI for monitoring
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-ui
  namespace: ai-persona-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-ui
  template:
    metadata:
      labels:
        app: kafka-ui
    spec:
      containers:
        - name: kafka-ui
          image: provectuslabs/kafka-ui:latest
          ports:
            - containerPort: 8080
          env:
            - name: KAFKA_CLUSTERS_0_NAME
              value: "ai-persona-cluster"
            - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS
              value: "kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092"
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/kafka/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/prometheus.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: ai-persona-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - nodes/proxy
      - services
      - endpoints
      - pods
    verbs: ["get", "list", "watch"]
  - apiGroups:
      - extensions
    resources:
      - ingresses
    verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: ai-persona-system

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: ai-persona-system
  labels:
    app: prometheus
spec:
  ports:
    - port: 9090
      targetPort: 9090
      name: http
  selector:
    app: prometheus
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: ai-persona-system
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
        - name: prometheus
          image: prom/prometheus:latest
          args:
            - '--config.file=/etc/prometheus/prometheus.yml'
            - '--storage.tsdb.path=/prometheus/'
            - '--web.console.libraries=/usr/share/prometheus/console_libraries'
            - '--web.console.templates=/usr/share/prometheus/consoles'
            - '--web.enable-lifecycle'
          ports:
            - containerPort: 9090
              name: http
          volumeMounts:
            - name: prometheus-config
              mountPath: /etc/prometheus
            - name: prometheus-storage
              mountPath: /prometheus
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
      volumes:
        - name: prometheus-config
          configMap:
            name: prometheus-config
        - name: prometheus-storage
          emptyDir: {}-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/grafana-dashboard-configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/prometheus-config.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/monitoring/grafana.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/minio/minio.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: ai-persona-system
  labels:
    app: minio
spec:
  ports:
    - port: 9000
      targetPort: 9000
      name: api
    - port: 9001
      targetPort: 9001
      name: console
  selector:
    app: minio
  type: ClusterIP

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
  namespace: ai-persona-system
spec:
  serviceName: minio
  replicas: 1
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
        - name: minio
          image: minio/minio:latest
          command:
            - /bin/sh
            - -c
          args:
            - |
              mkdir -p /data/agent-artifacts
              minio server /data --console-address :9001
          ports:
            - containerPort: 9000
              name: api
            - containerPort: 9001
              name: console
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: access-key
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: secret-key
            - name: MINIO_BROWSER
              value: "on"
          volumeMounts:
            - name: minio-storage
              mountPath: /data
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /minio/health/ready
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 20
  volumeClaimTemplates:
    - metadata:
        name: minio-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 20Gi-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/minio/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/postgres-templates/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/postgres-templates/postgres-templates.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-templates
  namespace: ai-persona-system
  labels:
    app: postgres-templates
spec:
  ports:
    - port: 5432
      targetPort: 5432
      name: postgres
  selector:
    app: postgres-templates
  type: ClusterIP
  clusterIP: None

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-templates
  namespace: ai-persona-system
spec:
  serviceName: postgres-templates
  replicas: 1
  selector:
    matchLabels:
      app: postgres-templates
  template:
    metadata:
      labels:
        app: postgres-templates
    spec:
      containers:
        - name: postgres
          image: postgres:16-alpine
          ports:
            - containerPort: 5432
              name: postgres
          env:
            - name: POSTGRES_DB
              value: "templates_db"
            - name: POSTGRES_USER
              value: "templates_user"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
            - name: PGDATA
              value: /var/lib/postgresql/data/pgdata
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - templates_user
            initialDelaySeconds: 30
            periodSeconds: 10
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 5Gi-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/postgres-clients/postgres-clients.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-clients
  namespace: ai-persona-system
  labels:
    app: postgres-clients
spec:
  ports:
    - port: 5432
      targetPort: 5432
      name: postgres
  selector:
    app: postgres-clients
  type: ClusterIP
  clusterIP: None  # Headless service for StatefulSet

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-clients
  namespace: ai-persona-system
spec:
  serviceName: postgres-clients
  replicas: 1
  selector:
    matchLabels:
      app: postgres-clients
  template:
    metadata:
      labels:
        app: postgres-clients
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9187"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: postgres
          image: pgvector/pgvector:pg16
          ports:
            - containerPort: 5432
              name: postgres
          env:
            - name: POSTGRES_DB
              value: "clients_db"
            - name: POSTGRES_USER
              value: "clients_user"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: PGDATA
              value: /var/lib/postgresql/data/pgdata
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - clients_user
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - clients_user
            initialDelaySeconds: 5
            periodSeconds: 5
        - name: postgres-exporter
          image: prometheuscommunity/postgres-exporter:latest
          ports:
            - containerPort: 9187
              name: metrics
          env:
            - name: DATA_SOURCE_NAME
              value: "postgresql://clients_user:$(POSTGRES_PASSWORD)@localhost:5432/clients_db?sslmode=disable"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 10Gi-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/postgres-clients/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/mysql-auth/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/infrastructure/mysql-auth/mysql-auth.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-auth
  namespace: ai-persona-system
  labels:
    app: mysql-auth
spec:
  ports:
    - port: 3306
      targetPort: 3306
      name: mysql
  selector:
    app: mysql-auth
  type: ClusterIP
  clusterIP: None

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-auth
  namespace: ai-persona-system
spec:
  serviceName: mysql-auth
  replicas: 1
  selector:
    matchLabels:
      app: mysql-auth
  template:
    metadata:
      labels:
        app: mysql-auth
    spec:
      containers:
        - name: mysql
          image: mysql:8.0
          ports:
            - containerPort: 3306
              name: mysql
          env:
            - name: MYSQL_DATABASE
              value: "auth_db"
            - name: MYSQL_USER
              value: "auth_user"
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: auth-db-password
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: mysql-root-password
          volumeMounts:
            - name: mysql-storage
              mountPath: /var/lib/mysql
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          livenessProbe:
            exec:
              command:
                - mysqladmin
                - ping
                - -h
                - localhost
            initialDelaySeconds: 30
            periodSeconds: 10
  volumeClaimTemplates:
    - metadata:
        name: mysql-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "standard"
        resources:
          requests:
            storage: 5Gi-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: user-frontend-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - host: "personae.yourdomain.com" # This will be patched by the overlay
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: user-frontend
                port:
                  number: 80
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
resources:
  - deployment.yaml
  - service.yaml
  - ingress.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-frontend
  labels:
    app: user-frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: user-frontend
  template:
    metadata:
      labels:
        app: user-frontend
    spec:
      containers:
        - name: user-frontend
          image: user-frontend:IMAGE_TAG_PLACEHOLDER
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
              name: http
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: user-frontend
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: user-frontend
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/overlays/development/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/overlays/staging/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/overlays/production/ingress-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/user-portal/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/ingress.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/deployment.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/overlays/development/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/overlays/staging/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/overlays/production/ingress-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/admin-dashboard/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/ingress.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/deployment.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/overlays/development/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/frontends/agent-playground/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/kafka-topics/job.yaml
# FILE: k8s/jobs/kafka-topics-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-topics-init
  namespace: ai-persona-system
  labels:
    app: kafka-topics-init
    component: initialization
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: kafka-topics-init
        component: initialization
    spec:
      restartPolicy: Never
      initContainers:
        # Wait for Kafka to be ready
        - name: wait-for-kafka
          image: confluentinc/cp-kafka:7.5.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for Kafka to be ready..."
              until kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list >/dev/null 2>&1; do
                echo "Kafka not ready, waiting..."
                sleep 5
              done
              echo "Kafka is ready!"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

      containers:
        - name: topic-creator
          image: confluentinc/cp-kafka:7.5.0
          command:
            - /bin/bash
            - -c
            - |
              set -e
              echo "üîß Creating Kafka topics..."
              
              # Function to create topic with error handling
              create_topic() {
                local topic_name=$1
                local partitions=$2
                local replication_factor=$3
                local description=$4
              
                echo "Creating topic: $topic_name ($description)"
                kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 \
                  --create \
                  --topic "$topic_name" \
                  --partitions "$partitions" \
                  --replication-factor "$replication_factor" \
                  --if-not-exists || {
                  echo "Failed to create topic: $topic_name"
                  return 1
                }
              }
              
              # System-level topics
              echo "üì® Creating system topics..."
              create_topic "orchestrator.state-changes" 12 1 "Orchestrator state change notifications"
              create_topic "human.approvals" 6 1 "Human approval workflow messages"
              create_topic "system.events" 3 1 "General system events"
              create_topic "system.notifications.ui" 3 1 "UI notifications"
              create_topic "system.commands.workflow.resume" 3 1 "Workflow resume commands"
              
              # Agent communication topics
              echo "ü§ñ Creating agent communication topics..."
              create_topic "system.agent.reasoning.process" 6 1 "Reasoning agent requests"
              create_topic "system.responses.reasoning" 6 1 "Reasoning agent responses"
              create_topic "system.adapter.image.generate" 3 1 "Image generation requests"
              create_topic "system.responses.image" 6 1 "Image generation responses"
              create_topic "system.adapter.web.search" 3 1 "Web search requests"
              create_topic "system.responses.websearch" 6 1 "Web search responses"
              
              # Generic agent chassis topics
              echo "üèóÔ∏è Creating generic agent topics..."
              create_topic "system.agent.generic.process" 6 1 "Generic agent chassis requests"
              create_topic "system.tasks.copywriter" 6 1 "Copywriter agent tasks"
              create_topic "system.tasks.researcher" 6 1 "Research agent tasks"
              create_topic "system.tasks.content-creator" 6 1 "Content creator tasks"
              create_topic "system.tasks.multimedia-creator" 6 1 "Multimedia creator tasks"
              
              # Response topics for agents
              create_topic "system.responses.copywriter" 6 1 "Copywriter responses"
              create_topic "system.responses.researcher" 6 1 "Research responses"
              create_topic "system.responses.content-creator" 6 1 "Content creator responses"
              create_topic "system.responses.multimedia-creator" 6 1 "Multimedia creator responses"
              
              # Dead letter queues
              echo "üíÄ Creating dead letter queue topics..."
              create_topic "dlq.reasoning-agent" 1 1 "Reasoning agent DLQ"
              create_topic "dlq.image-generator" 1 1 "Image generator DLQ"
              create_topic "dlq.web-search" 1 1 "Web search DLQ"
              create_topic "dlq.agent-chassis" 1 1 "Agent chassis DLQ"
              create_topic "dlq.orchestrator" 1 1 "Orchestrator DLQ"
              
              # Monitoring and logging topics
              echo "üìä Creating monitoring topics..."
              create_topic "system.metrics.agents" 3 1 "Agent performance metrics"
              create_topic "system.logs.errors" 3 1 "Error logs aggregation"
              create_topic "system.audit.actions" 6 1 "Audit trail for user actions"
              
              echo "‚úÖ All Kafka topics created successfully!"
              
              # List all topics to verify
              echo "üìã Current topics:"
              kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list

          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "500m"-------------------------------------------------
filepath = ./deployments/kustomize/jobs/kafka-topics/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/data-seeder/job.yaml
# k8s/data-seeder-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: seed-default-data
  namespace: ai-persona-system
spec:
  template:
    spec:
      containers:
        - name: seeder
          image: ai-persona-system/data-seeder:latest
          env:
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password-------------------------------------------------
filepath = ./deployments/kustomize/jobs/data-seeder/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/schema-creator/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: create-client-schema
  namespace: ai-persona-system
spec:
  template:
    spec:
      containers:
        - name: schema-creator
          image: ai-persona-system/schema-creator:latest
          env:
            - name: CLIENT_ID
              value: "{{ .Values.clientId }}"
          command: ['/app/create-client-schema.sh']-------------------------------------------------
filepath = ./deployments/kustomize/jobs/schema-creator/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/jobs/database-init/job.yaml
# FILE: k8s/jobs/database-init-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: database-init
  namespace: ai-persona-system
  labels:
    app: database-init
    component: initialization
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: database-init
        component: initialization
    spec:
      restartPolicy: Never
      initContainers:
        # Wait for PostgreSQL clients database
        - name: wait-for-postgres-clients
          image: postgres:16-alpine
          command:
            - sh
            - -c
            - |
              echo "Waiting for PostgreSQL clients database..."
              until pg_isready -h postgres-clients -p 5432; do
                echo "PostgreSQL clients not ready, waiting..."
                sleep 2
              done
              echo "PostgreSQL clients is ready!"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Wait for PostgreSQL templates database
        - name: wait-for-postgres-templates
          image: postgres:16-alpine
          command:
            - sh
            - -c
            - |
              echo "Waiting for PostgreSQL templates database..."
              until pg_isready -h postgres-templates -p 5432; do
                echo "PostgreSQL templates not ready, waiting..."
                sleep 2
              done
              echo "PostgreSQL templates is ready!"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Wait for MySQL auth database
        - name: wait-for-mysql-auth
          image: mysql:8.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for MySQL auth database..."
              until mysqladmin ping -h mysql-auth --silent; do
                echo "MySQL auth not ready, waiting..."
                sleep 2
              done
              echo "MySQL auth is ready!"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

      containers:
        - name: database-migrator
          image: ai-persona-system/database-migrator:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
            - name: AUTH_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: auth-db-password
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: mysql-root-password
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          command: ["/app/run-migrations.sh"]

---
apiVersion: batch/v1
kind: Job
metadata:
  name: data-seeder
  namespace: ai-persona-system
  labels:
    app: data-seeder
    component: initialization
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: data-seeder
        component: initialization
    spec:
      restartPolicy: Never
      containers:
        - name: data-seeder
          image: ai-persona-system/data-seeder:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
            - name: AUTH_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: auth-db-password
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          command: ["/app/seed-data.sh"]-------------------------------------------------
filepath = ./deployments/kustomize/jobs/database-init/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/base/deployment.yaml
# FILE: k8s/agent-chassis.yaml (Updated)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agent-chassis
  namespace: ai-persona-system
  labels:
    app: agent-chassis
spec:
  replicas: 5
  selector:
    matchLabels:
      app: agent-chassis
  template:
    metadata:
      labels:
        app: agent-chassis
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      initContainers:
        # Wait for Kafka
        - name: wait-for-kafka
          image: confluentinc/cp-kafka:7.5.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for Kafka..."
              until kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list >/dev/null 2>&1; do
                echo "Kafka not ready, waiting..."
                sleep 5
              done
              echo "Kafka is ready!"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

        # Wait for PostgreSQL clients database
        - name: wait-for-postgres-clients
          image: postgres:16-alpine
          env:
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
          command:
            - sh
            - -c
            - |
              echo "Waiting for PostgreSQL clients database..."
              export PGPASSWORD="$CLIENTS_DB_PASSWORD"
              until pg_isready -h postgres-clients -p 5432; do
                echo "PostgreSQL not ready, waiting..."
                sleep 2
              done
              
              # Wait for agent_definitions table
              max_attempts=30
              attempt=1
              while [ $attempt -le $max_attempts ]; do
                if psql -h postgres-clients -U clients_user -d clients_db -c "\d agent_definitions" >/dev/null 2>&1; then
                  echo "PostgreSQL clients database is ready!"
                  exit 0
                fi
                echo "Attempt $attempt/$max_attempts - database schema not ready..."
                sleep 10
                ((attempt++))
              done
              echo "Database schema not ready in time"
              exit 1
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Wait for required Kafka topics
        - name: wait-for-topics
          image: confluentinc/cp-kafka:7.5.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for required Kafka topics..."
              required_topics="system.agent.generic.process system.tasks.copywriter system.tasks.researcher"
              
              for topic in $required_topics; do
                echo "Checking for topic: $topic"
                max_attempts=20
                attempt=1
                while [ $attempt -le $max_attempts ]; do
                  if kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list | grep -q "^$topic$"; then
                    echo "Topic $topic exists!"
                    break
                  fi
                  if [ $attempt -eq $max_attempts ]; then
                    echo "Topic $topic not found after $max_attempts attempts"
                    exit 1
                  fi
                  echo "Attempt $attempt/$max_attempts - topic $topic not found..."
                  sleep 5
                  ((attempt++))
                done
              done
              echo "All required topics exist!"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

      containers:
        - name: agent-chassis
          image: ai-persona-system/agent-chassis:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9090
              name: metrics
          env:
            - name: SERVICE_NAME
              value: "agent-chassis"
            - name: SERVICE_VERSION
              value: "1.0.0"
            - name: SERVICE_ENVIRONMENT
              value: "production"
            - name: LOGGING_LEVEL
              value: "info"
            - name: AGENT_TYPE
              value: "generic"
            - name: KAFKA_CONSUMER_GROUP
              value: "agent-chassis-group"
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: access-key
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: secret-key
          envFrom:
            - configMapRef:
                name: common-config
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          livenessProbe:
            httpGet:
              path: /health
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 9090
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: agent-chassis-hpa
  namespace: ai-persona-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: agent-chassis
  minReplicas: 5
  maxReplicas: 50
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/development/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/staging/resources-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/staging/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/staging/deployment-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/agent-chassis/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: personae-system
resources:
  - deployment.yaml
  - service.yaml
  # ConfigMaps and Secrets will be managed by Terraform to inject dynamic values-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
  labels:
    app: auth-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: auth-service
  template:
    metadata:
      labels:
        app: auth-service
    spec:
      containers:
        - name: auth-service
          # The full image path and tag will be set by Terraform
          image: auth-service:IMAGE_TAG_PLACEHOLDER
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8081
              name: http
          envFrom:
            - configMapRef:
                name: auth-service-config
            - secretRef:
                name: auth-service-secrets
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 20
            periodSeconds: 10-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: auth-service
spec:
  ports:
    - port: 8081
      targetPort: 8081
  selector:
    app: auth-service
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/development/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/staging/resources-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/staging/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/staging/deployment-patch.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/auth-service/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/base/deployment.yaml
# FILE: k8s/web-search-adapter.yaml (Updated)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-search-adapter
  namespace: ai-persona-system
  labels:
    app: web-search-adapter
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-search-adapter
  template:
    metadata:
      labels:
        app: web-search-adapter
    spec:
      initContainers:
        # Wait for Kafka and topics
        - name: wait-for-kafka-topics
          image: confluentinc/cp-kafka:7.5.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for Kafka and required topics..."
              until kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list >/dev/null 2>&1; do
                echo "Kafka not ready, waiting..."
                sleep 5
              done
              
              required_topics="system.adapter.web.search system.responses.websearch"
              for topic in $required_topics; do
                echo "Checking for topic: $topic"
                max_attempts=20
                attempt=1
                while [ $attempt -le $max_attempts ]; do
                  if kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list | grep -q "^$topic$"; then
                    echo "Topic $topic exists!"
                    break
                  fi
                  if [ $attempt -eq $max_attempts ]; then
                    echo "Topic $topic not found after $max_attempts attempts"
                    exit 1
                  fi
                  echo "Attempt $attempt/$max_attempts - topic $topic not found..."
                  sleep 5
                  ((attempt++))
                done
              done
              echo "All required topics exist!"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

      containers:
        - name: web-search-adapter
          image: ai-persona-system/web-search-adapter:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: SERVICE_NAME
              value: "web-search-adapter"
            - name: SERVICE_VERSION
              value: "1.0.0"
            - name: SERVICE_ENVIRONMENT
              value: "production"
            - name: LOGGING_LEVEL
              value: "info"
            - name: SERP_API_KEY
              valueFrom:
                secretKeyRef:
                  name: ai-secrets
                  key: serp-api-key
          envFrom:
            - configMapRef:
                name: common-config
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/web-search-adapter/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/base/deployment.yaml
# FILE: k8s/reasoning-agent.yaml (Updated)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reasoning-agent
  namespace: ai-persona-system
  labels:
    app: reasoning-agent
spec:
  replicas: 2
  selector:
    matchLabels:
      app: reasoning-agent
  template:
    metadata:
      labels:
        app: reasoning-agent
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      initContainers:
        # Wait for Kafka and required topics
        - name: wait-for-kafka-topics
          image: confluentinc/cp-kafka:7.5.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for Kafka and required topics..."
              until kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list >/dev/null 2>&1; do
                echo "Kafka not ready, waiting..."
                sleep 5
              done
              
              required_topics="system.agent.reasoning.process system.responses.reasoning"
              for topic in $required_topics; do
                echo "Checking for topic: $topic"
                max_attempts=20
                attempt=1
                while [ $attempt -le $max_attempts ]; do
                  if kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list | grep -q "^$topic$"; then
                    echo "Topic $topic exists!"
                    break
                  fi
                  if [ $attempt -eq $max_attempts ]; then
                    echo "Topic $topic not found after $max_attempts attempts"
                    exit 1
                  fi
                  echo "Attempt $attempt/$max_attempts - topic $topic not found..."
                  sleep 5
                  ((attempt++))
                done
              done
              echo "All required topics exist!"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

      containers:
        - name: reasoning-agent
          image: ai-persona-system/reasoning-agent:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9090
              name: metrics
          env:
            - name: SERVICE_NAME
              value: "reasoning-agent"
            - name: SERVICE_VERSION
              value: "1.0.0"
            - name: SERVICE_ENVIRONMENT
              value: "production"
            - name: LOGGING_LEVEL
              value: "info"
            - name: ANTHROPIC_API_KEY
              valueFrom:
                secretKeyRef:
                  name: ai-secrets
                  key: anthropic-api-key
          envFrom:
            - configMapRef:
                name: common-config
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          livenessProbe:
            httpGet:
              path: /health
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 9090
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/reasoning-agent/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/base/deployment.yaml
# FILE: k8s/core-manager.yaml (Updated)
apiVersion: v1
kind: Service
metadata:
  name: core-manager
  namespace: ai-persona-system
  labels:
    app: core-manager
spec:
  ports:
    - port: 8088
      targetPort: 8088
      name: http
    - port: 9090
      targetPort: 9090
      name: metrics
  selector:
    app: core-manager
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: core-manager
  namespace: ai-persona-system
  labels:
    app: core-manager
spec:
  replicas: 3
  selector:
    matchLabels:
      app: core-manager
  template:
    metadata:
      labels:
        app: core-manager
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      initContainers:
        # Wait for PostgreSQL clients database
        - name: wait-for-postgres-clients
          image: postgres:16-alpine
          command:
            - sh
            - -c
            - |
              echo "Waiting for PostgreSQL clients database..."
              until pg_isready -h postgres-clients -p 5432; do
                echo "PostgreSQL clients not ready, waiting..."
                sleep 2
              done
              echo "PostgreSQL clients is ready!"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Wait for PostgreSQL templates database
        - name: wait-for-postgres-templates
          image: postgres:16-alpine
          command:
            - sh
            - -c
            - |
              echo "Waiting for PostgreSQL templates database..."
              until pg_isready -h postgres-templates -p 5432; do
                echo "PostgreSQL templates not ready, waiting..."
                sleep 2
              done
              echo "PostgreSQL templates is ready!"
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Wait for database migrations
        - name: wait-for-migrations
          image: postgres:16-alpine
          env:
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
          command:
            - sh
            - -c
            - |
              echo "Waiting for database migrations to complete..."
              export PGPASSWORD="$TEMPLATES_DB_PASSWORD"
              max_attempts=30
              attempt=1
              while [ $attempt -le $max_attempts ]; do
                if psql -h postgres-templates -U templates_user -d templates_db -c "\d persona_templates" >/dev/null 2>&1; then
                  echo "Templates database ready!"
                  break
                fi
                echo "Attempt $attempt/$max_attempts - templates database not ready..."
                sleep 10
                ((attempt++))
              done
              
              export PGPASSWORD="$CLIENTS_DB_PASSWORD"
              attempt=1
              while [ $attempt -le $max_attempts ]; do
                if psql -h postgres-clients -U clients_user -d clients_db -c "\d orchestrator_state" >/dev/null 2>&1; then
                  echo "Clients database ready!"
                  exit 0
                fi
                echo "Attempt $attempt/$max_attempts - clients database not ready..."
                sleep 10
                ((attempt++))
              done
              echo "Database migrations did not complete in time"
              exit 1
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"

        # Wait for MinIO
        - name: wait-for-minio
          image: curlimages/curl:latest
          command:
            - sh
            - -c
            - |
              echo "Waiting for MinIO..."
              until curl -f http://minio:9000/minio/health/live >/dev/null 2>&1; do
                echo "MinIO not ready, waiting..."
                sleep 5
              done
              echo "MinIO is ready!"
          resources:
            requests:
              memory: "32Mi"
              cpu: "50m"
            limits:
              memory: "64Mi"
              cpu: "100m"

      containers:
        - name: core-manager
          image: ai-persona-system/core-manager:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8088
              name: http
            - containerPort: 9090
              name: metrics
          env:
            - name: SERVICE_NAME
              value: "core-manager"
            - name: SERVICE_VERSION
              value: "1.0.0"
            - name: SERVICE_ENVIRONMENT
              value: "production"
            - name: SERVER_PORT
              value: "8088"
            - name: LOGGING_LEVEL
              value: "info"
            - name: CLIENTS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: clients-db-password
            - name: TEMPLATES_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: templates-db-password
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: access-key
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: secret-key
            - name: JWT_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: auth-secrets
                  key: jwt-secret
          envFrom:
            - configMapRef:
                name: common-config
          resources:
            requests:
              memory: "768Mi"
              cpu: "500m"
            limits:
              memory: "1.5Gi"
              cpu: "1500m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8088
            initialDelaySeconds: 45
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8088
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/base/configmap.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/core-manager/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/base/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/base/deployment.yaml
# FILE: k8s/image-generator-adapter.yaml (Updated)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: image-generator-adapter
  namespace: ai-persona-system
  labels:
    app: image-generator-adapter
spec:
  replicas: 2
  selector:
    matchLabels:
      app: image-generator-adapter
  template:
    metadata:
      labels:
        app: image-generator-adapter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      initContainers:
        # Wait for Kafka and topics
        - name: wait-for-kafka-topics
          image: confluentinc/cp-kafka:7.5.0
          command:
            - sh
            - -c
            - |
              echo "Waiting for Kafka and required topics..."
              until kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list >/dev/null 2>&1; do
                echo "Kafka not ready, waiting..."
                sleep 5
              done
              
              required_topics="system.adapter.image.generate system.responses.image"
              for topic in $required_topics; do
                echo "Checking for topic: $topic"
                max_attempts=20
                attempt=1
                while [ $attempt -le $max_attempts ]; do
                  if kafka-topics --bootstrap-server kafka-0.kafka-headless:9092 --list | grep -q "^$topic$"; then
                    echo "Topic $topic exists!"
                    break
                  fi
                  if [ $attempt -eq $max_attempts ]; then
                    echo "Topic $topic not found after $max_attempts attempts"
                    exit 1
                  fi
                  echo "Attempt $attempt/$max_attempts - topic $topic not found..."
                  sleep 5
                  ((attempt++))
                done
              done
              echo "All required topics exist!"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"

        # Wait for MinIO
        - name: wait-for-minio
          image: curlimages/curl:latest
          command:
            - sh
            - -c
            - |
              echo "Waiting for MinIO..."
              until curl -f http://minio:9000/minio/health/live >/dev/null 2>&1; do
                echo "MinIO not ready, waiting..."
                sleep 5
              done
              echo "MinIO is ready!"
          resources:
            requests:
              memory: "32Mi"
              cpu: "50m"
            limits:
              memory: "64Mi"
              cpu: "100m"

      containers:
        - name: image-generator-adapter
          image: ai-persona-system/image-generator-adapter:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9090
              name: metrics
          env:
            - name: SERVICE_NAME
              value: "image-generator-adapter"
            - name: SERVICE_VERSION
              value: "1.0.0"
            - name: SERVICE_ENVIRONMENT
              value: "production"
            - name: LOGGING_LEVEL
              value: "info"
            - name: STABILITY_API_KEY
              valueFrom:
                secretKeyRef:
                  name: ai-secrets
                  key: stability-api-key
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: access-key
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secrets
                  key: secret-key
          envFrom:
            - configMapRef:
                name: common-config
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1.5Gi"
              cpu: "1000m"
          livenessProbe:
            httpGet:
              path: /health
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 9090
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/base/service.yaml
-------------------------------------------------
filepath = ./deployments/kustomize/services/image-generator-adapter/overlays/production/kustomization.yaml
-------------------------------------------------
filepath = ./deployments/docker-compose/.env.example
-------------------------------------------------
filepath = ./deployments/docker-compose/docker-compose.yaml
# FILE: docker-compose.yml
version: '3.8'

services:
  # Infrastructure
  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    depends_on:
      - zookeeper

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  postgres-clients:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: clients_db
      POSTGRES_USER: clients_user
      POSTGRES_PASSWORD: ${CLIENTS_DB_PASSWORD}
    volumes:
      - clients_data:/var/lib/postgresql/data

  postgres-templates:
    image: postgres:16
    environment:
      POSTGRES_DB: templates_db
      POSTGRES_USER: templates_user
      POSTGRES_PASSWORD: ${TEMPLATES_DB_PASSWORD}
    volumes:
      - templates_data:/var/lib/postgresql/data

  mysql-auth:
    image: mysql:8
    environment:
      MYSQL_DATABASE: auth_db
      MYSQL_USER: auth_user
      MYSQL_PASSWORD: ${AUTH_DB_PASSWORD}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    volumes:
      - auth_data:/var/lib/mysql

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
    volumes:
      - minio_data:/data

  # Core Services
  auth-service:
    build:
      context: .
      dockerfile: Dockerfile.auth-service
    environment:
      AUTH_DB_PASSWORD: ${AUTH_DB_PASSWORD}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
    depends_on:
      - mysql-auth
    ports:
      - "8081:8081"

  core-manager:
    build:
      context: .
      dockerfile: Dockerfile.core-manager
    environment:
      CLIENTS_DB_PASSWORD: ${CLIENTS_DB_PASSWORD}
      TEMPLATES_DB_PASSWORD: ${TEMPLATES_DB_PASSWORD}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    depends_on:
      - postgres-clients
      - postgres-templates
      - kafka
    ports:
      - "8088:8088"

  # Agent Services
  agent-chassis:
    build:
      context: .
      dockerfile: Dockerfile.agent-chassis
    environment:
      CLIENTS_DB_PASSWORD: ${CLIENTS_DB_PASSWORD}
      TEMPLATES_DB_PASSWORD: ${TEMPLATES_DB_PASSWORD}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    depends_on:
      - postgres-clients
      - kafka
    deploy:
      replicas: 3

  reasoning-agent:
    build:
      context: .
      dockerfile: Dockerfile.reasoning
    environment:
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
    depends_on:
      - kafka

volumes:
  clients_data:
  templates_data:
  auth_data:
  minio_data:-------------------------------------------------
filepath = ./platform/kafka/types.go
// FILE: platform/kafka/types.go
package kafka

import (
	"github.com/segmentio/kafka-go"
)

// Message wraps kafka-go Message to implement our interface
type Message = kafka.Message

// Header wraps kafka-go Header
type Header = kafka.Header
-------------------------------------------------
filepath = ./platform/kafka/consumer.go
// FILE: platform/kafka/consumer.go (updated version)
package kafka

import (
	"context"
	"fmt"

	"github.com/segmentio/kafka-go"
	"go.uber.org/zap"
)

// Consumer wraps the kafka-go reader for standardized consumption
type Consumer struct {
	reader *kafka.Reader
	logger *zap.Logger
}

// NewConsumer creates a new standardized Kafka consumer
func NewConsumer(brokers []string, topic, groupID string, logger *zap.Logger) (*Consumer, error) {
	if len(brokers) == 0 {
		return nil, fmt.Errorf("kafka brokers list cannot be empty")
	}
	if topic == "" {
		return nil, fmt.Errorf("kafka topic cannot be empty")
	}
	if groupID == "" {
		return nil, fmt.Errorf("kafka groupID cannot be empty")
	}

	reader := kafka.NewReader(kafka.ReaderConfig{
		Brokers:        brokers,
		GroupID:        groupID,
		Topic:          topic,
		MinBytes:       10e3, // 10KB
		MaxBytes:       10e6, // 10MB
		CommitInterval: 0,    // Manual commit
	})

	logger.Info("Kafka consumer created",
		zap.Strings("brokers", brokers),
		zap.String("topic", topic),
		zap.String("groupID", groupID),
	)

	return &Consumer{
		reader: reader,
		logger: logger,
	}, nil
}

// FetchMessage fetches the next message from the topic
// Returns the native kafka.Message type
func (c *Consumer) FetchMessage(ctx context.Context) (Message, error) {
	msg, err := c.reader.FetchMessage(ctx)
	if err != nil {
		if err == context.Canceled {
			return Message{}, err
		}
		c.logger.Error("Failed to fetch message from Kafka", zap.Error(err))
		return Message{}, err
	}
	return msg, nil
}

// CommitMessages commits the offset for the given messages
func (c *Consumer) CommitMessages(ctx context.Context, msgs ...Message) error {
	err := c.reader.CommitMessages(ctx, msgs...)
	if err != nil {
		c.logger.Error("Failed to commit Kafka messages", zap.Error(err))
	}
	return err
}

// Close gracefully closes the consumer's reader
func (c *Consumer) Close() error {
	c.logger.Info("Closing Kafka consumer...")
	return c.reader.Close()
}
-------------------------------------------------
filepath = ./platform/kafka/utils.go
// FILE: platform/kafka/utils.go
package kafka

import "github.com/segmentio/kafka-go"

// HeadersToMap converts Kafka headers to a map for easier access
func HeadersToMap(headers []kafka.Header) map[string]string {
	result := make(map[string]string)
	for _, h := range headers {
		result[h.Key] = string(h.Value)
	}
	return result
}
-------------------------------------------------
filepath = ./platform/kafka/mock_producer.go
// FILE: platform/kafka/mock_producer.go
// Mock producer for testing
package kafka

import (
	"context"
	"github.com/stretchr/testify/mock"
)

// MockProducer is a mock implementation of the Producer interface
type MockProducer struct {
	mock.Mock
}

// Produce mocks the Produce method
func (m *MockProducer) Produce(ctx context.Context, topic string, headers map[string]string, key, value []byte) error {
	args := m.Called(ctx, topic, headers, key, value)
	return args.Error(0)
}

// Close mocks the Close method
func (m *MockProducer) Close() error {
	args := m.Called()
	return args.Error(0)
}
-------------------------------------------------
filepath = ./platform/kafka/producer.go
// FILE: platform/kafka/producer.go
package kafka

import (
	"context"
	"fmt"
	"time"

	"github.com/segmentio/kafka-go"
	"go.uber.org/zap"
)

// Producer defines the interface for Kafka message production
type Producer interface {
	Produce(ctx context.Context, topic string, headers map[string]string, key, value []byte) error
	Close() error
}

// KafkaProducer wraps the kafka-go writer for standardized message production
type KafkaProducer struct {
	writer *kafka.Writer
	logger *zap.Logger
}

// NewProducer creates a new standardized Kafka producer
func NewProducer(brokers []string, logger *zap.Logger) (Producer, error) {
	if len(brokers) == 0 {
		return nil, fmt.Errorf("kafka brokers list cannot be empty")
	}

	writer := &kafka.Writer{
		Addr:         kafka.TCP(brokers...),
		Balancer:     &kafka.LeastBytes{},
		RequiredAcks: kafka.RequireAll,
		Async:        false,
		WriteTimeout: 10 * time.Second,
	}

	logger.Info("Kafka producer created", zap.Strings("brokers", brokers))

	return &KafkaProducer{
		writer: writer,
		logger: logger,
	}, nil
}

// Produce sends a message to a specific topic with standard headers
func (p *KafkaProducer) Produce(ctx context.Context, topic string, headers map[string]string, key, value []byte) error {
	kafkaHeaders := make([]kafka.Header, 0, len(headers))
	for k, v := range headers {
		kafkaHeaders = append(kafkaHeaders, kafka.Header{Key: k, Value: []byte(v)})
	}

	msg := kafka.Message{
		Topic:   topic,
		Key:     key,
		Value:   value,
		Headers: kafkaHeaders,
		Time:    time.Now().UTC(),
	}

	err := p.writer.WriteMessages(ctx, msg)
	if err != nil {
		p.logger.Error("Failed to produce Kafka message",
			zap.String("topic", topic),
			zap.Error(err),
		)
		return fmt.Errorf("failed to write message to kafka: %w", err)
	}

	p.logger.Debug("Successfully produced message", zap.String("topic", topic), zap.String("key", string(key)))
	return nil
}

// Close gracefully closes the producer's writer
func (p *KafkaProducer) Close() error {
	p.logger.Info("Closing Kafka producer...")
	return p.writer.Close()
}
-------------------------------------------------
filepath = ./platform/kafka/producer_test.go
// FILE: platform/kafka/producer_test.go
package kafka

import (
	"testing"

	"github.com/stretchr/testify/assert"
	"go.uber.org/zap"
)

func TestNewProducer(t *testing.T) {
	// Test with empty brokers
	_, err := NewProducer([]string{}, zap.NewNop())
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "kafka brokers list cannot be empty")

	// Test with valid brokers (won't actually connect in unit test)
	producer, err := NewProducer([]string{"localhost:9092"}, zap.NewNop())
	assert.NoError(t, err)
	assert.NotNil(t, producer)

	// Clean up
	producer.Close()
}

func TestProducerInterface(t *testing.T) {
	// Ensure KafkaProducer implements Producer interface
	var _ Producer = (*KafkaProducer)(nil)
}
-------------------------------------------------
filepath = ./platform/logger/logger.go
// FILE: platform/logger/logger.go
package logger

import (
	"fmt"
	"log"

	"go.uber.org/zap"
	"go.uber.org/zap/zapcore"
)

// New creates a new Zap logger with a specified log level
func New(logLevel string) (*zap.Logger, error) {
	var level zapcore.Level
	if err := level.UnmarshalText([]byte(logLevel)); err != nil {
		log.Printf("logger: invalid log level '%s', defaulting to 'info'", logLevel)
		level = zapcore.InfoLevel
	}

	config := zap.Config{
		Level:       zap.NewAtomicLevelAt(level),
		Development: false,
		Encoding:    "json",
		EncoderConfig: zapcore.EncoderConfig{
			TimeKey:        "ts",
			LevelKey:       "level",
			NameKey:        "logger",
			CallerKey:      "caller",
			MessageKey:     "msg",
			StacktraceKey:  "stacktrace",
			LineEnding:     zapcore.DefaultLineEnding,
			EncodeLevel:    zapcore.LowercaseLevelEncoder,
			EncodeTime:     zapcore.ISO8601TimeEncoder,
			EncodeDuration: zapcore.SecondsDurationEncoder,
			EncodeCaller:   zapcore.ShortCallerEncoder,
		},
		OutputPaths:      []string{"stdout"},
		ErrorOutputPaths: []string{"stderr"},
	}

	logger, err := config.Build()
	if err != nil {
		return nil, fmt.Errorf("logger: failed to initialize zap logger: %w", err)
	}

	logger.Info("Logger initialized successfully", zap.String("level", level.String()))
	return logger, nil
}
-------------------------------------------------
filepath = ./platform/resilience/circuit_breaker.go
// FILE: platform/resilience/circuit_breaker.go
package resilience

import (
	"context"
	"net/http"
	"time"

	"github.com/sony/gobreaker"
	"go.uber.org/zap"
)

// CircuitBreakerConfig holds configuration for circuit breakers
type CircuitBreakerConfig struct {
	Name                string
	MaxRequests         uint32
	Interval            time.Duration
	Timeout             time.Duration
	ConsecutiveFailures uint32
	FailureRatio        float64
}

// DefaultCircuitBreakerConfig returns sensible defaults
func DefaultCircuitBreakerConfig(name string) CircuitBreakerConfig {
	return CircuitBreakerConfig{
		Name:                name,
		MaxRequests:         3,
		Interval:            60 * time.Second,
		Timeout:             60 * time.Second,
		ConsecutiveFailures: 5,
		FailureRatio:        0.6,
	}
}

// CircuitBreaker wraps the gobreaker implementation
type CircuitBreaker struct {
	breaker *gobreaker.CircuitBreaker
	logger  *zap.Logger
	config  CircuitBreakerConfig
}

// NewCircuitBreaker creates a new circuit breaker
func NewCircuitBreaker(config CircuitBreakerConfig, logger *zap.Logger) *CircuitBreaker {
	settings := gobreaker.Settings{
		Name:        config.Name,
		MaxRequests: config.MaxRequests,
		Interval:    config.Interval,
		Timeout:     config.Timeout,
		ReadyToTrip: func(counts gobreaker.Counts) bool {
			failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
			return counts.Requests >= config.ConsecutiveFailures && failureRatio >= config.FailureRatio
		},
		OnStateChange: func(name string, from gobreaker.State, to gobreaker.State) {
			logger.Warn("Circuit breaker state change",
				zap.String("name", name),
				zap.String("from", from.String()),
				zap.String("to", to.String()),
			)
		},
	}

	return &CircuitBreaker{
		breaker: gobreaker.NewCircuitBreaker(settings),
		logger:  logger,
		config:  config,
	}
}

// Execute runs a function through the circuit breaker
func (cb *CircuitBreaker) Execute(fn func() (interface{}, error)) (interface{}, error) {
	return cb.breaker.Execute(fn)
}

// ExecuteWithContext runs a function with context through the circuit breaker
func (cb *CircuitBreaker) ExecuteWithContext(ctx context.Context, fn func(context.Context) (interface{}, error)) (interface{}, error) {
	return cb.breaker.Execute(func() (interface{}, error) {
		return fn(ctx)
	})
}

// State returns the current state of the circuit breaker
func (cb *CircuitBreaker) State() string {
	return cb.breaker.State().String()
}

// IsOpen returns true if the circuit breaker is open
func (cb *CircuitBreaker) IsOpen() bool {
	return cb.breaker.State() == gobreaker.StateOpen
}

// Counts returns the current counts
func (cb *CircuitBreaker) Counts() gobreaker.Counts {
	return cb.breaker.Counts()
}

// HTTPClient wraps an HTTP client with circuit breaker functionality
type HTTPClientWithBreaker struct {
	client  HTTPDoer
	Breaker *CircuitBreaker
	logger  *zap.Logger
}

// HTTPDoer interface for HTTP client
type HTTPDoer interface {
	Do(req *http.Request) (*http.Response, error)
}

// NewHTTPClientWithBreaker creates a new HTTP client with circuit breaker
func NewHTTPClientWithBreaker(client HTTPDoer, config CircuitBreakerConfig, logger *zap.Logger) *HTTPClientWithBreaker {
	return &HTTPClientWithBreaker{
		client:  client,
		Breaker: NewCircuitBreaker(config, logger),
		logger:  logger,
	}
}

// Do executes an HTTP request through the circuit breaker
func (c *HTTPClientWithBreaker) Do(req *http.Request) (*http.Response, error) {
	result, err := c.Breaker.Execute(func() (interface{}, error) {
		return c.client.Do(req)
	})

	if err != nil {
		return nil, err
	}

	return result.(*http.Response), nil
}

// State returns the current state of the circuit breaker
func (c *HTTPClientWithBreaker) State() string {
	return c.Breaker.State()
}

// Counts returns the current counts
func (c *HTTPClientWithBreaker) Counts() gobreaker.Counts {
	return c.Breaker.Counts()
}

// IsCircuitBreakerError checks if an error is from a circuit breaker
func IsCircuitBreakerError(err error) bool {
	if err == nil {
		return false
	}
	return err == gobreaker.ErrOpenState || err == gobreaker.ErrTooManyRequests
}
-------------------------------------------------
filepath = ./platform/storage/s3.go
// FILE: platform/storage/s3.go
package storage

import (
	"context"
	"fmt"
	"io"
	"os"
	"strings"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	platform_config "github.com/gqls/agentchassis/platform/config"
)

// S3Client implements the Client interface for S3-compatible services
type S3Client struct {
	client *s3.Client
	bucket string
}

// NewS3Client creates a new client for interacting with S3 or MinIO
func NewS3Client(ctx context.Context, cfg platform_config.ObjectStorageConfig) (*S3Client, error) {
	accessKey := os.Getenv(cfg.AccessKeyEnvVar)
	secretKey := os.Getenv(cfg.SecretKeyEnvVar)

	if accessKey == "" || secretKey == "" {
		return nil, fmt.Errorf("object storage credentials not found in environment variables (%s, %s)",
			cfg.AccessKeyEnvVar, cfg.SecretKeyEnvVar)
	}

	resolver := aws.EndpointResolverWithOptionsFunc(func(service, region string, options ...interface{}) (aws.Endpoint, error) {
		return aws.Endpoint{
			URL:           cfg.Endpoint,
			SigningRegion: "us-east-1", // This can be anything for MinIO
		}, nil
	})

	awsCfg, err := config.LoadDefaultConfig(ctx,
		config.WithEndpointResolverWithOptions(resolver),
		config.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(accessKey, secretKey, "")),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to load s3 config: %w", err)
	}

	// For MinIO, you must use path-style addressing
	s3Client := s3.NewFromConfig(awsCfg, func(o *s3.Options) {
		o.UsePathStyle = true
	})

	return &S3Client{
		client: s3Client,
		bucket: cfg.Bucket,
	}, nil
}

// Upload puts a new object into the storage bucket
func (c *S3Client) Upload(ctx context.Context, key, contentType string, body io.Reader) (string, error) {
	_, err := c.client.PutObject(ctx, &s3.PutObjectInput{
		Bucket:      aws.String(c.bucket),
		Key:         aws.String(key),
		Body:        body,
		ContentType: aws.String(contentType),
	})
	if err != nil {
		return "", fmt.Errorf("failed to upload object to s3: %w", err)
	}
	// Return the S3 URI for the object
	return fmt.Sprintf("s3://%s/%s", c.bucket, key), nil
}

// Download retrieves an object from the storage bucket
func (c *S3Client) Download(ctx context.Context, key string) (io.ReadCloser, error) {
	output, err := c.client.GetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(c.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		return nil, fmt.Errorf("failed to download object from s3: %w", err)
	}
	return output.Body, nil
}

// Delete removes an object from storage
func (c *S3Client) Delete(ctx context.Context, key string) error {
	_, err := c.client.DeleteObject(ctx, &s3.DeleteObjectInput{
		Bucket: aws.String(c.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		return fmt.Errorf("failed to delete object: %w", err)
	}
	return nil
}

// Exists checks if an object exists
func (c *S3Client) Exists(ctx context.Context, key string) (bool, error) {
	_, err := c.client.HeadObject(ctx, &s3.HeadObjectInput{
		Bucket: aws.String(c.bucket),
		Key:    aws.String(key),
	})
	if err != nil {
		// Check if it's a not found error
		if strings.Contains(err.Error(), "NotFound") {
			return false, nil
		}
		return false, fmt.Errorf("failed to check object existence: %w", err)
	}
	return true, nil
}

// ListObjects lists objects with a given prefix
func (c *S3Client) ListObjects(ctx context.Context, prefix string) ([]ObjectInfo, error) {
	paginator := s3.NewListObjectsV2Paginator(c.client, &s3.ListObjectsV2Input{
		Bucket: aws.String(c.bucket),
		Prefix: aws.String(prefix),
	})

	var objects []ObjectInfo
	for paginator.HasMorePages() {
		page, err := paginator.NextPage(ctx)
		if err != nil {
			return nil, fmt.Errorf("failed to list objects: %w", err)
		}

		for _, obj := range page.Contents {
			objects = append(objects, ObjectInfo{
				Key:          aws.ToString(obj.Key),
				Size:         aws.ToInt64(obj.Size),
				LastModified: aws.ToTime(obj.LastModified),
				ETag:         aws.ToString(obj.ETag),
			})
		}
	}

	return objects, nil
}

// GetPresignedURL generates a temporary access URL
func (c *S3Client) GetPresignedURL(ctx context.Context, key string, expiryMinutes int) (string, error) {
	presignClient := s3.NewPresignClient(c.client)

	request, err := presignClient.PresignGetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(c.bucket),
		Key:    aws.String(key),
	}, func(opts *s3.PresignOptions) {
		opts.Expires = time.Duration(expiryMinutes) * time.Minute
	})

	if err != nil {
		return "", fmt.Errorf("failed to create presigned URL: %w", err)
	}

	return request.URL, nil
}
-------------------------------------------------
filepath = ./platform/storage/interface.go
// FILE: platform/storage/interface.go
package storage

import (
	"context"
	"io"
	"time"
)

// Client defines the interface for object storage operations
type Client interface {
	// Upload stores an object and returns its URI
	Upload(ctx context.Context, key, contentType string, body io.Reader) (string, error)

	// Download retrieves an object by its key
	Download(ctx context.Context, key string) (io.ReadCloser, error)

	// Delete removes an object
	Delete(ctx context.Context, key string) error

	// Exists checks if an object exists
	Exists(ctx context.Context, key string) (bool, error)

	// ListObjects lists objects with a given prefix
	ListObjects(ctx context.Context, prefix string) ([]ObjectInfo, error)

	// GetPresignedURL generates a temporary access URL
	GetPresignedURL(ctx context.Context, key string, expiry int) (string, error)
}

// ObjectInfo contains metadata about a stored object
type ObjectInfo struct {
	Key          string
	Size         int64
	LastModified time.Time
	ContentType  string
	ETag         string
}
-------------------------------------------------
filepath = ./platform/errors/errors.go
// FILE: platform/errors/errors.go
package errors

import (
	"encoding/json"
	"fmt"
	"net/http"
	"time"
)

// ErrorCode represents standardized error codes across the platform
type ErrorCode string

const (
	// General errors
	ErrInternal     ErrorCode = "INTERNAL_ERROR"
	ErrValidation   ErrorCode = "VALIDATION_ERROR"
	ErrNotFound     ErrorCode = "NOT_FOUND"
	ErrUnauthorized ErrorCode = "UNAUTHORIZED"
	ErrForbidden    ErrorCode = "FORBIDDEN"
	ErrConflict     ErrorCode = "CONFLICT"
	ErrRateLimited  ErrorCode = "RATE_LIMITED"

	// Workflow errors
	ErrWorkflowInvalid  ErrorCode = "WORKFLOW_INVALID"
	ErrWorkflowTimeout  ErrorCode = "WORKFLOW_TIMEOUT"
	ErrWorkflowFailed   ErrorCode = "WORKFLOW_FAILED"
	ErrInsufficientFuel ErrorCode = "INSUFFICIENT_FUEL"

	// Agent errors
	ErrAgentNotFound   ErrorCode = "AGENT_NOT_FOUND"
	ErrAgentTimeout    ErrorCode = "AGENT_TIMEOUT"
	ErrAgentOverloaded ErrorCode = "AGENT_OVERLOADED"

	// External service errors
	ErrExternalService ErrorCode = "EXTERNAL_SERVICE_ERROR"
	ErrAIServiceError  ErrorCode = "AI_SERVICE_ERROR"
)

// DomainError represents a standardized error in the platform
type DomainError struct {
	Code       ErrorCode              `json:"code"`
	Message    string                 `json:"message"`
	Details    map[string]interface{} `json:"details,omitempty"`
	Cause      error                  `json:"-"`
	Timestamp  time.Time              `json:"timestamp"`
	TraceID    string                 `json:"trace_id,omitempty"`
	Retryable  bool                   `json:"retryable"`
	RetryAfter *time.Duration         `json:"retry_after,omitempty"`
}

// Error implements the error interface
func (e *DomainError) Error() string {
	if e.Cause != nil {
		return fmt.Sprintf("%s: %s (caused by: %v)", e.Code, e.Message, e.Cause)
	}
	return fmt.Sprintf("%s: %s", e.Code, e.Message)
}

// Unwrap allows for error chain inspection
func (e *DomainError) Unwrap() error {
	return e.Cause
}

// HTTPStatus returns the appropriate HTTP status code for the error
func (e *DomainError) HTTPStatus() int {
	switch e.Code {
	case ErrValidation:
		return http.StatusBadRequest
	case ErrUnauthorized:
		return http.StatusUnauthorized
	case ErrForbidden:
		return http.StatusForbidden
	case ErrNotFound:
		return http.StatusNotFound
	case ErrConflict:
		return http.StatusConflict
	case ErrRateLimited:
		return http.StatusTooManyRequests
	case ErrAgentOverloaded:
		return http.StatusServiceUnavailable
	default:
		return http.StatusInternalServerError
	}
}

// MarshalJSON customizes JSON serialization
func (e *DomainError) MarshalJSON() ([]byte, error) {
	type Alias DomainError
	return json.Marshal(&struct {
		*Alias
		HTTPStatus int `json:"http_status"`
	}{
		Alias:      (*Alias)(e),
		HTTPStatus: e.HTTPStatus(),
	})
}

// ErrorBuilder provides a fluent interface for building errors
type ErrorBuilder struct {
	err *DomainError
}

// New creates a new error builder
func New(code ErrorCode, message string) *ErrorBuilder {
	return &ErrorBuilder{
		err: &DomainError{
			Code:      code,
			Message:   message,
			Timestamp: time.Now().UTC(),
			Retryable: false,
		},
	}
}

// WithCause adds an underlying cause
func (b *ErrorBuilder) WithCause(cause error) *ErrorBuilder {
	b.err.Cause = cause
	return b
}

// WithDetails adds additional details
func (b *ErrorBuilder) WithDetails(details map[string]interface{}) *ErrorBuilder {
	b.err.Details = details
	return b
}

// WithDetail adds a single detail
func (b *ErrorBuilder) WithDetail(key string, value interface{}) *ErrorBuilder {
	if b.err.Details == nil {
		b.err.Details = make(map[string]interface{})
	}
	b.err.Details[key] = value
	return b
}

// WithTraceID adds a trace ID for correlation
func (b *ErrorBuilder) WithTraceID(traceID string) *ErrorBuilder {
	b.err.TraceID = traceID
	return b
}

// AsRetryable marks the error as retryable
func (b *ErrorBuilder) AsRetryable(retryAfter *time.Duration) *ErrorBuilder {
	b.err.Retryable = true
	b.err.RetryAfter = retryAfter
	return b
}

// Build returns the constructed error
func (b *ErrorBuilder) Build() *DomainError {
	return b.err
}

// Helper functions for common errors

// NotFound creates a not found error
func NotFound(resource string, id string) *DomainError {
	return New(ErrNotFound, fmt.Sprintf("%s not found", resource)).
		WithDetail("resource", resource).
		WithDetail("id", id).
		Build()
}

// ValidationError creates a validation error
func ValidationError(field string, issue string) *DomainError {
	return New(ErrValidation, "Validation failed").
		WithDetail("field", field).
		WithDetail("issue", issue).
		Build()
}

// InternalError creates an internal error
func InternalError(message string, cause error) *DomainError {
	return New(ErrInternal, message).
		WithCause(cause).
		Build()
}

// InsufficientFuel creates an insufficient fuel error
func InsufficientFuel(required, available int, action string) *DomainError {
	return New(ErrInsufficientFuel, "Insufficient fuel for operation").
		WithDetail("required", required).
		WithDetail("available", available).
		WithDetail("action", action).
		Build()
}

// IsRetryable checks if an error is retryable
func IsRetryable(err error) bool {
	if domainErr, ok := err.(*DomainError); ok {
		return domainErr.Retryable
	}
	return false
}

// GetRetryAfter gets the retry after duration if available
func GetRetryAfter(err error) *time.Duration {
	if domainErr, ok := err.(*DomainError); ok {
		return domainErr.RetryAfter
	}
	return nil
}
-------------------------------------------------
filepath = ./platform/aiservice/anthropic.go
// FILE: platform/aiservice/anthropic.go
package aiservice

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
)

// AnthropicClient implements the AIService interface for Anthropic's Claude
type AnthropicClient struct {
	apiKey     string
	model      string
	httpClient *http.Client
}

// NewAnthropicClient creates a new Anthropic client
func NewAnthropicClient(ctx context.Context, config map[string]interface{}) (*AnthropicClient, error) {
	apiKeyEnvVar := config["api_key_env_var"].(string)
	apiKey := os.Getenv(apiKeyEnvVar)
	if apiKey == "" {
		return nil, fmt.Errorf("API key not found in environment variable %s", apiKeyEnvVar)
	}

	model := config["model"].(string)

	return &AnthropicClient{
		apiKey:     apiKey,
		model:      model,
		httpClient: &http.Client{},
	}, nil
}

// GenerateText generates text using Claude
func (c *AnthropicClient) GenerateText(ctx context.Context, prompt string, options map[string]interface{}) (string, error) {
	// Build request
	requestBody := map[string]interface{}{
		"model":       c.model,
		"max_tokens":  2048,
		"temperature": 0.7,
		"messages": []map[string]string{
			{
				"role":    "user",
				"content": prompt,
			},
		},
	}

	// Override with provided options
	if options != nil {
		if maxTokens, ok := options["max_tokens"]; ok {
			requestBody["max_tokens"] = maxTokens
		}
		if temperature, ok := options["temperature"]; ok {
			requestBody["temperature"] = temperature
		}
	}

	jsonBody, err := json.Marshal(requestBody)
	if err != nil {
		return "", fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	req, err := http.NewRequestWithContext(ctx, "POST", "https://api.anthropic.com/v1/messages", bytes.NewBuffer(jsonBody))
	if err != nil {
		return "", fmt.Errorf("failed to create request: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("x-api-key", c.apiKey)
	req.Header.Set("anthropic-version", "2023-06-01")

	// Execute request
	resp, err := c.httpClient.Do(req)
	if err != nil {
		return "", fmt.Errorf("failed to execute request: %w", err)
	}
	defer resp.Body.Close()

	// Read response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", fmt.Errorf("failed to read response: %w", err)
	}

	if resp.StatusCode != http.StatusOK {
		return "", fmt.Errorf("API request failed with status %d: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var response struct {
		Content []struct {
			Text string `json:"text"`
		} `json:"content"`
	}

	if err := json.Unmarshal(body, &response); err != nil {
		return "", fmt.Errorf("failed to parse response: %w", err)
	}

	if len(response.Content) == 0 {
		return "", fmt.Errorf("no content in response")
	}

	return response.Content[0].Text, nil
}

// GenerateEmbedding generates embeddings (not implemented for Anthropic)
func (c *AnthropicClient) GenerateEmbedding(ctx context.Context, text string) ([]float32, error) {
	return nil, fmt.Errorf("embedding generation not supported by Anthropic")
}
-------------------------------------------------
filepath = ./platform/aiservice/interface.go
// FILE: platform/aiservice/interface.go
package aiservice

import "context"

// AIService defines the interface for AI providers
type AIService interface {
	GenerateText(ctx context.Context, prompt string, options map[string]interface{}) (string, error)
	GenerateEmbedding(ctx context.Context, text string) ([]float32, error)
}

// TextGenerationOptions contains common options for text generation
type TextGenerationOptions struct {
	Temperature float64
	MaxTokens   int
	Model       string
}
-------------------------------------------------
filepath = ./platform/memory/service.go
// FILE: platform/memory/service.go
package memory

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/aiservice"
	"github.com/gqls/agentchassis/platform/database"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// Service handles memory operations for agents
type Service struct {
	pool       *pgxpool.Pool
	aiClient   aiservice.AIService
	logger     *zap.Logger
	memoryRepo *database.MemoryRepository
}

// NewService creates a new memory service
func NewService(pool *pgxpool.Pool, aiClient aiservice.AIService, logger *zap.Logger) *Service {
	return &Service{
		pool:       pool,
		aiClient:   aiClient,
		logger:     logger,
		memoryRepo: database.NewMemoryRepository(pool, logger),
	}
}

// StoreMemory stores a memory entry for an agent
func (s *Service) StoreMemory(ctx context.Context, agentID uuid.UUID, entry models.MemoryEntry) error {
	// Generate embedding
	embedding, err := s.aiClient.GenerateEmbedding(ctx, entry.Content)
	if err != nil {
		s.logger.Error("Failed to generate embedding", zap.Error(err))
		return fmt.Errorf("failed to generate embedding: %w", err)
	}

	// Add timestamp to metadata
	if entry.Metadata == nil {
		entry.Metadata = make(map[string]interface{})
	}
	entry.Metadata["timestamp"] = entry.Timestamp
	entry.Metadata["type"] = entry.Type

	// Store in database
	return s.memoryRepo.StoreMemory(ctx, agentID, entry.Content, embedding, entry.Metadata)
}

// RetrieveRelevantMemories retrieves memories relevant to a query
func (s *Service) RetrieveRelevantMemories(ctx context.Context, agentID uuid.UUID, query string, limit int) ([]models.MemoryEntry, error) {
	// Generate query embedding
	queryEmbedding, err := s.aiClient.GenerateEmbedding(ctx, query)
	if err != nil {
		s.logger.Error("Failed to generate query embedding", zap.Error(err))
		return nil, fmt.Errorf("failed to generate query embedding: %w", err)
	}

	// Search for similar memories
	records, err := s.memoryRepo.SearchMemory(ctx, agentID, queryEmbedding, limit)
	if err != nil {
		return nil, err
	}

	// Convert to memory entries
	entries := make([]models.MemoryEntry, len(records))
	for i, record := range records {
		entries[i] = models.MemoryEntry{
			Content:  record.Content,
			Metadata: record.Metadata,
		}

		// Extract type and timestamp from metadata
		if typeStr, ok := record.Metadata["type"].(string); ok {
			entries[i].Type = typeStr
		}
		if timestampStr, ok := record.Metadata["timestamp"].(string); ok {
			if t, err := time.Parse(time.RFC3339, timestampStr); err == nil {
				entries[i].Timestamp = t
			}
		}
	}

	return entries, nil
}

// ProcessWorkflowMemory handles memory storage for workflow steps
func (s *Service) ProcessWorkflowMemory(ctx context.Context, agentID uuid.UUID, config models.MemoryConfiguration, step models.Step, input, output interface{}) error {
	// Check if memory is enabled and this step should store memory
	if !config.Enabled || !step.StoreMemory {
		return nil
	}

	// Create memory entry
	entry := models.MemoryEntry{
		Type:      "workflow_step",
		Timestamp: time.Now(),
		Metadata: map[string]interface{}{
			"step_action":      step.Action,
			"step_description": step.Description,
		},
	}

	// Format content based on input and output
	content := map[string]interface{}{
		"action": step.Action,
		"input":  input,
		"output": output,
	}

	contentBytes, err := json.MarshalIndent(content, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal memory content: %w", err)
	}
	entry.Content = string(contentBytes)

	// Store the memory
	return s.StoreMemory(ctx, agentID, entry)
}

// GetMemoryContext retrieves relevant memories for a given context
func (s *Service) GetMemoryContext(ctx context.Context, agentID uuid.UUID, config models.MemoryConfiguration, currentContext string) ([]models.MemoryEntry, error) {
	if !config.Enabled {
		return nil, nil
	}

	count := config.RetrievalCount
	if count == 0 {
		count = 5 // default
	}

	return s.RetrieveRelevantMemories(ctx, agentID, currentContext, count)
}
-------------------------------------------------
filepath = ./platform/messaging/context.go
// FILE: platform/messaging/context.go
package messaging

import (
	"encoding/json"
	"fmt"
	"time"

	"github.com/google/uuid"
	"github.com/gqls/agentchassis/platform/kafka"
	"go.uber.org/zap"
)

// MessageContext holds the context for processing a single message
type MessageContext struct {
	Message   kafka.Message
	Headers   map[string]string
	Action    string
	StartTime time.Time
	Logger    *zap.Logger
}

// ExtractAction extracts the action from the message payload
func (m *MessageContext) ExtractAction() error {
	var payload struct {
		Action string `json:"action"`
	}
	if err := json.Unmarshal(m.Message.Value, &payload); err != nil {
		return fmt.Errorf("failed to extract action: %w", err)
	}
	m.Action = payload.Action
	return nil
}

// ValidateHeaders ensures required headers are present
func (m *MessageContext) ValidateHeaders() error {
	required := []string{"correlation_id", "request_id", "client_id", "agent_instance_id"}
	for _, key := range required {
		if m.Headers[key] == "" {
			return fmt.Errorf("missing required header: %s", key)
		}
	}
	return nil
}

// CreateResponseHeaders creates headers for a response message
func (m *MessageContext) CreateResponseHeaders(agentType string) map[string]string {
	return map[string]string{
		"correlation_id": m.Headers["correlation_id"],
		"causation_id":   m.Headers["request_id"],
		"request_id":     uuid.NewString(),
		"client_id":      m.Headers["client_id"],
		"agent_type":     agentType,
		"timestamp":      time.Now().UTC().Format(time.RFC3339),
	}
}
-------------------------------------------------
filepath = ./platform/messaging/processor.go
// FILE: platform/messaging/processor.go
package messaging

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/errors"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/gqls/agentchassis/platform/observability"
	"github.com/gqls/agentchassis/platform/orchestration"
	"github.com/gqls/agentchassis/platform/validation"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// MessageProcessor handles processing of Kafka messages for agents
type MessageProcessor struct {
	agentType    string
	db           *pgxpool.Pool
	producer     kafka.Producer
	orchestrator *orchestration.SagaCoordinator
	validator    *validation.WorkflowValidator
	configLoader *config.AgentConfigLoader
	logger       *zap.Logger
}

// NewMessageProcessor creates a new message processor
func NewMessageProcessor(
	agentType string,
	db *pgxpool.Pool,
	producer kafka.Producer,
	orchestrator *orchestration.SagaCoordinator,
	validator *validation.WorkflowValidator,
	logger *zap.Logger,
) *MessageProcessor {
	return &MessageProcessor{
		agentType:    agentType,
		db:           db,
		producer:     producer,
		orchestrator: orchestrator,
		validator:    validator,
		configLoader: config.NewAgentConfigLoader(logger),
		logger:       logger,
	}
}

// ProcessMessage handles a single message
func (p *MessageProcessor) ProcessMessage(ctx context.Context, msg kafka.Message) error {
	startTime := time.Now()
	headers := kafka.HeadersToMap(msg.Headers)

	// Create a message context for this specific message
	msgCtx := &MessageContext{
		Message:   msg,
		Headers:   headers,
		StartTime: startTime,
		Logger: p.logger.With(
			zap.String("correlation_id", headers["correlation_id"]),
			zap.String("request_id", headers["request_id"]),
			zap.String("client_id", headers["client_id"]),
			zap.String("agent_instance_id", headers["agent_instance_id"]),
		),
	}

	// Extract action
	if err := msgCtx.ExtractAction(); err != nil {
		return p.handleError(ctx, msgCtx, err, "invalid_payload")
	}

	// Record metrics
	observability.AgentTasksReceived.WithLabelValues(p.agentType, msgCtx.Action).Inc()
	defer func() {
		observability.AgentProcessingDuration.WithLabelValues(p.agentType, msgCtx.Action).
			Observe(time.Since(startTime).Seconds())
	}()

	// Process the message
	if err := p.process(ctx, msgCtx); err != nil {
		return p.handleError(ctx, msgCtx, err, "processing_failed")
	}

	// Success
	observability.AgentTasksProcessed.WithLabelValues(p.agentType, msgCtx.Action, "success").Inc()
	return nil
}

func (p *MessageProcessor) process(ctx context.Context, msgCtx *MessageContext) error {
	// Validate headers
	if err := msgCtx.ValidateHeaders(); err != nil {
		return errors.ValidationError("headers", err.Error())
	}

	// Load agent configuration
	agentConfig, err := p.configLoader.LoadFromDatabase(
		ctx,
		p.db,
		msgCtx.Headers["client_id"],
		msgCtx.Headers["agent_instance_id"],
		p.agentType,
	)
	if err != nil {
		return errors.InternalError("Failed to load configuration", err)
	}

	// Validate workflow
	if err := p.validator.ValidateWorkflowPlan(agentConfig.Workflow); err != nil {
		return errors.New(errors.ErrWorkflowInvalid, "Invalid workflow configuration").
			WithCause(err).
			WithDetail("workflow_metrics", p.validator.GetWorkflowMetrics(agentConfig.Workflow)).
			Build()
	}

	// Execute workflow
	return p.executeWorkflow(ctx, msgCtx, agentConfig)
}

func (p *MessageProcessor) executeWorkflow(ctx context.Context, msgCtx *MessageContext, config *models.AgentConfig) error {
	// Start workflow timer
	workflowTimer := observability.StartWorkflowTimer(p.agentType, config.Workflow.StartStep)
	defer workflowTimer.Complete("success")

	// Update metrics
	observability.WorkflowsStarted.WithLabelValues(p.agentType, config.Workflow.StartStep, msgCtx.Headers["client_id"]).Inc()
	observability.ActiveWorkflows.WithLabelValues(p.agentType).Inc()
	defer observability.ActiveWorkflows.WithLabelValues(p.agentType).Dec()

	// Execute through orchestrator
	return p.orchestrator.ExecuteWorkflow(ctx, config.Workflow, msgCtx.Headers, msgCtx.Message.Value)
}

func (p *MessageProcessor) handleError(ctx context.Context, msgCtx *MessageContext, err error, errorType string) error {
	msgCtx.Logger.Error("Processing failed", zap.Error(err))
	observability.AgentTasksProcessed.WithLabelValues(p.agentType, msgCtx.Action, errorType).Inc()

	// Check for specific error types
	if domainErr, ok := err.(*errors.DomainError); ok {
		if domainErr.Code == errors.ErrInsufficientFuel {
			observability.FuelExhausted.WithLabelValues(p.agentType, msgCtx.Action, msgCtx.Headers["client_id"]).Inc()
		}
		p.sendErrorResponse(ctx, msgCtx, domainErr)
	} else {
		p.sendErrorResponse(ctx, msgCtx, errors.InternalError("Processing failed", err))
	}

	return err
}

func (p *MessageProcessor) sendErrorResponse(ctx context.Context, msgCtx *MessageContext, domainErr *errors.DomainError) {
	responseHeaders := msgCtx.CreateResponseHeaders(p.agentType)
	domainErr.TraceID = msgCtx.Headers["correlation_id"]

	errorResponse := map[string]interface{}{
		"success": false,
		"error":   domainErr,
		"agent":   p.agentType,
	}

	responseBytes, _ := json.Marshal(errorResponse)
	errorTopic := fmt.Sprintf("system.errors.%s", p.agentType)

	if err := p.producer.Produce(ctx, errorTopic, responseHeaders,
		[]byte(msgCtx.Headers["correlation_id"]), responseBytes); err != nil {
		msgCtx.Logger.Error("Failed to send error response", zap.Error(err))
		observability.SystemErrors.WithLabelValues(p.agentType, "produce_error").Inc()
	} else {
		observability.KafkaMessagesProduced.WithLabelValues(errorTopic).Inc()
	}
}
-------------------------------------------------
filepath = ./platform/infrastructure/connections.go
// FILE: platform/infrastructure/connections.go
package infrastructure

import (
	"context"
	"fmt"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/database"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// Connections holds all infrastructure connections
type Connections struct {
	ClientsDB     *pgxpool.Pool
	TemplatesDB   *pgxpool.Pool
	KafkaConsumer *kafka.Consumer
	KafkaProducer kafka.Producer
}

// Manager handles infrastructure lifecycle
type Manager struct {
	logger      *zap.Logger
	connections *Connections
}

// NewManager creates a new infrastructure manager
func NewManager(logger *zap.Logger) *Manager {
	return &Manager{
		logger:      logger,
		connections: &Connections{},
	}
}

// Initialize sets up all infrastructure connections
func (m *Manager) Initialize(ctx context.Context, cfg *config.ServiceConfig, topic, consumerGroup string) error {
	// Initialize database
	clientsPool, err := database.NewPostgresConnection(ctx, cfg.Infrastructure.ClientsDatabase, m.logger)
	if err != nil {
		return fmt.Errorf("failed to connect to clients database: %w", err)
	}
	m.connections.ClientsDB = clientsPool

	// Initialize Templates DB if needed
	if cfg.Infrastructure.TemplatesDatabase.Host != "" {
		templatesPool, err := database.NewPostgresConnection(ctx, cfg.Infrastructure.TemplatesDatabase, m.logger)
		if err != nil {
			m.Close()
			return fmt.Errorf("failed to connect to templates database: %w", err)
		}
		m.connections.TemplatesDB = templatesPool
	}

	// Initialize Kafka
	consumer, err := kafka.NewConsumer(cfg.Infrastructure.KafkaBrokers, topic, consumerGroup, m.logger)
	if err != nil {
		m.Close()
		return fmt.Errorf("failed to create consumer: %w", err)
	}
	m.connections.KafkaConsumer = consumer

	producer, err := kafka.NewProducer(cfg.Infrastructure.KafkaBrokers, m.logger)
	if err != nil {
		m.Close()
		return fmt.Errorf("failed to create producer: %w", err)
	}
	m.connections.KafkaProducer = producer

	return nil
}

// GetConnections returns the infrastructure connections
func (m *Manager) GetConnections() *Connections {
	return m.connections
}

// Close gracefully closes all connections
func (m *Manager) Close() error {
	var errs []error

	if m.connections.KafkaConsumer != nil {
		if err := m.connections.KafkaConsumer.Close(); err != nil {
			errs = append(errs, fmt.Errorf("failed to close kafka consumer: %w", err))
		}
	}

	if m.connections.KafkaProducer != nil {
		if err := m.connections.KafkaProducer.Close(); err != nil {
			errs = append(errs, fmt.Errorf("failed to close kafka producer: %w", err))
		}
	}

	if m.connections.ClientsDB != nil {
		m.connections.ClientsDB.Close()
	}

	if m.connections.TemplatesDB != nil {
		m.connections.TemplatesDB.Close()
	}

	if len(errs) > 0 {
		return fmt.Errorf("infrastructure shutdown errors: %v", errs)
	}
	return nil
}
-------------------------------------------------
filepath = ./platform/observability/metrics.go
// FILE: platform/observability/metrics.go
package observability

import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"net/http"
	"time"
)

var (
	// Workflow metrics
	WorkflowsStarted = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_workflows_started_total",
		Help: "Total number of workflows started",
	}, []string{"agent_type", "workflow_type", "client_id"})

	WorkflowsCompleted = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_workflows_completed_total",
		Help: "Total number of workflows completed",
	}, []string{"agent_type", "workflow_type", "status", "client_id"})

	WorkflowDuration = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_workflow_duration_seconds",
		Help:    "Duration of workflow execution",
		Buckets: prometheus.ExponentialBuckets(0.1, 2, 10), // 0.1s to ~100s
	}, []string{"agent_type", "workflow_type", "status"})

	// Agent metrics
	AgentTasksReceived = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_agent_tasks_received_total",
		Help: "Total number of tasks received by agents",
	}, []string{"agent_type", "action"})

	AgentTasksProcessed = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_agent_tasks_processed_total",
		Help: "Total number of tasks processed by agents",
	}, []string{"agent_type", "action", "status"})

	AgentProcessingDuration = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_agent_processing_duration_seconds",
		Help:    "Duration of agent task processing",
		Buckets: prometheus.ExponentialBuckets(0.01, 2, 10), // 10ms to ~10s
	}, []string{"agent_type", "action"})

	// Fuel metrics
	FuelConsumed = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_fuel_consumed_total",
		Help: "Total fuel consumed by operations",
	}, []string{"agent_type", "action", "client_id"})

	FuelExhausted = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_fuel_exhausted_total",
		Help: "Total number of operations that failed due to insufficient fuel",
	}, []string{"agent_type", "action", "client_id"})

	// Kafka metrics
	KafkaMessagesProduced = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_kafka_messages_produced_total",
		Help: "Total number of messages produced to Kafka",
	}, []string{"topic"})

	KafkaMessagesConsumed = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_kafka_messages_consumed_total",
		Help: "Total number of messages consumed from Kafka",
	}, []string{"topic", "consumer_group"})

	KafkaConsumerLag = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "ai_persona_kafka_consumer_lag",
		Help: "Current consumer lag in messages",
	}, []string{"topic", "consumer_group", "partition"})

	// Database metrics
	DatabaseQueries = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_database_queries_total",
		Help: "Total number of database queries",
	}, []string{"database", "operation", "table"})

	DatabaseQueryDuration = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_database_query_duration_seconds",
		Help:    "Duration of database queries",
		Buckets: prometheus.ExponentialBuckets(0.001, 2, 10), // 1ms to ~1s
	}, []string{"database", "operation"})

	// AI Service metrics
	AIServiceRequests = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_ai_service_requests_total",
		Help: "Total number of AI service requests",
	}, []string{"provider", "model", "operation"})

	AIServiceTokensUsed = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_ai_service_tokens_used_total",
		Help: "Total number of tokens used by AI services",
	}, []string{"provider", "model", "type"}) // type: input/output

	AIServiceLatency = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_ai_service_latency_seconds",
		Help:    "Latency of AI service requests",
		Buckets: prometheus.ExponentialBuckets(0.1, 2, 10), // 100ms to ~100s
	}, []string{"provider", "model", "operation"})

	// Memory/Vector DB metrics
	VectorSearchQueries = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_vector_search_queries_total",
		Help: "Total number of vector similarity searches",
	}, []string{"agent_type"})

	VectorSearchLatency = promauto.NewHistogramVec(prometheus.HistogramOpts{
		Name:    "ai_persona_vector_search_latency_seconds",
		Help:    "Latency of vector similarity searches",
		Buckets: prometheus.ExponentialBuckets(0.01, 2, 8), // 10ms to ~2.5s
	}, []string{"agent_type"})

	MemoriesStored = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_memories_stored_total",
		Help: "Total number of memories stored",
	}, []string{"agent_type", "memory_type"})

	// Circuit breaker metrics
	CircuitBreakerState = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "ai_persona_circuit_breaker_state",
		Help: "Current state of circuit breakers (0=closed, 1=open, 2=half-open)",
	}, []string{"service"})

	CircuitBreakerTrips = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_circuit_breaker_trips_total",
		Help: "Total number of circuit breaker trips",
	}, []string{"service"})

	// System health metrics
	ActiveWorkflows = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "ai_persona_active_workflows",
		Help: "Number of currently active workflows",
	}, []string{"agent_type"})

	AgentPoolSize = promauto.NewGaugeVec(prometheus.GaugeOpts{
		Name: "ai_persona_agent_pool_size",
		Help: "Current size of agent pools",
	}, []string{"agent_type"})

	SystemErrors = promauto.NewCounterVec(prometheus.CounterOpts{
		Name: "ai_persona_system_errors_total",
		Help: "Total number of system errors",
	}, []string{"service", "error_code"})
)

// MetricsServer provides an HTTP server for Prometheus metrics
type MetricsServer struct {
	port string
}

// NewMetricsServer creates a new metrics server
func NewMetricsServer(port string) *MetricsServer {
	return &MetricsServer{port: port}
}

// Start starts the metrics HTTP server
func (m *MetricsServer) Start() error {
	http.Handle("/metrics", promhttp.Handler())
	http.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		w.Write([]byte("OK"))
	})

	return http.ListenAndServe(":"+m.port, nil)
}

// WorkflowTimer helps track workflow execution time
type WorkflowTimer struct {
	agentType    string
	workflowType string
	startTime    time.Time
	timer        *prometheus.Timer
}

// StartWorkflowTimer starts timing a workflow execution
func StartWorkflowTimer(agentType, workflowType string) *WorkflowTimer {
	return &WorkflowTimer{
		agentType:    agentType,
		workflowType: workflowType,
		timer: prometheus.NewTimer(prometheus.ObserverFunc(func(v float64) {
			WorkflowDuration.WithLabelValues(agentType, workflowType, "unknown").Observe(v)
		})),
	}
}

// Complete marks the workflow as completed with the given status
func (wt *WorkflowTimer) Complete(status string) {
	// The ObserveDuration method records the observation on the histogram provided to NewTimer.
	// It's designed to be called once. By calling it here, we override the "unknown" status.
	duration := wt.timer.ObserveDuration()
	WorkflowDuration.WithLabelValues(wt.agentType, wt.workflowType, status).Observe(duration.Seconds())
}

// CircuitBreakerStateValue converts circuit breaker state to numeric value
func CircuitBreakerStateValue(state string) float64 {
	switch state {
	case "closed":
		return 0
	case "open":
		return 1
	case "half-open":
		return 2
	default:
		return -1
	}
}
-------------------------------------------------
filepath = ./platform/observability/tracing.go
// FILE: platform/observability/tracing.go
package observability

import (
	"context"
	"fmt"

	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
	"go.opentelemetry.io/otel/propagation"
	"go.opentelemetry.io/otel/sdk/resource"
	sdktrace "go.opentelemetry.io/otel/sdk/trace"
	semconv "go.opentelemetry.io/otel/semconv/v1.17.0"
	"go.opentelemetry.io/otel/trace"
	"go.uber.org/zap"
)

// TracingConfig holds configuration for tracing
type TracingConfig struct {
	ServiceName    string
	ServiceVersion string
	Environment    string
	Endpoint       string
}

// InitTracing initializes OpenTelemetry tracing
func InitTracing(ctx context.Context, cfg TracingConfig, logger *zap.Logger) (func(), error) {
	// Create OTLP exporter
	exporter, err := otlptrace.New(
		ctx,
		otlptracegrpc.NewClient(
			otlptracegrpc.WithEndpoint(cfg.Endpoint),
			otlptracegrpc.WithInsecure(),
		),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create OTLP exporter: %w", err)
	}

	// Create resource
	res, err := resource.New(ctx,
		resource.WithAttributes(
			semconv.ServiceNameKey.String(cfg.ServiceName),
			semconv.ServiceVersionKey.String(cfg.ServiceVersion),
			attribute.String("environment", cfg.Environment),
		),
		resource.WithHost(),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create resource: %w", err)
	}

	// Create tracer provider
	tracerProvider := sdktrace.NewTracerProvider(
		sdktrace.WithBatcher(exporter),
		sdktrace.WithResource(res),
		sdktrace.WithSampler(sdktrace.AlwaysSample()),
	)

	// Set global tracer provider
	otel.SetTracerProvider(tracerProvider)

	// Set global propagator
	otel.SetTextMapPropagator(
		propagation.NewCompositeTextMapPropagator(
			propagation.TraceContext{},
			propagation.Baggage{},
		),
	)

	logger.Info("Tracing initialized",
		zap.String("service", cfg.ServiceName),
		zap.String("endpoint", cfg.Endpoint),
	)

	// Return cleanup function
	cleanup := func() {
		if err := tracerProvider.Shutdown(ctx); err != nil {
			logger.Error("Failed to shutdown tracer provider", zap.Error(err))
		}
	}

	return cleanup, nil
}

// StartSpan creates a new span with standard attributes
func StartSpan(ctx context.Context, name string, opts ...trace.SpanStartOption) (context.Context, trace.Span) {
	tracer := otel.Tracer("ai-persona-system")
	return tracer.Start(ctx, name, opts...)
}

// AddSpanAttributes adds attributes to the current span
func AddSpanAttributes(ctx context.Context, attrs ...attribute.KeyValue) {
	span := trace.SpanFromContext(ctx)
	span.SetAttributes(attrs...)
}

// RecordError records an error on the current span
func RecordError(ctx context.Context, err error) {
	span := trace.SpanFromContext(ctx)
	span.RecordError(err)
}
-------------------------------------------------
filepath = ./platform/health/server.go
// FILE: platform/health/server.go
package health

import (
	"context"
	"encoding/json"
	"net/http"

	"github.com/prometheus/client_golang/prometheus/promhttp"
	"go.uber.org/zap"
)

// CheckFunc is a health check function
type CheckFunc func(ctx context.Context) error

// Checkers is a map of named health checks
type Checkers map[string]CheckFunc

// Config for health server
type Config struct {
	HealthPort  string
	MetricsPort string
}

// Server handles health and metrics endpoints
type Server struct {
	serviceName string
	config      Config
	checkers    Checkers
	logger      *zap.Logger
}

// NewServer creates a new health server
func NewServer(serviceName string, config Config, checkers Checkers, logger *zap.Logger) *Server {
	return &Server{
		serviceName: serviceName,
		config:      config,
		checkers:    checkers,
		logger:      logger,
	}
}

// Start starts the health and metrics servers
func (s *Server) Start() {
	go s.startMetricsServer()
	go s.startHealthServer()
}

func (s *Server) startMetricsServer() {
	mux := http.NewServeMux()
	mux.Handle("/metrics", promhttp.Handler())

	s.logger.Info("Starting metrics server", zap.String("port", s.config.MetricsPort))
	if err := http.ListenAndServe(":"+s.config.MetricsPort, mux); err != nil {
		s.logger.Error("Metrics server failed", zap.Error(err))
	}
}

func (s *Server) startHealthServer() {
	mux := http.NewServeMux()
	mux.HandleFunc("/health", s.handleHealth)
	mux.HandleFunc("/ready", s.handleReady)

	s.logger.Info("Starting health server", zap.String("port", s.config.HealthPort))
	if err := http.ListenAndServe(":"+s.config.HealthPort, mux); err != nil {
		s.logger.Error("Health server failed", zap.Error(err))
	}
}

func (s *Server) handleHealth(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	checks := make(map[string]interface{})
	healthy := true

	for name, checker := range s.checkers {
		if err := checker(ctx); err != nil {
			checks[name] = map[string]interface{}{
				"status": "unhealthy",
				"error":  err.Error(),
			}
			healthy = false
		} else {
			checks[name] = map[string]interface{}{
				"status": "healthy",
			}
		}
	}

	response := map[string]interface{}{
		"service": s.serviceName,
		"status":  "healthy",
		"checks":  checks,
	}

	if !healthy {
		response["status"] = "unhealthy"
		w.WriteHeader(http.StatusServiceUnavailable)
	} else {
		w.WriteHeader(http.StatusOK)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

func (s *Server) handleReady(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()

	for _, checker := range s.checkers {
		if err := checker(ctx); err != nil {
			w.WriteHeader(http.StatusServiceUnavailable)
			w.Write([]byte("NOT READY"))
			return
		}
	}

	w.WriteHeader(http.StatusOK)
	w.Write([]byte("READY"))
}
-------------------------------------------------
filepath = ./platform/validation/workflow.go
// FILE: platform/validation/workflow.go
package validation

import (
	"fmt"
	"github.com/gqls/agentchassis/pkg/models"
)

// WorkflowValidator provides validation for workflow plans
type WorkflowValidator struct{}

// NewWorkflowValidator creates a new workflow validator
func NewWorkflowValidator() *WorkflowValidator {
	return &WorkflowValidator{}
}

// ValidateWorkflowPlan validates a workflow plan for correctness
func (v *WorkflowValidator) ValidateWorkflowPlan(plan models.WorkflowPlan) error {
	if plan.StartStep == "" {
		return fmt.Errorf("workflow plan must have a start step")
	}

	if len(plan.Steps) == 0 {
		return fmt.Errorf("workflow plan must have at least one step")
	}

	// Check start step exists
	if _, ok := plan.Steps[plan.StartStep]; !ok {
		return fmt.Errorf("start step '%s' not found in steps", plan.StartStep)
	}

	// Validate each step
	for stepName, step := range plan.Steps {
		if err := v.validateStep(stepName, step, plan); err != nil {
			return err
		}
	}

	// Check for cycles
	if err := v.checkForCycles(plan); err != nil {
		return err
	}

	// Check all dependencies exist
	if err := v.validateDependencies(plan); err != nil {
		return err
	}

	return nil
}

// validateStep validates an individual step
func (v *WorkflowValidator) validateStep(name string, step models.Step, plan models.WorkflowPlan) error {
	if step.Action == "" {
		return fmt.Errorf("step '%s' must have an action", name)
	}

	// Validate based on action type
	switch step.Action {
	case "fan_out":
		if len(step.SubTasks) == 0 {
			return fmt.Errorf("fan_out step '%s' must have at least one sub-task", name)
		}
		for i, subTask := range step.SubTasks {
			if subTask.StepName == "" {
				return fmt.Errorf("sub-task %d in step '%s' must have a step name", i, name)
			}
			if subTask.Topic == "" {
				return fmt.Errorf("sub-task %d in step '%s' must have a topic", i, name)
			}
		}
	case "complete_workflow":
		if step.NextStep != "" {
			return fmt.Errorf("complete_workflow step '%s' should not have a next step", name)
		}
	default:
		// For standard actions, ensure topic is set if not internal actions
		if step.Topic == "" && !v.isInternalAction(step.Action) {
			return fmt.Errorf("step '%s' with action '%s' requires a topic", name, step.Action)
		}
	}

	// Validate next step exists
	if step.NextStep != "" {
		if _, ok := plan.Steps[step.NextStep]; !ok {
			return fmt.Errorf("step '%s' references non-existent next step '%s'", name, step.NextStep)
		}
	}

	return nil
}

// isInternalAction checks if an action is handled internally
func (v *WorkflowValidator) isInternalAction(action string) bool {
	internalActions := map[string]bool{
		"complete_workflow":     true,
		"pause_for_human_input": true,
		"store_memory":          true,
		"retrieve_memory":       true,
	}
	return internalActions[action]
}

// validateDependencies ensures all dependencies exist
func (v *WorkflowValidator) validateDependencies(plan models.WorkflowPlan) error {
	for stepName, step := range plan.Steps {
		for _, dep := range step.Dependencies {
			if _, ok := plan.Steps[dep]; !ok {
				return fmt.Errorf("step '%s' has dependency on non-existent step '%s'", stepName, dep)
			}
		}
	}
	return nil
}

// checkForCycles detects cycles in the workflow
func (v *WorkflowValidator) checkForCycles(plan models.WorkflowPlan) error {
	visited := make(map[string]bool)
	recStack := make(map[string]bool)

	var hasCycle func(string) bool
	hasCycle = func(stepName string) bool {
		visited[stepName] = true
		recStack[stepName] = true

		step, ok := plan.Steps[stepName]
		if !ok {
			return false
		}

		// Check next step
		if step.NextStep != "" {
			if !visited[step.NextStep] {
				if hasCycle(step.NextStep) {
					return true
				}
			} else if recStack[step.NextStep] {
				return true
			}
		}

		// Check dependencies
		for _, dep := range step.Dependencies {
			if !visited[dep] {
				if hasCycle(dep) {
					return true
				}
			} else if recStack[dep] {
				return true
			}
		}

		recStack[stepName] = false
		return false
	}

	// Start from the start step
	if hasCycle(plan.StartStep) {
		return fmt.Errorf("workflow contains a cycle")
	}

	// Check any unvisited steps (disconnected components)
	for stepName := range plan.Steps {
		if !visited[stepName] {
			if hasCycle(stepName) {
				return fmt.Errorf("workflow contains a cycle")
			}
		}
	}

	return nil
}

// GetWorkflowMetrics calculates metrics about a workflow
func (v *WorkflowValidator) GetWorkflowMetrics(plan models.WorkflowPlan) map[string]interface{} {
	metrics := map[string]interface{}{
		"total_steps":    len(plan.Steps),
		"fan_out_steps":  0,
		"external_calls": 0,
		"max_depth":      0,
	}

	for _, step := range plan.Steps {
		if step.Action == "fan_out" {
			metrics["fan_out_steps"] = metrics["fan_out_steps"].(int) + 1
		}
		if step.Topic != "" {
			metrics["external_calls"] = metrics["external_calls"].(int) + 1
		}
	}

	// Calculate max depth
	metrics["max_depth"] = v.calculateMaxDepth(plan)

	return metrics
}

// calculateMaxDepth calculates the maximum depth of the workflow
func (v *WorkflowValidator) calculateMaxDepth(plan models.WorkflowPlan) int {
	depths := make(map[string]int)

	var calculateDepth func(string) int
	calculateDepth = func(stepName string) int {
		if depth, ok := depths[stepName]; ok {
			return depth
		}

		step, ok := plan.Steps[stepName]
		if !ok {
			return 0
		}

		maxDepth := 0

		// Check dependencies
		for _, dep := range step.Dependencies {
			depDepth := calculateDepth(dep)
			if depDepth > maxDepth {
				maxDepth = depDepth
			}
		}

		// Check next step
		if step.NextStep != "" {
			nextDepth := calculateDepth(step.NextStep)
			if nextDepth > maxDepth {
				maxDepth = nextDepth
			}
		}

		depths[stepName] = maxDepth + 1
		return maxDepth + 1
	}

	return calculateDepth(plan.StartStep)
}
-------------------------------------------------
filepath = ./platform/config/agent_config_loader.go
// FILE: platform/config/agent_config_loader.go
package config

import (
	"context"
	"encoding/json"
	"fmt"

	"github.com/gqls/agentchassis/pkg/models"
	"github.com/jackc/pgx/v5"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// AgentConfigLoader handles loading agent configurations from the database
type AgentConfigLoader struct {
	logger *zap.Logger
}

// NewAgentConfigLoader creates a new agent config loader
func NewAgentConfigLoader(logger *zap.Logger) *AgentConfigLoader {
	return &AgentConfigLoader{logger: logger}
}

// LoadFromDatabase fetches agent configuration from a client-specific schema
func (l *AgentConfigLoader) LoadFromDatabase(ctx context.Context, db *pgxpool.Pool, clientID, agentInstanceID, agentType string) (*models.AgentConfig, error) {
	query := fmt.Sprintf(`
		SELECT name, config, template_id 
		FROM client_%s.agent_instances 
		WHERE id = $1 AND is_active = true
	`, clientID)

	var name string
	var configJSON []byte
	var templateID string

	err := db.QueryRow(ctx, query, agentInstanceID).Scan(&name, &configJSON, &templateID)
	if err != nil {
		if err == pgx.ErrNoRows {
			l.logger.Warn("Agent instance not found, using default configuration",
				zap.String("agent_instance_id", agentInstanceID))
			return l.GetDefaultConfig(agentInstanceID, agentType), nil
		}
		return nil, fmt.Errorf("failed to query agent instance: %w", err)
	}

	// Parse and build config
	return l.parseConfig(configJSON, agentInstanceID, agentType)
}

// LoadFromJSON loads agent configuration from JSON data
func (l *AgentConfigLoader) LoadFromJSON(data []byte, agentInstanceID, agentType string) (*models.AgentConfig, error) {
	return l.parseConfig(data, agentInstanceID, agentType)
}

// GetDefaultConfig returns a default configuration for an agent type
func (l *AgentConfigLoader) GetDefaultConfig(agentInstanceID, agentType string) *models.AgentConfig {
	return &models.AgentConfig{
		AgentID:   agentInstanceID,
		AgentType: agentType,
		Version:   1,
		CoreLogic: l.getDefaultCoreLogic(agentType),
		Workflow:  l.getDefaultWorkflow(agentType),
	}
}

func (l *AgentConfigLoader) parseConfig(configJSON []byte, agentInstanceID, agentType string) (*models.AgentConfig, error) {
	var config map[string]interface{}
	if err := json.Unmarshal(configJSON, &config); err != nil {
		return nil, fmt.Errorf("failed to parse agent config: %w", err)
	}

	// Extract workflow
	var workflow models.WorkflowPlan
	if workflowData, ok := config["workflow"]; ok {
		workflowBytes, _ := json.Marshal(workflowData)
		if err := json.Unmarshal(workflowBytes, &workflow); err != nil {
			l.logger.Warn("Failed to parse workflow, using default", zap.Error(err))
			workflow = l.getDefaultWorkflow(agentType)
		}
	} else {
		workflow = l.getDefaultWorkflow(agentType)
	}

	// Extract memory configuration
	var memoryConfig models.MemoryConfiguration
	if memData, ok := config["memory_config"]; ok {
		memBytes, _ := json.Marshal(memData)
		json.Unmarshal(memBytes, &memoryConfig)
	}

	return &models.AgentConfig{
		AgentID:      agentInstanceID,
		AgentType:    agentType,
		Version:      1,
		CoreLogic:    config,
		Workflow:     workflow,
		MemoryConfig: memoryConfig,
	}, nil
}

func (l *AgentConfigLoader) getDefaultCoreLogic(agentType string) map[string]interface{} {
	// Different defaults for different agent types
	switch agentType {
	case "copywriter":
		return map[string]interface{}{
			"model":       "claude-3-sonnet",
			"temperature": 0.7,
			"max_tokens":  2000,
		}
	case "researcher":
		return map[string]interface{}{
			"model":       "claude-3-opus",
			"temperature": 0.3,
			"max_tokens":  4000,
		}
	default:
		return map[string]interface{}{
			"model":       "claude-3-haiku",
			"temperature": 0.5,
			"max_tokens":  1000,
		}
	}
}

func (l *AgentConfigLoader) getDefaultWorkflow(agentType string) models.WorkflowPlan {
	// Could have type-specific default workflows
	return models.WorkflowPlan{
		StartStep: "process",
		Steps: map[string]models.Step{
			"process": {
				Action:      "ai_text_generate",
				Description: "Process the request",
				NextStep:    "complete",
			},
			"complete": {
				Action:      "complete_workflow",
				Description: "Mark workflow as complete",
			},
		},
	}
}
-------------------------------------------------
filepath = ./platform/config/loader.go
// FILE: platform/config/loader.go
package config

import (
	"fmt"
	"log"
	"strings"

	"github.com/spf13/viper"
)

// ServiceConfig is the top-level configuration struct for any service
type ServiceConfig struct {
	ServiceInfo    ServiceInfoConfig      `mapstructure:"service_info"`
	Server         ServerConfig           `mapstructure:"server"`
	Logging        LoggingConfig          `mapstructure:"logging"`
	Observability  ObservabilityConfig    `mapstructure:"observability"`
	Infrastructure InfrastructureConfig   `mapstructure:"infrastructure"`
	Custom         map[string]interface{} `mapstructure:"custom"`
}

type ServiceInfoConfig struct {
	Name        string `mapstructure:"name"`
	Version     string `mapstructure:"version"`
	Environment string `mapstructure:"environment"`
}

type ServerConfig struct {
	Port string `mapstructure:"port"`
}

type LoggingConfig struct {
	Level string `mapstructure:"level"`
}

type ObservabilityConfig struct {
	TracingEndpoint string `mapstructure:"tracing_endpoint"`
}

type InfrastructureConfig struct {
	KafkaBrokers      []string            `mapstructure:"kafka_brokers"`
	ClientsDatabase   DatabaseConfig      `mapstructure:"clients_database"`
	TemplatesDatabase DatabaseConfig      `mapstructure:"templates_database"`
	AuthDatabase      DatabaseConfig      `mapstructure:"auth_database"`
	ObjectStorage     ObjectStorageConfig `mapstructure:"object_storage"`
}

type DatabaseConfig struct {
	Host           string `mapstructure:"host"`
	Port           int    `mapstructure:"port"`
	User           string `mapstructure:"user"`
	PasswordEnvVar string `mapstructure:"password_env_var"`
	DBName         string `mapstructure:"db_name"`
	SSLMode        string `mapstructure:"sslmode"`
}

type ObjectStorageConfig struct {
	Provider        string `mapstructure:"provider"`
	Endpoint        string `mapstructure:"endpoint"`
	Bucket          string `mapstructure:"bucket"`
	AccessKeyEnvVar string `mapstructure:"access_key_env_var"`
	SecretKeyEnvVar string `mapstructure:"secret_key_env_var"`
}

// Load reads a YAML config file and overrides with environment variables
func Load(path string) (*ServiceConfig, error) {
	v := viper.New()
	v.SetDefault("server.port", "8080")
	v.SetDefault("logging.level", "info")
	v.SetDefault("infrastructure.database.sslmode", "disable")

	v.SetConfigFile(path)
	v.SetConfigType("yaml")
	if err := v.ReadInConfig(); err != nil {
		if _, ok := err.(viper.ConfigFileNotFoundError); ok {
			log.Printf("config: file not found at %s, relying on defaults and environment variables", path)
		} else {
			return nil, fmt.Errorf("config: error reading config file %s: %w", path, err)
		}
	}

	v.SetEnvPrefix("SERVICE")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
	v.AutomaticEnv()

	var cfg ServiceConfig
	if err := v.Unmarshal(&cfg); err != nil {
		return nil, fmt.Errorf("config: unable to unmarshal config: %w", err)
	}

	return &cfg, nil
}
-------------------------------------------------
filepath = ./platform/agentbase/agent_test.go
// FILE: platform/agentbase/agent_test.go
package agentbase

import (
	"context"
	"testing"

	"github.com/segmentio/kafka-go"
	"github.com/stretchr/testify/mock"
)

// MockKafkaConsumer for testing
type MockKafkaConsumer struct {
	mock.Mock
}

func (m *MockKafkaConsumer) FetchMessage(ctx context.Context) (kafka.Message, error) {
	args := m.Called(ctx)
	return args.Get(0).(kafka.Message), args.Error(1)
}

func (m *MockKafkaConsumer) CommitMessages(ctx context.Context, msgs ...kafka.Message) error {
	args := m.Called(ctx, msgs)
	return args.Error(0)
}

func (m *MockKafkaConsumer) Close() error {
	return nil
}

func TestAgentHandleMessage(t *testing.T) {
	// This test needs to be redesigned since handleMessage is private
	// and the Agent struct expects real Kafka connections
	// For now, we'll skip this test or make it integration-only
	t.Skip("Skipping unit test that requires real Kafka connections")
}
-------------------------------------------------
filepath = ./platform/agentbase/agent.go
// FILE: platform/agentbase/agent.go (refactored)
package agentbase

import (
	"context"
	"fmt"
	"github.com/gqls/agentchassis/platform/config"
	"github.com/gqls/agentchassis/platform/health"
	"github.com/gqls/agentchassis/platform/infrastructure"
	"github.com/gqls/agentchassis/platform/messaging"
	"github.com/gqls/agentchassis/platform/observability"
	"github.com/gqls/agentchassis/platform/orchestration"
	"github.com/gqls/agentchassis/platform/validation"
	"github.com/jackc/pgx/v5/stdlib"
	"go.uber.org/zap"
)

// Agent represents a generic agent chassis
type Agent struct {
	ctx           context.Context
	cfg           *config.ServiceConfig
	logger        *zap.Logger
	agentType     string
	consumerGroup string

	// Managers
	infraManager  *infrastructure.Manager
	messageRunner *MessageRunner
	healthServer  *health.Server
}

// New creates a new agent with defaults from config
func New(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger) (*Agent, error) {
	agentType := "generic"
	topic := "system.agent.generic.process"

	if cfg.Custom != nil {
		if at, ok := cfg.Custom["agent_type"].(string); ok {
			agentType = at
		}
		if t, ok := cfg.Custom["topic"].(string); ok {
			topic = t
		}
	}

	return NewWithType(ctx, cfg, logger, agentType, topic)
}

// NewWithType creates an agent with specific type and topic
func NewWithType(ctx context.Context, cfg *config.ServiceConfig, logger *zap.Logger, agentType string, topic string) (*Agent, error) {
	// Consumer group
	consumerGroup := fmt.Sprintf("%s-group", agentType)
	if cfg.Custom != nil {
		if cg, ok := cfg.Custom["kafka_consumer_group"].(string); ok {
			consumerGroup = cg
		}
	}

	// Initialize infrastructure
	infraManager := infrastructure.NewManager(logger)
	if err := infraManager.Initialize(ctx, cfg, topic, consumerGroup); err != nil {
		return nil, fmt.Errorf("failed to initialize infrastructure: %w", err)
	}

	connections := infraManager.GetConnections()

	// Create components
	components, err := createComponents(connections, agentType, logger)
	if err != nil {
		infraManager.Close()
		return nil, fmt.Errorf("failed to create components: %w", err)
	}

	// Create message runner
	messageRunner := NewMessageRunner(
		ctx,
		logger,
		connections.KafkaConsumer,
		components.messageProcessor,
		consumerGroup,
		agentType,
	)

	// Create health server
	healthServer := createHealthServer(cfg, connections, agentType, logger)

	// Record metrics
	observability.AgentPoolSize.WithLabelValues(agentType).Inc()

	return &Agent{
		ctx:           ctx,
		cfg:           cfg,
		logger:        logger,
		agentType:     agentType,
		consumerGroup: consumerGroup,
		infraManager:  infraManager,
		messageRunner: messageRunner,
		healthServer:  healthServer,
	}, nil
}

// Components holds the processing components
type Components struct {
	messageProcessor *messaging.MessageProcessor
	orchestrator     *orchestration.SagaCoordinator
	validator        *validation.WorkflowValidator
}

func createComponents(connections *infrastructure.Connections, agentType string, logger *zap.Logger) (*Components, error) {
	// Create orchestrator
	connConfig := connections.ClientsDB.Config().ConnConfig.Copy()
	stdDB := stdlib.OpenDB(*connConfig)
	orchestrator := orchestration.NewSagaCoordinator(stdDB, connections.KafkaProducer, logger)

	// Create validator
	validator := validation.NewWorkflowValidator()

	// Create message processor
	messageProcessor := messaging.NewMessageProcessor(
		agentType,
		connections.ClientsDB,
		connections.KafkaProducer,
		orchestrator,
		validator,
		logger,
	)

	return &Components{
		messageProcessor: messageProcessor,
		orchestrator:     orchestrator,
		validator:        validator,
	}, nil
}

func createHealthServer(cfg *config.ServiceConfig, connections *infrastructure.Connections, agentType string, logger *zap.Logger) *health.Server {
	return health.NewServer(
		agentType,
		health.Config{
			HealthPort:  "8080",
			MetricsPort: "9090",
		},
		health.Checkers{
			"database": func(ctx context.Context) error {
				return connections.ClientsDB.Ping(ctx)
			},
			"kafka": func(ctx context.Context) error {
				// Simplified check - could be enhanced
				return nil
			},
		},
		logger,
	)
}

// Run starts the agent
func (a *Agent) Run() error {
	a.logger.Info("Agent starting", zap.String("type", a.agentType))

	// Start health server
	a.healthServer.Start()

	// Run message processing
	return a.messageRunner.Run()
}

// Shutdown gracefully shuts down the agent
func (a *Agent) Shutdown() error {
	a.logger.Info("Agent shutting down")
	observability.AgentPoolSize.WithLabelValues(a.agentType).Dec()
	return a.infraManager.Close()
}
-------------------------------------------------
filepath = ./platform/agentbase/runner.go
// FILE: platform/agentbase/runner.go
package agentbase

import (
	"context"
	"github.com/gqls/agentchassis/platform/kafka"
	"github.com/gqls/agentchassis/platform/messaging"
	"github.com/gqls/agentchassis/platform/observability"
	"go.uber.org/zap"
	"time"
)

// MessageRunner handles the message processing loop
type MessageRunner struct {
	ctx           context.Context
	logger        *zap.Logger
	consumer      *kafka.Consumer
	processor     *messaging.MessageProcessor
	consumerGroup string
	agentType     string
}

// NewMessageRunner creates a new message runner
func NewMessageRunner(
	ctx context.Context,
	logger *zap.Logger,
	consumer *kafka.Consumer,
	processor *messaging.MessageProcessor,
	consumerGroup string,
	agentType string,
) *MessageRunner {
	return &MessageRunner{
		ctx:           ctx,
		logger:        logger,
		consumer:      consumer,
		processor:     processor,
		consumerGroup: consumerGroup,
		agentType:     agentType,
	}
}

// Run starts the message processing loop
func (r *MessageRunner) Run() error {
	r.logger.Info("Starting message runner", zap.String("agent_type", r.agentType))

	for {
		select {
		case <-r.ctx.Done():
			r.logger.Info("Message runner shutting down")
			return nil
		default:
			msg, err := r.consumer.FetchMessage(r.ctx)
			if err != nil {
				if err == context.Canceled {
					continue
				}
				r.logger.Error("Failed to fetch message", zap.Error(err))
				observability.SystemErrors.WithLabelValues(r.agentType, "fetch_message").Inc()
				time.Sleep(1 * time.Second)
				continue
			}

			// Record metric
			observability.KafkaMessagesConsumed.WithLabelValues(msg.Topic, r.consumerGroup).Inc()

			// Process asynchronously
			go r.processMessage(msg)
		}
	}
}

func (r *MessageRunner) processMessage(msg kafka.Message) {
	if err := r.processor.ProcessMessage(r.ctx, msg); err != nil {
		r.logger.Error("Failed to process message", zap.Error(err))
	}

	// Always commit
	if err := r.consumer.CommitMessages(context.Background(), msg); err != nil {
		r.logger.Error("Failed to commit message", zap.Error(err))
		observability.SystemErrors.WithLabelValues(r.agentType, "commit_message").Inc()
	}
}
-------------------------------------------------
filepath = ./platform/contracts/contracts.go
// FILE: platform/contracts/contracts.go
// This package defines the core data structures used for agent configuration
// and communication throughout the entire system. It is the "shared language"
// that all services and agents will use.
package contracts

// AgentConfig is the master configuration for a single agent instance.
// This is stored as a JSONB object in the `agent_instances` table and
// loaded by the Agent Chassis at runtime to determine its behavior.
type AgentConfig struct {
	AgentID   string          `json:"agent_id"`
	AgentType string          `json:"agent_type"` // e.g., "copywriter", "orchestrator", "reasoning-agent"
	Version   int             `json:"version"`
	CoreLogic CoreLogicConfig `json:"core_logic"` // The "Who" - parameters for its main skill
	Workflow  WorkflowPlan    `json:"workflow"`   // The "How" - the plan it follows to do its job
}

// CoreLogicConfig is a generic map to hold the specific parameters
// for an agent's primary function.
// For an AI agent, this would contain prompts and model info.
// For an adapter, it might contain API endpoints and credentials.
type CoreLogicConfig map[string]interface{}

// WorkflowPlan defines the orchestration logic for an agent.
// It is a declarative, directed graph of steps.
type WorkflowPlan struct {
	StartStep string          `json:"start_step"`
	Steps     map[string]Step `json:"steps"`
}

// Step represents a single node in the workflow graph. It can be either
// a direct action for the agent to perform, or a call to another agent.
type Step struct {
	// Action is the specific function this agent should perform for this step.
	// e.g., "generate_text", "fan_out", "pause_for_human_input".
	Action string `json:"action"`

	// Description provides a human-readable explanation of the step's purpose.
	Description string `json:"description"`

	// Topic is the Kafka topic to send a message to if this step involves
	// calling another agent or service.
	Topic string `json:"topic,omitempty"`

	// Dependencies lists the `step_name`s that must be completed before
	// this step can begin. The orchestrator will not execute this step
	// until it has received responses from all dependencies.
	Dependencies []string `json:"dependencies,omitempty"`

	// NextStep defines the name of the next step to execute upon successful
	// completion of this one, for simple linear workflows.
	NextStep string `json:"next_step,omitempty"`

	// SubTasks is used for "fan_out" actions, defining a list of parallel
	// tasks to be executed.
	SubTasks []SubTask `json:"sub_tasks,omitempty"`
}

// SubTask defines a single task to be executed in parallel within a "fan_out" step.
type SubTask struct {
	// StepName is the logical name for this sub-task, used for dependency tracking.
	StepName string `json:"step_name"`
	// Topic is the Kafka topic to which the request for this sub-task will be sent.
	Topic string `json:"topic"`
}
-------------------------------------------------
filepath = ./platform/orchestration/coordinator_test.go
// FILE: platform/orchestration/coordinator_test.go
package orchestration

import (
	"context"
	"database/sql"
	"encoding/json"
	"testing"
	"time"

	"github.com/DATA-DOG/go-sqlmock"
	"github.com/google/uuid"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/governance"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
	"github.com/stretchr/testify/require"
	"go.uber.org/zap"
)

// MockKafkaProducer allows us to test the coordinator without a real Kafka connection.
type MockKafkaProducer struct {
	mock.Mock
}

func (m *MockKafkaProducer) Produce(ctx context.Context, topic string, headers map[string]string, key, value []byte) error {
	args := m.Called(ctx, topic, headers, key, value)
	return args.Error(0)
}

func (m *MockKafkaProducer) Close() error {
	args := m.Called()
	return args.Error(0)
}

// setupTest creates the coordinator with mocked dependencies for testing.
func setupTest(t *testing.T) (*SagaCoordinator, *MockKafkaProducer, *sql.DB, sqlmock.Sqlmock) {
	db, mockDB, err := sqlmock.New()
	require.NoError(t, err)

	mockProducer := new(MockKafkaProducer)
	logger := zap.NewNop()

	coordinator := NewSagaCoordinator(db, mockProducer, logger)
	require.NotNil(t, coordinator)

	return coordinator, mockProducer, db, mockDB
}

// TestExecuteWorkflow_InitialStep verifies the start of a new workflow.
func TestExecuteWorkflow_InitialStep(t *testing.T) {
	coordinator, mockProducer, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id":      correlationID,
		"request_id":          uuid.NewString(),
		"client_id":           clientID,
		governance.FuelHeader: "1000",
	}
	initialData, _ := json.Marshal(map[string]string{"goal": "test"})

	plan := models.WorkflowPlan{
		StartStep: "step1",
		Steps: map[string]models.Step{
			"step1":  {Action: "do_something", Topic: "topic.do_something", NextStep: "finish"},
			"finish": {Action: "complete_workflow"},
		},
	}

	// First, expect check if state exists (it won't)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnError(sql.ErrNoRows)

	// Then expect creation of initial state
	mockDB.ExpectExec("INSERT INTO orchestrator_state").
		WithArgs(
			correlationID,    // correlation_id
			clientID,         // client_id
			StatusRunning,    // status
			"step1",          // current_step
			sqlmock.AnyArg(), // awaited_steps
			sqlmock.AnyArg(), // collected_data
			initialData,      // initial_request_data
			sqlmock.AnyArg(), // created_at
			sqlmock.AnyArg(), // updated_at
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Then expect fetch of the newly created state
	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, clientID, StatusRunning, "step1", "[]",
		"{}", initialData, nil, nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Expect Kafka message production - verify headers are properly set
	mockProducer.On("Produce", ctx, "topic.do_something", mock.MatchedBy(func(h map[string]string) bool {
		// Verify required headers are present and correct
		return h["correlation_id"] == correlationID &&
			h["causation_id"] == headers["request_id"] &&
			h["request_id"] != "" && h["request_id"] != headers["request_id"] &&
			h[governance.FuelHeader] == "999" // 1000 - 1 for default_step cost
	}), []byte(correlationID), mock.AnythingOfType("[]uint8")).Return(nil).Once()

	// Expect state update
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,           // WHERE correlation_id = $1
			StatusAwaitingResponses, // status = $2
			"finish",                // current_step = $3
			sqlmock.AnyArg(),        // awaited_steps = $4
			sqlmock.AnyArg(),        // collected_data = $5
			sqlmock.AnyArg(),        // final_result = $6
			"",                      // error = $7
			sqlmock.AnyArg(),        // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	err := coordinator.ExecuteWorkflow(ctx, plan, headers, initialData)
	require.NoError(t, err)

	mockProducer.AssertExpectations(t)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestExecuteWorkflow_AlreadyCompleted verifies handling of already completed workflows.
func TestExecuteWorkflow_AlreadyCompleted(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id": correlationID,
		"client_id":      clientID,
	}

	plan := models.WorkflowPlan{
		StartStep: "step1",
		Steps: map[string]models.Step{
			"step1": {Action: "do_something"},
		},
	}

	// State already exists and is completed
	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, clientID, StatusCompleted, "step1", "[]",
		"{}", nil, []byte(`{"result": "done"}`), nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Should not do anything else
	err := coordinator.ExecuteWorkflow(ctx, plan, headers, nil)
	require.NoError(t, err)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestExecuteWorkflow_MissingClientID verifies error when client_id is missing.
func TestExecuteWorkflow_MissingClientID(t *testing.T) {
	coordinator, _, db, _ := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	headers := map[string]string{
		"correlation_id": uuid.NewString(),
		// client_id is missing
	}

	plan := models.WorkflowPlan{
		StartStep: "step1",
		Steps:     map[string]models.Step{},
	}

	err := coordinator.ExecuteWorkflow(ctx, plan, headers, nil)
	require.Error(t, err)
	assert.Contains(t, err.Error(), "client_id header is required")
}

// TestExecuteWorkflow_DependenciesNotMet verifies the workflow waits correctly.
func TestExecuteWorkflow_DependenciesNotMet(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id":      correlationID,
		"client_id":           clientID,
		governance.FuelHeader: "1000",
	}

	plan := models.WorkflowPlan{
		StartStep: "step2",
		Steps: map[string]models.Step{
			"step1": {Action: "do_something"},
			"step2": {Action: "do_something_else", Dependencies: []string{"step1"}},
		},
	}

	// First check - state doesn't exist
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnError(sql.ErrNoRows)

	// Create initial state
	mockDB.ExpectExec("INSERT INTO orchestrator_state").
		WithArgs(
			correlationID,    // correlation_id
			clientID,         // client_id
			StatusRunning,    // status
			"step2",          // current_step
			sqlmock.AnyArg(), // awaited_steps
			sqlmock.AnyArg(), // collected_data
			sqlmock.AnyArg(), // initial_request_data (nil)
			sqlmock.AnyArg(), // created_at
			sqlmock.AnyArg(), // updated_at
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Fetch state - missing step1 dependency
	stateJSON := `{}` // No step1 data
	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, clientID, StatusRunning, "step2", "[]",
		stateJSON, nil, nil, nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Should not produce any messages or update state
	err := coordinator.ExecuteWorkflow(ctx, plan, headers, nil)
	require.NoError(t, err, "Waiting for dependencies should not be an error")

	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestExecuteWorkflow_FuelCheckFail verifies that a workflow stops if out of fuel.
func TestExecuteWorkflow_FuelCheckFail(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id":      correlationID,
		"client_id":           clientID,
		governance.FuelHeader: "5", // Low fuel (need 50 for claude opus)
	}

	plan := models.WorkflowPlan{
		StartStep: "step1",
		Steps: map[string]models.Step{
			"step1": {Action: "ai_text_generate_claude_opus", Topic: "topic.expensive"},
		},
	}

	// First check - state doesn't exist
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnError(sql.ErrNoRows)

	// Create initial state
	mockDB.ExpectExec("INSERT INTO orchestrator_state").
		WithArgs(
			correlationID,    // correlation_id
			clientID,         // client_id
			StatusRunning,    // status
			"step1",          // current_step
			sqlmock.AnyArg(), // awaited_steps
			sqlmock.AnyArg(), // collected_data
			sqlmock.AnyArg(), // initial_request_data
			sqlmock.AnyArg(), // created_at
			sqlmock.AnyArg(), // updated_at
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Fetch state
	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, clientID, StatusRunning, "step1", "[]",
		"{}", nil, nil, nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Expect update to FAILED status
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,    // WHERE correlation_id = $1
			StatusFailed,     // status = $2
			"step1",          // current_step = $3
			sqlmock.AnyArg(), // awaited_steps = $4
			sqlmock.AnyArg(), // collected_data = $5
			sqlmock.AnyArg(), // final_result = $6
			sqlmock.AnyArg(), // error = $7 (will contain "insufficient fuel")
			sqlmock.AnyArg(), // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Execute the workflow - it should fail with insufficient fuel error
	err := coordinator.ExecuteWorkflow(ctx, plan, headers, nil)

	// Assert that we got an error
	require.Error(t, err, "Expected an error for insufficient fuel")
	assert.Contains(t, err.Error(), "insufficient fuel", "Error should mention insufficient fuel")

	// Verify all expectations were met
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestHandleFanOut verifies that multiple messages are sent in parallel.
func TestHandleFanOut(t *testing.T) {
	coordinator, mockProducer, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	clientID := "test_client_123"
	headers := map[string]string{
		"correlation_id":      correlationID,
		"client_id":           clientID,
		"request_id":          "parent_req_1",
		governance.FuelHeader: "995", // Already deducted by ExecuteWorkflow (1000 - 5)
	}

	step := models.Step{
		Action:   "fan_out",
		NextStep: "aggregate_results",
		SubTasks: []models.SubTask{
			{StepName: "get_research", Topic: "topic.research"},
			{StepName: "get_style", Topic: "topic.style"},
		},
	}
	state := &OrchestrationState{
		CorrelationID: correlationID,
		ClientID:      clientID,
		CollectedData: make(map[string]interface{}),
	}

	// Expect messages to be produced with validation
	var capturedRequestIDs []string
	mockProducer.On("Produce", ctx, "topic.research", mock.MatchedBy(func(h map[string]string) bool {
		if h["causation_id"] == "parent_req_1" && h["request_id"] != "parent_req_1" &&
			h[governance.FuelHeader] == "995" { // Fuel already deducted
			capturedRequestIDs = append(capturedRequestIDs, h["request_id"])
			return true
		}
		return false
	}), []byte(correlationID), mock.AnythingOfType("[]uint8")).Return(nil).Once()

	mockProducer.On("Produce", ctx, "topic.style", mock.MatchedBy(func(h map[string]string) bool {
		if h["causation_id"] == "parent_req_1" && h["request_id"] != "parent_req_1" &&
			h[governance.FuelHeader] == "995" { // Fuel already deducted
			capturedRequestIDs = append(capturedRequestIDs, h["request_id"])
			return true
		}
		return false
	}), []byte(correlationID), mock.AnythingOfType("[]uint8")).Return(nil).Once()

	// Expect state update
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,           // WHERE correlation_id = $1
			StatusAwaitingResponses, // status = $2
			"aggregate_results",     // current_step = $3
			sqlmock.AnyArg(),        // awaited_steps = $4
			sqlmock.AnyArg(),        // collected_data = $5
			sqlmock.AnyArg(),        // final_result = $6
			"",                      // error = $7
			sqlmock.AnyArg(),        // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	err := coordinator.handleFanOut(ctx, headers, step, state)
	require.NoError(t, err)

	// Verify state was updated correctly
	assert.Equal(t, StatusAwaitingResponses, state.Status)
	assert.Equal(t, "aggregate_results", state.CurrentStep)
	assert.Len(t, state.AwaitedSteps, 2)

	mockProducer.AssertExpectations(t)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestHandlePauseForHumanInput verifies human approval pause functionality.
func TestHandlePauseForHumanInput(t *testing.T) {
	coordinator, mockProducer, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	projectID := "project_123"
	clientID := "client_123"
	headers := map[string]string{
		"correlation_id": correlationID,
		"project_id":     projectID,
		"client_id":      clientID,
	}

	step := models.Step{
		Action:      "pause_for_human_input",
		NextStep:    "after_approval",
		Description: "Review generated content",
	}
	state := &OrchestrationState{
		CorrelationID: correlationID,
		ClientID:      clientID,
		CollectedData: map[string]interface{}{
			"generated_content": "Some content to review",
		},
	}

	// Expect state update
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,        // WHERE correlation_id = $1
			StatusPausedForHuman, // status = $2
			"after_approval",     // current_step = $3
			sqlmock.AnyArg(),     // awaited_steps = $4
			sqlmock.AnyArg(),     // collected_data = $5
			sqlmock.AnyArg(),     // final_result = $6
			"",                   // error = $7
			sqlmock.AnyArg(),     // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	// Expect notification to be sent
	mockProducer.On("Produce", ctx, NotificationTopic, headers, []byte(correlationID), mock.MatchedBy(func(payload []byte) bool {
		var notification map[string]interface{}
		json.Unmarshal(payload, &notification)
		return notification["event_type"] == "WORKFLOW_PAUSED_FOR_APPROVAL" &&
			notification["correlation_id"] == correlationID &&
			notification["project_id"] == projectID &&
			notification["client_id"] == clientID
	})).Return(nil).Once()

	err := coordinator.handlePauseForHumanInput(ctx, headers, step, state)
	require.NoError(t, err)

	assert.Equal(t, StatusPausedForHuman, state.Status)
	assert.Equal(t, "after_approval", state.CurrentStep)

	mockProducer.AssertExpectations(t)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestHandleResponse verifies processing of sub-task responses.
func TestHandleResponse(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	causationID := "request_123"
	headers := map[string]string{
		"correlation_id": correlationID,
		"causation_id":   causationID,
	}

	taskResponse := models.TaskResponse{
		Success: true,
		Data: map[string]interface{}{
			"result": "task completed",
		},
	}
	responseBytes, _ := json.Marshal(taskResponse)

	// Existing state with awaited steps
	existingData := map[string]interface{}{
		"existing": "data",
	}
	existingDataJSON, _ := json.Marshal(existingData)
	awaitedSteps := []string{causationID, "another_request"}
	awaitedStepsJSON, _ := json.Marshal(awaitedSteps)

	rows := sqlmock.NewRows([]string{
		"correlation_id", "client_id", "status", "current_step", "awaited_steps",
		"collected_data", "initial_request_data", "final_result", "error",
		"created_at", "updated_at",
	}).AddRow(
		correlationID, "client_123", StatusAwaitingResponses, "aggregate", string(awaitedStepsJSON),
		string(existingDataJSON), nil, nil, nil,
		time.Now(), time.Now(),
	)
	mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
		WithArgs(correlationID).
		WillReturnRows(rows)

	// Expect state update with response data added
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,           // WHERE correlation_id = $1
			StatusAwaitingResponses, // status = $2 (still awaiting one more)
			"aggregate",             // current_step = $3
			sqlmock.AnyArg(),        // awaited_steps = $4 (should have one less)
			sqlmock.AnyArg(),        // collected_data = $5 (should include new data)
			sqlmock.AnyArg(),        // final_result = $6
			"",                      // error = $7
			sqlmock.AnyArg(),        // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	err := coordinator.HandleResponse(ctx, headers, responseBytes)
	require.NoError(t, err)
	require.NoError(t, mockDB.ExpectationsWereMet())
}

// TestResumeWorkflow verifies resuming after human approval.
func TestResumeWorkflow(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()
	headers := map[string]string{
		"correlation_id": correlationID,
	}

	t.Run("approved", func(t *testing.T) {
		resumePayload := struct {
			Approved bool                   `json:"approved"`
			Feedback map[string]interface{} `json:"feedback,omitempty"`
		}{
			Approved: true,
			Feedback: map[string]interface{}{
				"comment": "Looks good!",
			},
		}
		resumeData, _ := json.Marshal(resumePayload)

		// Existing paused state
		rows := sqlmock.NewRows([]string{
			"correlation_id", "client_id", "status", "current_step", "awaited_steps",
			"collected_data", "initial_request_data", "final_result", "error",
			"created_at", "updated_at",
		}).AddRow(
			correlationID, "client_123", StatusPausedForHuman, "after_approval", "[]",
			"{}", nil, nil, nil,
			time.Now(), time.Now(),
		)
		mockDB.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
			WithArgs(correlationID).
			WillReturnRows(rows)

		// Expect state update to running
		mockDB.ExpectExec("UPDATE orchestrator_state SET").
			WithArgs(
				correlationID,    // WHERE correlation_id = $1
				StatusRunning,    // status = $2
				"after_approval", // current_step = $3
				sqlmock.AnyArg(), // awaited_steps = $4
				sqlmock.AnyArg(), // collected_data = $5 (should include feedback)
				sqlmock.AnyArg(), // final_result = $6
				"",               // error = $7
				sqlmock.AnyArg(), // updated_at = $8
			).WillReturnResult(sqlmock.NewResult(1, 1))

		err := coordinator.ResumeWorkflow(ctx, headers, resumeData)
		require.NoError(t, err)
		require.NoError(t, mockDB.ExpectationsWereMet())
	})

	t.Run("rejected", func(t *testing.T) {
		// Create fresh mocks for this subtest
		db2, mockDB2, err := sqlmock.New()
		require.NoError(t, err)
		defer db2.Close()

		// Create a new coordinator with the fresh DB
		coordinator2 := NewSagaCoordinator(db2, coordinator.producer, coordinator.logger)

		resumePayload := struct {
			Approved bool `json:"approved"`
		}{
			Approved: false,
		}
		resumeData, _ := json.Marshal(resumePayload)

		// Existing paused state
		rows := sqlmock.NewRows([]string{
			"correlation_id", "client_id", "status", "current_step", "awaited_steps",
			"collected_data", "initial_request_data", "final_result", "error",
			"created_at", "updated_at",
		}).AddRow(
			correlationID, "client_123", StatusPausedForHuman, "after_approval", "[]",
			"{}", nil, nil, nil,
			time.Now(), time.Now(),
		)
		mockDB2.ExpectQuery("SELECT .* FROM orchestrator_state WHERE correlation_id = \\$1").
			WithArgs(correlationID).
			WillReturnRows(rows)

		// Expect state update to failed
		mockDB2.ExpectExec("UPDATE orchestrator_state SET").
			WithArgs(
				correlationID,               // WHERE correlation_id = $1
				StatusFailed,                // status = $2
				"after_approval",            // current_step = $3
				sqlmock.AnyArg(),            // awaited_steps = $4
				sqlmock.AnyArg(),            // collected_data = $5
				sqlmock.AnyArg(),            // final_result = $6
				"Workflow rejected by user", // error = $7
				sqlmock.AnyArg(),            // updated_at = $8
			).WillReturnResult(sqlmock.NewResult(1, 1))

		err = coordinator2.ResumeWorkflow(ctx, headers, resumeData)
		require.NoError(t, err)
		require.NoError(t, mockDB2.ExpectationsWereMet())
	})
}

// TestCompleteWorkflow verifies workflow completion.
func TestCompleteWorkflow(t *testing.T) {
	coordinator, _, db, mockDB := setupTest(t)
	defer db.Close()

	ctx := context.Background()
	correlationID := uuid.NewString()

	state := &OrchestrationState{
		CorrelationID: correlationID,
		ClientID:      "client_123",
		Status:        StatusRunning,
		CurrentStep:   "final",
		CollectedData: map[string]interface{}{
			"step1_result": "data1",
			"step2_result": "data2",
		},
	}

	// Expect state update to completed
	mockDB.ExpectExec("UPDATE orchestrator_state SET").
		WithArgs(
			correlationID,    // WHERE correlation_id = $1
			StatusCompleted,  // status = $2
			"final",          // current_step = $3
			sqlmock.AnyArg(), // awaited_steps = $4
			sqlmock.AnyArg(), // collected_data = $5
			sqlmock.AnyArg(), // final_result = $6 (should be marshaled collected_data)
			"",               // error = $7
			sqlmock.AnyArg(), // updated_at = $8
		).WillReturnResult(sqlmock.NewResult(1, 1))

	err := coordinator.completeWorkflow(ctx, state)
	require.NoError(t, err)

	assert.Equal(t, StatusCompleted, state.Status)
	assert.NotNil(t, state.FinalResult)

	require.NoError(t, mockDB.ExpectationsWereMet())
}
-------------------------------------------------
filepath = ./platform/orchestration/coordinator.go
// FILE: platform/orchestration/coordinator.go
package orchestration

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"github.com/google/uuid"
	"github.com/gqls/agentchassis/pkg/models"
	"github.com/gqls/agentchassis/platform/governance"
	"github.com/gqls/agentchassis/platform/kafka"
	"go.uber.org/zap"
)

const (
	// Topic for notifications to the UI
	NotificationTopic = "system.notifications.ui"
	// Topic for receiving resume commands
	ResumeWorkflowTopic = "system.commands.workflow.resume"
)

// SagaCoordinator manages the execution of complex workflows
type SagaCoordinator struct {
	db          *sql.DB
	producer    kafka.Producer
	logger      *zap.Logger
	fuelManager *governance.FuelManager
}

// NewSagaCoordinator creates a new coordinator instance
func NewSagaCoordinator(db *sql.DB, producer kafka.Producer, logger *zap.Logger) *SagaCoordinator {
	return &SagaCoordinator{
		db:          db,
		producer:    producer,
		logger:      logger,
		fuelManager: governance.NewFuelManager(),
	}
}

// ExecuteWorkflow manages the execution of a workflow plan
func (s *SagaCoordinator) ExecuteWorkflow(ctx context.Context, plan models.WorkflowPlan, headers map[string]string, initialData []byte) error {
	correlationID := headers["correlation_id"]
	l := s.logger.With(zap.String("correlation_id", correlationID))

	// Get clientID from headers to pass to state creation
	clientID := headers["client_id"]
	if clientID == "" {
		return fmt.Errorf("client_id header is required to execute a workflow")
	}

	// Get or create state
	state, err := s.getOrCreateState(ctx, correlationID, clientID, plan, initialData)
	if err != nil {
		return err
	}

	// Check if workflow is already complete
	if state.Status == StatusCompleted || state.Status == StatusFailed {
		l.Info("Workflow already finished", zap.String("status", string(state.Status)))
		return nil
	}

	// Get current step configuration
	currentStepConfig, ok := plan.Steps[state.CurrentStep]
	if !ok {
		return s.failWorkflow(ctx, state, fmt.Sprintf("step '%s' not found in plan", state.CurrentStep))
	}

	// Check dependencies
	if !s.dependenciesMet(currentStepConfig.Dependencies, state) {
		l.Info("Dependencies not met, waiting", zap.Strings("dependencies", currentStepConfig.Dependencies))
		return nil
	}

	// Check fuel budget
	fuel, err := governance.GetFuelFromHeader(headers)
	if err != nil {
		return s.failWorkflow(ctx, state, fmt.Sprintf("failed to get fuel from headers: %v", err))
	}

	if !s.fuelManager.HasEnoughFuel(fuel, currentStepConfig.Action) {
		return s.failWorkflow(ctx, state, fmt.Sprintf("insufficient fuel for action '%s': have %d, need %d",
			currentStepConfig.Action, fuel, s.fuelManager.GetCost(currentStepConfig.Action)))
	}

	// Deduct fuel and update headers
	remainingFuel := s.fuelManager.DeductFuel(fuel, currentStepConfig.Action)
	governance.SetFuelHeader(headers, remainingFuel)

	// Execute the action
	switch currentStepConfig.Action {
	case "fan_out":
		return s.handleFanOut(ctx, headers, currentStepConfig, state)
	case "pause_for_human_input":
		return s.handlePauseForHumanInput(ctx, headers, currentStepConfig, state)
	case "complete_workflow":
		return s.completeWorkflow(ctx, state)
	default:
		return s.handleStandardAction(ctx, headers, currentStepConfig, state)
	}
}

// getOrCreateState retrieves existing state or creates new one
func (s *SagaCoordinator) getOrCreateState(ctx context.Context, correlationID string, clientID string, plan models.WorkflowPlan, initialData []byte) (*OrchestrationState, error) {
	repo := NewStateRepository(s.db, s.logger)

	state, err := repo.GetState(ctx, correlationID)
	if err != nil {
		// State doesn't exist, create it
		if err := repo.CreateInitialState(ctx, correlationID, clientID, plan.StartStep, initialData); err != nil {
			return nil, fmt.Errorf("failed to create initial state: %w", err)
		}
		return repo.GetState(ctx, correlationID)
	}

	return state, nil
}

// dependenciesMet checks if all required dependencies have been completed
func (s *SagaCoordinator) dependenciesMet(dependencies []string, state *OrchestrationState) bool {
	for _, dep := range dependencies {
		if _, ok := state.CollectedData[dep]; !ok {
			return false
		}
	}
	return true
}

// handleStandardAction sends a message to the specified topic
func (s *SagaCoordinator) handleStandardAction(ctx context.Context, headers map[string]string, step models.Step, state *OrchestrationState) error {
	l := s.logger.With(zap.String("correlation_id", state.CorrelationID))

	// Prepare the message payload
	payload := models.TaskRequest{
		Action: step.Action,
		Data:   state.CollectedData,
	}
	payloadBytes, _ := json.Marshal(payload)

	// Create new request ID for this sub-task
	newRequestID := uuid.NewString()
	outHeaders := make(map[string]string)
	for k, v := range headers {
		outHeaders[k] = v
	}
	outHeaders["causation_id"] = headers["request_id"]
	outHeaders["request_id"] = newRequestID

	// Send the message
	if err := s.producer.Produce(ctx, step.Topic, outHeaders, []byte(state.CorrelationID), payloadBytes); err != nil {
		return fmt.Errorf("failed to produce message: %w", err)
	}

	// Update state to await response
	state.Status = StatusAwaitingResponses
	state.CurrentStep = step.NextStep
	state.AwaitedSteps = []string{newRequestID}

	repo := NewStateRepository(s.db, s.logger)
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	l.Info("Standard action executed", zap.String("action", step.Action), zap.String("topic", step.Topic))
	return nil
}

// handleFanOut sends multiple parallel requests
func (s *SagaCoordinator) handleFanOut(ctx context.Context, headers map[string]string, step models.Step, state *OrchestrationState) error {
	l := s.logger.With(zap.String("correlation_id", state.CorrelationID))

	awaitedSteps := make([]string, 0, len(step.SubTasks))

	for _, subTask := range step.SubTasks {
		payload := models.TaskRequest{
			Action: subTask.StepName,
			Data:   state.CollectedData,
		}
		payloadBytes, _ := json.Marshal(payload)

		newRequestID := uuid.NewString()
		outHeaders := make(map[string]string)
		for k, v := range headers {
			outHeaders[k] = v
		}
		outHeaders["causation_id"] = headers["request_id"]
		outHeaders["request_id"] = newRequestID

		if err := s.producer.Produce(ctx, subTask.Topic, outHeaders, []byte(state.CorrelationID), payloadBytes); err != nil {
			return fmt.Errorf("failed to produce fan-out message: %w", err)
		}

		awaitedSteps = append(awaitedSteps, newRequestID)
	}

	// Update state
	state.Status = StatusAwaitingResponses
	state.CurrentStep = step.NextStep
	state.AwaitedSteps = awaitedSteps

	repo := NewStateRepository(s.db, s.logger)
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	l.Info("Fan-out executed", zap.Int("subtasks", len(step.SubTasks)))
	return nil
}

// handlePauseForHumanInput pauses the workflow and notifies the UI
func (s *SagaCoordinator) handlePauseForHumanInput(ctx context.Context, headers map[string]string, step models.Step, state *OrchestrationState) error {
	l := s.logger.With(zap.String("correlation_id", state.CorrelationID))

	state.Status = StatusPausedForHuman
	state.CurrentStep = step.NextStep

	repo := NewStateRepository(s.db, s.logger)
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	// Send notification
	notification := map[string]interface{}{
		"event_type":      "WORKFLOW_PAUSED_FOR_APPROVAL",
		"correlation_id":  state.CorrelationID,
		"project_id":      headers["project_id"],
		"client_id":       headers["client_id"],
		"message":         fmt.Sprintf("Step '%s' requires your approval", step.Description),
		"data_for_review": state.CollectedData,
	}
	notificationBytes, _ := json.Marshal(notification)

	if err := s.producer.Produce(ctx, NotificationTopic, headers, []byte(state.CorrelationID), notificationBytes); err != nil {
		return fmt.Errorf("failed to send notification: %w", err)
	}

	l.Info("Workflow paused for human input")
	return nil
}

// HandleResponse processes a response from a sub-task
func (s *SagaCoordinator) HandleResponse(ctx context.Context, headers map[string]string, response []byte) error {
	correlationID := headers["correlation_id"]
	causationID := headers["causation_id"]

	l := s.logger.With(
		zap.String("correlation_id", correlationID),
		zap.String("causation_id", causationID),
	)

	repo := NewStateRepository(s.db, s.logger)
	state, err := repo.GetState(ctx, correlationID)
	if err != nil {
		return fmt.Errorf("failed to get state: %w", err)
	}

	// Parse response
	var taskResponse models.TaskResponse
	if err := json.Unmarshal(response, &taskResponse); err != nil {
		return fmt.Errorf("failed to unmarshal response: %w", err)
	}

	// Store response data
	state.CollectedData[causationID] = taskResponse.Data

	// Remove from awaited steps
	newAwaitedSteps := make([]string, 0)
	for _, step := range state.AwaitedSteps {
		if step != causationID {
			newAwaitedSteps = append(newAwaitedSteps, step)
		}
	}
	state.AwaitedSteps = newAwaitedSteps

	// If all responses received, set status back to running
	if len(state.AwaitedSteps) == 0 {
		state.Status = StatusRunning
	}

	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	l.Info("Response processed", zap.Int("remaining_awaited", len(state.AwaitedSteps)))

	// If all responses received, continue workflow
	if len(state.AwaitedSteps) == 0 {
		// Need to reload the workflow plan - this would come from the agent config
		// For now, we'll need to pass it through somehow
		// This is a limitation we'll address in the actual implementation
	}

	return nil
}

// ResumeWorkflow resumes a paused workflow after human input
func (s *SagaCoordinator) ResumeWorkflow(ctx context.Context, headers map[string]string, resumeData []byte) error {
	correlationID := headers["correlation_id"]
	l := s.logger.With(zap.String("correlation_id", correlationID))

	var resumePayload struct {
		Approved bool                   `json:"approved"`
		Feedback map[string]interface{} `json:"feedback,omitempty"`
	}
	if err := json.Unmarshal(resumeData, &resumePayload); err != nil {
		return fmt.Errorf("failed to unmarshal resume payload: %w", err)
	}

	repo := NewStateRepository(s.db, s.logger)
	state, err := repo.GetState(ctx, correlationID)
	if err != nil {
		return fmt.Errorf("failed to get state: %w", err)
	}

	if state.Status != StatusPausedForHuman {
		return fmt.Errorf("workflow not in paused state: %s", state.Status)
	}

	if !resumePayload.Approved {
		state.Status = StatusFailed
		state.Error = "Workflow rejected by user"
		return repo.UpdateState(ctx, state)
	}

	// Add feedback to collected data
	if resumePayload.Feedback != nil {
		state.CollectedData["human_feedback"] = resumePayload.Feedback
	}

	state.Status = StatusRunning
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state: %w", err)
	}

	l.Info("Workflow resumed after human approval")

	// Continue workflow execution
	// This would trigger re-execution with the current state

	return nil
}

// completeWorkflow marks the workflow as completed
func (s *SagaCoordinator) completeWorkflow(ctx context.Context, state *OrchestrationState) error {
	state.Status = StatusCompleted
	finalResult, _ := json.Marshal(state.CollectedData)
	state.FinalResult = finalResult

	repo := NewStateRepository(s.db, s.logger)
	return repo.UpdateState(ctx, state)
}

// failWorkflow marks the workflow as failed
func (s *SagaCoordinator) failWorkflow(ctx context.Context, state *OrchestrationState, errorMsg string) error {
	state.Status = StatusFailed
	state.Error = errorMsg

	repo := NewStateRepository(s.db, s.logger)
	if err := repo.UpdateState(ctx, state); err != nil {
		return fmt.Errorf("failed to update state to failed: %w", err)
	}

	// IMPORTANT: Return the error message as an error
	return fmt.Errorf(errorMsg)
}
-------------------------------------------------
filepath = ./platform/orchestration/state.go
// FILE: platform/orchestration/state.go
package orchestration

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"time"

	"go.uber.org/zap"
)

// OrchestrationStatus represents the current state of a workflow
type OrchestrationStatus string

const (
	StatusRunning           OrchestrationStatus = "RUNNING"
	StatusAwaitingResponses OrchestrationStatus = "AWAITING_RESPONSES"
	StatusPausedForHuman    OrchestrationStatus = "PAUSED_FOR_HUMAN_INPUT"
	StatusCompleted         OrchestrationStatus = "COMPLETED"
	StatusFailed            OrchestrationStatus = "FAILED"
)

// OrchestrationState is the database model for a Saga instance
type OrchestrationState struct {
	CorrelationID      string                 `db:"correlation_id"`
	ClientID           string                 `db:"client_id"`
	Status             OrchestrationStatus    `db:"status"`
	CurrentStep        string                 `db:"current_step"`
	AwaitedSteps       []string               `db:"awaited_steps"`
	CollectedData      map[string]interface{} `db:"collected_data"`
	InitialRequestData json.RawMessage        `db:"initial_request_data"`
	FinalResult        json.RawMessage        `db:"final_result"`
	Error              string                 `db:"error"`
	CreatedAt          time.Time              `db:"created_at"`
	UpdatedAt          time.Time              `db:"updated_at"`
}

// StateRepository provides an interface for persisting and retrieving workflow state
type StateRepository struct {
	db     *sql.DB
	logger *zap.Logger
}

// NewStateRepository creates a new state repository
func NewStateRepository(db *sql.DB, logger *zap.Logger) *StateRepository {
	return &StateRepository{db: db, logger: logger}
}

// CreateInitialState creates a new record for a workflow
func (r *StateRepository) CreateInitialState(ctx context.Context, correlationID, clientID, startStep string, initialData []byte) error {
	awaitedStepsJSON, _ := json.Marshal([]string{})
	collectedDataJSON, _ := json.Marshal(map[string]interface{}{})

	query := `
        INSERT INTO orchestrator_state 
        (correlation_id, client_id, status, current_step, awaited_steps, collected_data, initial_request_data, created_at, updated_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
    `

	now := time.Now().UTC()
	_, err := r.db.ExecContext(ctx, query,
		correlationID, clientID, StatusRunning, startStep, awaitedStepsJSON, collectedDataJSON, initialData, now, now)

	if err != nil {
		r.logger.Error("Failed to create initial orchestration state", zap.Error(err))
		return fmt.Errorf("failed to create initial state: %w", err)
	}

	r.logger.Info("Initial orchestration state created", zap.String("correlation_id", correlationID))
	return nil
}

// GetState retrieves the current state of a workflow
func (r *StateRepository) GetState(ctx context.Context, correlationID string) (*OrchestrationState, error) {
	query := `
        SELECT correlation_id, client_id, status, current_step, awaited_steps, collected_data, 
               initial_request_data, final_result, error, created_at, updated_at
        FROM orchestrator_state
        WHERE correlation_id = $1
    `

	var state OrchestrationState
	var awaitedStepsJSON, collectedDataJSON []byte
	var initialRequestDataNull sql.NullString // Handle NULL for initial_request_data
	var finalResultNull sql.NullString
	var errorNull sql.NullString

	err := r.db.QueryRowContext(ctx, query, correlationID).Scan(
		&state.CorrelationID,
		&state.ClientID,
		&state.Status,
		&state.CurrentStep,
		&awaitedStepsJSON,
		&collectedDataJSON,
		&initialRequestDataNull, // Scan into NullString
		&finalResultNull,
		&errorNull,
		&state.CreatedAt,
		&state.UpdatedAt,
	)

	if err != nil {
		if err == sql.ErrNoRows {
			return nil, fmt.Errorf("state not found for correlation_id: %s", correlationID)
		}
		return nil, fmt.Errorf("failed to get state: %w", err)
	}

	// Handle nullable fields
	if initialRequestDataNull.Valid {
		state.InitialRequestData = json.RawMessage(initialRequestDataNull.String)
	} else {
		state.InitialRequestData = json.RawMessage("{}") // Default to empty JSON
	}

	if finalResultNull.Valid {
		state.FinalResult = json.RawMessage(finalResultNull.String)
	} else {
		state.FinalResult = json.RawMessage("{}") // Default to empty JSON
	}

	if errorNull.Valid {
		state.Error = errorNull.String
	}

	// Unmarshal JSON fields
	if err := json.Unmarshal(awaitedStepsJSON, &state.AwaitedSteps); err != nil {
		return nil, fmt.Errorf("failed to unmarshal awaited_steps: %w", err)
	}
	if err := json.Unmarshal(collectedDataJSON, &state.CollectedData); err != nil {
		return nil, fmt.Errorf("failed to unmarshal collected_data: %w", err)
	}

	return &state, nil
}

// UpdateState persists changes to a workflow's state
func (r *StateRepository) UpdateState(ctx context.Context, state *OrchestrationState) error {
	awaitedStepsJSON, _ := json.Marshal(state.AwaitedSteps)
	collectedDataJSON, _ := json.Marshal(state.CollectedData)

	query := `
        UPDATE orchestrator_state 
        SET status = $2, current_step = $3, awaited_steps = $4, collected_data = $5, 
            final_result = $6, error = $7, updated_at = $8
        WHERE correlation_id = $1
    `

	_, err := r.db.ExecContext(ctx, query,
		state.CorrelationID,
		state.Status,
		state.CurrentStep,
		awaitedStepsJSON,
		collectedDataJSON,
		state.FinalResult,
		state.Error,
		time.Now().UTC(),
	)

	if err != nil {
		r.logger.Error("Failed to update orchestration state", zap.Error(err))
		return fmt.Errorf("failed to update state: %w", err)
	}

	r.logger.Debug("Orchestration state updated",
		zap.String("correlation_id", state.CorrelationID),
		zap.String("status", string(state.Status)))
	return nil
}

// GetOrchestratorStateTableSchema returns the SQL for creating the state table
func GetOrchestratorStateTableSchema() string {
	return `
CREATE TABLE IF NOT EXISTS orchestrator_state (
    correlation_id UUID PRIMARY KEY,
    status VARCHAR(50) NOT NULL,
    current_step VARCHAR(255) NOT NULL,
    awaited_steps JSONB DEFAULT '[]',
    collected_data JSONB DEFAULT '{}',
    initial_request_data JSONB,
    final_result JSONB,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_orchestrator_state_status ON orchestrator_state(status);
CREATE INDEX idx_orchestrator_state_updated_at ON orchestrator_state(updated_at);
`
}
-------------------------------------------------
filepath = ./platform/governance/fuel.go
// FILE: platform/governance/fuel.go
package governance

import (
	"fmt"
	"strconv"
)

const (
	// FuelHeader is the standard Kafka message header key for the fuel budget
	FuelHeader = "fuel_budget"
)

// CostTable defines the "price" in fuel units for various agent actions
var CostTable = map[string]int{
	"default_step":                   1,
	"fan_out":                        5,
	"ai_text_generate_claude_haiku":  10,
	"ai_text_generate_claude_sonnet": 25,
	"ai_text_generate_claude_opus":   50,
	"ai_image_generate_sdxl":         40,
	"web_search":                     5,
	"database_query":                 1,
	"memory_store":                   2,
	"memory_search":                  2,
	"pause_for_human_input":          0, // No cost for waiting
}

// FuelManager provides methods for checking and managing task fuel
type FuelManager struct{}

// NewFuelManager creates a new fuel manager
func NewFuelManager() *FuelManager {
	return &FuelManager{}
}

// GetCost returns the fuel cost for a given action
func (fm *FuelManager) GetCost(action string) int {
	if cost, ok := CostTable[action]; ok {
		return cost
	}
	// Return a default cost if the specific action isn't priced
	return CostTable["default_step"]
}

// HasEnoughFuel checks if the current budget is sufficient for an action
func (fm *FuelManager) HasEnoughFuel(currentFuel int, action string) bool {
	cost := fm.GetCost(action)
	return currentFuel >= cost
}

// DeductFuel subtracts the cost of an action from the current budget
func (fm *FuelManager) DeductFuel(currentFuel int, action string) int {
	cost := fm.GetCost(action)
	return currentFuel - cost
}

// GetFuelFromHeader safely parses the fuel value from Kafka message headers
func GetFuelFromHeader(headers map[string]string) (int, error) {
	fuelStr, ok := headers[FuelHeader]
	if !ok {
		return 0, fmt.Errorf("'%s' header not found", FuelHeader)
	}
	fuel, err := strconv.Atoi(fuelStr)
	if err != nil {
		return 0, fmt.Errorf("invalid fuel value in header: %w", err)
	}
	return fuel, nil
}

// SetFuelHeader sets the fuel budget in the headers map
func SetFuelHeader(headers map[string]string, fuel int) {
	headers[FuelHeader] = strconv.Itoa(fuel)
}
-------------------------------------------------
filepath = ./platform/database/migrations/001_enable_pgvector.sql
-- FILE: platform/database/migrations/001_enable_pgvector.sql
-- Run this on the clients database as superuser
CREATE EXTENSION IF NOT EXISTS vector;
-------------------------------------------------
filepath = ./platform/database/migrations/005_projects_schema.sql
// FILE: platform/database/migrations/005_projects_schema.sql
-- Projects table for auth database
CREATE TABLE IF NOT EXISTS projects (
                                        id VARCHAR(36) PRIMARY KEY,
    client_id VARCHAR(100) NOT NULL,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    owner_id VARCHAR(36) NOT NULL,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    FOREIGN KEY (owner_id) REFERENCES users(id),
    INDEX idx_projects_client (client_id),
    INDEX idx_projects_owner (owner_id)
    );

-- Subscriptions table
CREATE TABLE IF NOT EXISTS subscriptions (
                                             id VARCHAR(36) PRIMARY KEY,
    user_id VARCHAR(36) NOT NULL UNIQUE,
    tier VARCHAR(50) NOT NULL,
    status VARCHAR(50) NOT NULL,
    start_date TIMESTAMPTZ NOT NULL,
    end_date TIMESTAMPTZ,
    trial_ends_at TIMESTAMPTZ,
    cancelled_at TIMESTAMPTZ,
    payment_method VARCHAR(100),
    stripe_customer_id VARCHAR(255),
    stripe_subscription_id VARCHAR(255),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    FOREIGN KEY (user_id) REFERENCES users(id),
    INDEX idx_subscriptions_status (status)
    );

-- Subscription tiers table
CREATE TABLE IF NOT EXISTS subscription_tiers (
                                                  id VARCHAR(36) PRIMARY KEY,
    name VARCHAR(50) NOT NULL UNIQUE,
    display_name VARCHAR(100) NOT NULL,
    description TEXT,
    price_monthly DECIMAL(10,2) NOT NULL,
    price_yearly DECIMAL(10,2) NOT NULL,
    max_personas INT NOT NULL,
    max_projects INT NOT NULL,
    max_content_items INT NOT NULL,
    features JSON,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

-- Insert default tiers
INSERT INTO subscription_tiers (id, name, display_name, description, price_monthly, price_yearly, max_personas, max_projects, max_content_items, features) VALUES
                                                                                                                                                               ('00000000-0000-0000-0000-000000000001', 'free', 'Free', 'Basic features for getting started', 0.00, 0.00, 1, 3, 10, '["Basic personas", "Limited content generation"]'),
                                                                                                                                                               ('00000000-0000-0000-0000-000000000002', 'basic', 'Basic', 'For individual users', 9.99, 99.99, 5, 10, 100, '["All persona types", "Priority support", "Advanced templates"]'),
                                                                                                                                                               ('00000000-0000-0000-0000-000000000003', 'premium', 'Premium', 'For power users', 29.99, 299.99, 20, 50, 1000, '["All basic features", "Custom personas", "API access", "Analytics"]'),
                                                                                                                                                               ('00000000-0000-0000-0000-000000000004', 'enterprise', 'Enterprise', 'For organizations', 99.99, 999.99, -1, -1, -1, '["All premium features", "Unlimited usage", "Dedicated support", "Custom integrations"]');

-- User profiles table
CREATE TABLE IF NOT EXISTS user_profiles (
                                             user_id VARCHAR(36) PRIMARY KEY,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    company VARCHAR(255),
    phone VARCHAR(50),
    avatar_url VARCHAR(500),
    preferences JSON,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    FOREIGN KEY (user_id) REFERENCES users(id)
    );

-- Permissions table
CREATE TABLE IF NOT EXISTS permissions (
                                           id VARCHAR(36) PRIMARY KEY,
    name VARCHAR(100) NOT NULL UNIQUE,
    description TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

-- User permissions junction table
CREATE TABLE IF NOT EXISTS user_permissions (
                                                user_id VARCHAR(36) NOT NULL,
    permission_id VARCHAR(36) NOT NULL,
    granted_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    PRIMARY KEY (user_id, permission_id),
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (permission_id) REFERENCES permissions(id)
    );

-- Insert default permissions
INSERT INTO permissions (id, name, description) VALUES
                                                    ('00000000-0000-0000-0000-000000000001', 'personas.create', 'Create new personas'),
                                                    ('00000000-0000-0000-0000-000000000002', 'personas.delete', 'Delete personas'),
                                                    ('00000000-0000-0000-0000-000000000003', 'projects.manage', 'Manage all projects'),
                                                    ('00000000-0000-0000-0000-000000000004', 'admin.users', 'Manage users'),
                                                    ('00000000-0000-0000-0000-000000000005', 'admin.subscriptions', 'Manage subscriptions'),
                                                    ('00000000-0000-0000-0000-000000000006', '*', 'Super admin - all permissions');-------------------------------------------------
filepath = ./platform/database/migrations/003_create_client_schema.sql
-- FILE: platform/database/migrations/003_create_client_schema.sql


-- Enable required extensions

-- Global agent definitions table (shared across all clients)
CREATE TABLE IF NOT EXISTS agent_definitions (
                                                 id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    type VARCHAR(100) NOT NULL UNIQUE,
    display_name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(50) NOT NULL CHECK (category IN ('data-driven', 'code-driven', 'adapter')),
    default_config JSONB NOT NULL DEFAULT '{}',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    deleted_at TIMESTAMPTZ
    );

-- Index for active agent types
CREATE INDEX IF NOT EXISTS idx_agent_definitions_type_active
    ON agent_definitions(type, is_active) WHERE deleted_at IS NULL;

-- Global orchestrator state table (shared across all clients)
CREATE TABLE IF NOT EXISTS orchestrator_state (
                                                  correlation_id UUID PRIMARY KEY,
                                                  client_id VARCHAR(100) NOT NULL,
    status VARCHAR(50) NOT NULL,
    current_step VARCHAR(255) NOT NULL,
    awaited_steps JSONB DEFAULT '[]',
    collected_data JSONB DEFAULT '{}',
    initial_request_data JSONB,
    final_result JSONB,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

-- Indexes for orchestrator state
CREATE INDEX IF NOT EXISTS idx_orchestrator_state_status ON orchestrator_state(status);
CREATE INDEX IF NOT EXISTS idx_orchestrator_state_client ON orchestrator_state(client_id);
CREATE INDEX IF NOT EXISTS idx_orchestrator_state_updated_at ON orchestrator_state(updated_at);

-- Function to create client-specific schema
CREATE OR REPLACE FUNCTION create_client_schema(client_id TEXT)
RETURNS VOID AS $$
DECLARE
schema_name TEXT := 'client_' || client_id;
BEGIN
    -- Create schema
EXECUTE format('CREATE SCHEMA IF NOT EXISTS %I', schema_name);

-- Agent instances table for this client
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.agent_instances (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            template_id UUID NOT NULL,
            owner_user_id VARCHAR(255) NOT NULL,
            name VARCHAR(255) NOT NULL,
            config JSONB NOT NULL DEFAULT ''{}''::jsonb,
            is_active BOOLEAN DEFAULT true,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
        )', schema_name);

-- Indexes for agent instances
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_instances_owner
        ON %I.agent_instances(owner_user_id)', schema_name);

EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_instances_template
        ON %I.agent_instances(template_id)', schema_name);

-- Agent memory table with vector support
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.agent_memory (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            agent_instance_id UUID NOT NULL REFERENCES %I.agent_instances(id),
            content TEXT NOT NULL,
            embedding vector(1536) NOT NULL,
            metadata JSONB DEFAULT ''{}''::jsonb,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
        )', schema_name, schema_name);

-- Vector index for similarity search
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_memory_embedding
        ON %I.agent_memory USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100)', schema_name);

-- Index for agent memory queries
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_memory_agent_created
        ON %I.agent_memory(agent_instance_id, created_at DESC)', schema_name);

-- Projects table for this client
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.projects (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name VARCHAR(255) NOT NULL,
            description TEXT,
            owner_user_id VARCHAR(255) NOT NULL,
            settings JSONB DEFAULT ''{}''::jsonb,
            is_active BOOLEAN DEFAULT true,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
        )', schema_name);

-- Index for project queries
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_projects_owner
        ON %I.projects(owner_user_id)', schema_name);

-- Workflow executions table for this client
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.workflow_executions (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            correlation_id UUID NOT NULL,
            project_id UUID REFERENCES %I.projects(id),
            agent_instance_id UUID REFERENCES %I.agent_instances(id),
            status VARCHAR(50) NOT NULL,
            input_data JSONB,
            output_data JSONB,
            error_message TEXT,
            started_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            completed_at TIMESTAMPTZ,
            created_by VARCHAR(255) NOT NULL
        )', schema_name, schema_name, schema_name);

-- Indexes for workflow executions
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_workflow_executions_correlation
        ON %I.workflow_executions(correlation_id)', schema_name);

EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_workflow_executions_status
        ON %I.workflow_executions(status)', schema_name);

EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_workflow_executions_project
        ON %I.workflow_executions(project_id)', schema_name);

-- Usage analytics table for this client
EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I.usage_analytics (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id VARCHAR(255) NOT NULL,
            agent_type VARCHAR(100) NOT NULL,
            action VARCHAR(100) NOT NULL,
            fuel_consumed INTEGER NOT NULL DEFAULT 0,
            execution_time_ms INTEGER,
            success BOOLEAN NOT NULL,
            metadata JSONB DEFAULT ''{}''::jsonb,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
        )', schema_name);

-- Indexes for analytics
EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_usage_analytics_user_date
        ON %I.usage_analytics(user_id, created_at)', schema_name);

EXECUTE format('
        CREATE INDEX IF NOT EXISTS idx_usage_analytics_agent_type
        ON %I.usage_analytics(agent_type, created_at)', schema_name);

END;
$$ LANGUAGE plpgsql;

-- Insert default agent definitions
INSERT INTO agent_definitions (type, display_name, description, category, default_config) VALUES
                                                                                              ('copywriter', 'Copywriter', 'Creates compelling marketing and content copy', 'data-driven', '{"model": "claude-3-sonnet", "temperature": 0.7}'),
                                                                                              ('researcher', 'Research Assistant', 'Conducts thorough research and analysis', 'data-driven', '{"model": "claude-3-opus", "temperature": 0.3}'),
                                                                                              ('reasoning', 'Reasoning Agent', 'Performs logical analysis and decision making', 'code-driven', '{"model": "claude-3-opus", "temperature": 0.2}'),
                                                                                              ('image-generator', 'Image Generator', 'Creates images using AI generation', 'adapter', '{"provider": "stability_ai", "model": "sdxl"}'),
                                                                                              ('web-search', 'Web Search', 'Searches the internet for information', 'adapter', '{"provider": "serpapi", "max_results": 10}')
    ON CONFLICT (type) DO UPDATE SET
    display_name = EXCLUDED.display_name,
                              description = EXCLUDED.description,
                              category = EXCLUDED.category,
                              default_config = EXCLUDED.default_config,
                              updated_at = NOW();

-- Create a demo client schema for testing
SELECT create_client_schema('demo_client');


-- This should be run for each new client
-- Replace {client_id} with actual client ID

CREATE SCHEMA IF NOT EXISTS client_{client_id};

-- Agent instances table
CREATE TABLE IF NOT EXISTS client_{client_id}.agent_instances (
                                                                  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    template_id UUID NOT NULL,
    owner_user_id VARCHAR(255) NOT NULL,
    name VARCHAR(255) NOT NULL,
    config JSONB NOT NULL DEFAULT '{}',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_instances_owner ON client_{client_id}.agent_instances(owner_user_id);
CREATE INDEX idx_instances_template ON client_{client_id}.agent_instances(template_id);

-- Agent memory table with vector support
CREATE TABLE IF NOT EXISTS client_{client_id}.agent_memory (
                                                               id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    agent_instance_id UUID NOT NULL REFERENCES client_{client_id}.agent_instances(id),
    content TEXT NOT NULL,
    embedding vector(1536) NOT NULL,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

-- Create vector index for similarity search
CREATE INDEX ON client_{client_id}.agent_memory USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- Orchestrator state table
CREATE TABLE IF NOT EXISTS client_{client_id}.orchestrator_state (
                                                                     correlation_id UUID PRIMARY KEY,
                                                                     status VARCHAR(50) NOT NULL,
    current_step VARCHAR(255) NOT NULL,
    awaited_steps JSONB DEFAULT '[]',
    collected_data JSONB DEFAULT '{}',
    initial_request_data JSONB,
    final_result JSONB,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_orchestrator_status ON client_{client_id}.orchestrator_state(status);

CREATE INDEX idx_memory_agent_created ON client_{client_id}.agent_memory(agent_instance_id, created_at DESC);
-------------------------------------------------
filepath = ./platform/database/migrations/004_auth_schema.sql
-- FILE: platform/database/migrations/004_auth_schema.sql
-- Auth database schema
CREATE TABLE IF NOT EXISTS users (
                                     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    role VARCHAR(50) DEFAULT 'user',
    client_id VARCHAR(100) NOT NULL,
    subscription_tier VARCHAR(50) DEFAULT 'free',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_client ON users(client_id);

CREATE TABLE IF NOT EXISTS auth_tokens (
                                           id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id),
    token_hash VARCHAR(255) NOT NULL,
    expires_at TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_tokens_user ON auth_tokens(user_id);
CREATE INDEX idx_tokens_expires ON auth_tokens(expires_at);

CREATE INDEX idx_users_email_active ON users(email, is_active);-------------------------------------------------
filepath = ./platform/database/migrations/002_create_templates_schema.sql
-- FILE: platform/database/migrations/002_create_templates_schema.sql
-- Templates database schema
CREATE TABLE IF NOT EXISTS persona_templates (
                                                 id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100),
    config JSONB NOT NULL DEFAULT '{}',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

CREATE INDEX idx_templates_category ON persona_templates(category);
CREATE INDEX idx_templates_active ON persona_templates(is_active);
-------------------------------------------------
filepath = ./platform/database/postgres.go
// FILE: platform/database/postgres.go
package database

import (
	"context"
	"fmt"
	"os"
	"time"

	"github.com/gqls/agentchassis/platform/config"
	"github.com/jackc/pgx/v5/pgxpool"
	"go.uber.org/zap"
)

// NewPostgresConnection creates a new PostgreSQL connection pool with retry logic
func NewPostgresConnection(ctx context.Context, dbCfg config.DatabaseConfig, logger *zap.Logger) (*pgxpool.Pool, error) {
	password := os.Getenv(dbCfg.PasswordEnvVar)
	if password == "" {
		return nil, fmt.Errorf("database password environment variable %s is not set", dbCfg.PasswordEnvVar)
	}

	connStr := fmt.Sprintf("postgresql://%s:%s@%s:%d/%s?sslmode=%s",
		dbCfg.User, password, dbCfg.Host, dbCfg.Port, dbCfg.DBName, dbCfg.SSLMode)

	poolConfig, err := pgxpool.ParseConfig(connStr)
	if err != nil {
		return nil, fmt.Errorf("failed to parse postgres connection string: %w", err)
	}

	poolConfig.MaxConns = 10
	poolConfig.MinConns = 2
	poolConfig.MaxConnLifetime = time.Hour
	poolConfig.MaxConnIdleTime = 30 * time.Minute

	var pool *pgxpool.Pool
	for i := 0; i < 5; i++ {
		pool, err = pgxpool.NewWithConfig(ctx, poolConfig)
		if err == nil {
			pingCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
			defer cancel()
			if err = pool.Ping(pingCtx); err == nil {
				logger.Info("Successfully connected to PostgreSQL database.", zap.String("database", dbCfg.DBName))
				return pool, nil
			}
		}
		logger.Warn("Failed to connect to PostgreSQL, retrying...",
			zap.Int("attempt", i+1),
			zap.String("database", dbCfg.DBName),
			zap.Error(err),
		)
		time.Sleep(5 * time.Second)
	}

	return nil, fmt.Errorf("failed to connect to postgres after multiple attempts: %w", err)
}
-------------------------------------------------
filepath = ./platform/database/mysql.go
// FILE: platform/database/mysql.go
package database

import (
	"context"
	"database/sql"
	"fmt"
	"os"
	"time"

	_ "github.com/go-sql-driver/mysql"
	"github.com/gqls/agentchassis/platform/config"
	"go.uber.org/zap"
)

// NewMySQLConnection creates a new MySQL database connection pool with retry logic
func NewMySQLConnection(ctx context.Context, dbCfg config.DatabaseConfig, logger *zap.Logger) (*sql.DB, error) {
	password := os.Getenv(dbCfg.PasswordEnvVar)
	if password == "" {
		return nil, fmt.Errorf("database password environment variable %s is not set", dbCfg.PasswordEnvVar)
	}

	// DSN format for MySQL
	dsn := fmt.Sprintf("%s:%s@tcp(%s:%d)/%s?parseTime=true",
		dbCfg.User, password, dbCfg.Host, dbCfg.Port, dbCfg.DBName)

	var db *sql.DB
	var err error

	// Retry loop for initial connection
	for i := 0; i < 5; i++ {
		db, err = sql.Open("mysql", dsn)
		if err == nil {
			pingCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
			defer cancel()
			if err = db.PingContext(pingCtx); err == nil {
				// Set connection pool parameters
				db.SetMaxOpenConns(10)
				db.SetMaxIdleConns(5)
				db.SetConnMaxLifetime(time.Hour)

				logger.Info("Successfully connected to MySQL database.", zap.String("database", dbCfg.DBName))
				return db, nil
			}
		}
		logger.Warn("Failed to connect to MySQL, retrying...",
			zap.Int("attempt", i+1),
			zap.String("database", dbCfg.DBName),
			zap.Error(err),
		)
		time.Sleep(5 * time.Second)
	}

	return nil, fmt.Errorf("failed to connect to mysql after multiple attempts: %w", err)
}
-------------------------------------------------
filepath = ./platform/database/pgvector.go
// FILE: platform/database/pgvector.go
package database

import (
	"context"
	"fmt"

	"github.com/google/uuid"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/pgvector/pgvector-go"
	"go.uber.org/zap"
)

// MemoryRecord represents a single entry in the agent_memory table
type MemoryRecord struct {
	ID              uuid.UUID
	AgentInstanceID uuid.UUID
	Content         string
	Embedding       []float32
	Metadata        map[string]interface{}
}

// MemoryRepository provides methods for storing and retrieving agent memories
type MemoryRepository struct {
	pool   *pgxpool.Pool
	logger *zap.Logger
}

// NewMemoryRepository creates a new repository for memory operations
func NewMemoryRepository(pool *pgxpool.Pool, logger *zap.Logger) *MemoryRepository {
	return &MemoryRepository{pool: pool, logger: logger}
}

// StoreMemory saves a new memory record to the database for a specific agent
func (r *MemoryRepository) StoreMemory(ctx context.Context, agentID uuid.UUID, content string, embedding []float32, metadata map[string]interface{}) error {
	l := r.logger.With(zap.String("agent_id", agentID.String()))
	l.Info("Storing new memory")

	query := `
        INSERT INTO agent_memory (agent_instance_id, content, embedding, metadata)
        VALUES ($1, $2, $3, $4)
    `
	_, err := r.pool.Exec(ctx, query, agentID, content, pgvector.NewVector(embedding), metadata)
	if err != nil {
		l.Error("Failed to store agent memory", zap.Error(err))
		return fmt.Errorf("failed to insert memory record: %w", err)
	}

	l.Debug("Successfully stored memory record")
	return nil
}

// SearchMemory performs a semantic similarity search to find the most relevant memories
func (r *MemoryRepository) SearchMemory(ctx context.Context, agentID uuid.UUID, queryEmbedding []float32, limit int) ([]MemoryRecord, error) {
	l := r.logger.With(zap.String("agent_id", agentID.String()))
	l.Info("Searching for relevant memories", zap.Int("limit", limit))

	query := `
        SELECT id, content, metadata
        FROM agent_memory
        WHERE agent_instance_id = $1
        ORDER BY embedding <=> $2
        LIMIT $3
    `
	rows, err := r.pool.Query(ctx, query, agentID, pgvector.NewVector(queryEmbedding), limit)
	if err != nil {
		l.Error("Failed to execute memory search query", zap.Error(err))
		return nil, fmt.Errorf("failed to search memory: %w", err)
	}
	defer rows.Close()

	var results []MemoryRecord
	for rows.Next() {
		var record MemoryRecord
		record.AgentInstanceID = agentID
		if err := rows.Scan(&record.ID, &record.Content, &record.Metadata); err != nil {
			l.Error("Failed to scan memory search result", zap.Error(err))
			continue
		}
		results = append(results, record)
	}

	l.Info("Memory search completed", zap.Int("results_found", len(results)))
	return results, nil
}
-------------------------------------------------
filepath = ./makefile.old
# AI Persona System - Comprehensive Makefile
# This Makefile handles the complete deployment lifecycle

.PHONY: help setup build deploy quickstart clean logs port-forward
.DEFAULT_GOAL := help

# Colors for output
GREEN := \033[0;32m
YELLOW := \033[1;33m
RED := \033[0;31m
NC := \033[0m # No Color

# Configuration
NAMESPACE := ai-persona-system
DOCKER_REGISTRY := ai-persona-system
TIMEOUT := 300s

help: ## Show this help message
	@echo "$(GREEN)AI Persona System - Available Commands:$(NC)"
	@echo ""
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "$(YELLOW)%-20s$(NC) %s\n", $$1, $$2}'
	@echo ""
	@echo "$(GREEN)Quick Start:$(NC)"
	@echo "  make quickstart    # Complete setup and deployment"
	@echo "  make status        # Check system status"
	@echo "  make clean         # Clean up everything"

# =============================================================================
# SETUP AND INITIALIZATION
# =============================================================================

setup: ## Run initial setup (creates secrets, namespaces)
	@echo "$(GREEN)üöÄ Running initial setup...$(NC)"
	@chmod +x scripts/setup.sh
	@./scripts/setup.sh
	@echo "$(GREEN)‚úÖ Setup complete$(NC)"

check-prerequisites: ## Check if required tools are installed
	@echo "$(GREEN)üîç Checking prerequisites...$(NC)"
	@command -v kubectl >/dev/null 2>&1 || { echo "$(RED)kubectl is required but not installed$(NC)"; exit 1; }
	@command -v docker >/dev/null 2>&1 || { echo "$(RED)docker is required but not installed$(NC)"; exit 1; }
	@kubectl cluster-info >/dev/null 2>&1 || { echo "$(RED)Cannot connect to Kubernetes cluster$(NC)"; exit 1; }
	@echo "$(GREEN)‚úÖ Prerequisites check passed$(NC)"

# =============================================================================
# BUILD TARGETS
# =============================================================================

build: ## Build all Docker images
	@echo "$(GREEN)üî® Building Docker images...$(NC)"
	@docker build -t $(DOCKER_REGISTRY)/auth-service:latest -f Dockerfile.auth-service .
	@docker build -t $(DOCKER_REGISTRY)/core-manager:latest -f Dockerfile.core-manager .
	@docker build -t $(DOCKER_REGISTRY)/agent-chassis:latest -f Dockerfile.agent-chassis .
	@docker build -t $(DOCKER_REGISTRY)/reasoning-agent:latest -f Dockerfile.reasoning-agent .
	@docker build -t $(DOCKER_REGISTRY)/image-generator-adapter:latest -f Dockerfile.image-generator-adapter .
	@docker build -t $(DOCKER_REGISTRY)/web-search-adapter:latest -f Dockerfile.web-search-adapter .
	@echo "$(GREEN)‚úÖ All images built successfully$(NC)"

build-init-images: ## Build initialization utility images
	@echo "$(GREEN)üî® Building initialization images...$(NC)"
	@docker build -t $(DOCKER_REGISTRY)/database-migrator:latest -f docker/Dockerfile.migrator .
	@docker build -t $(DOCKER_REGISTRY)/data-seeder:latest -f docker/Dockerfile.seeder .
	@echo "$(GREEN)‚úÖ Initialization images built$(NC)"

build-all: build build-init-images ## Build all images including initialization utilities

# =============================================================================
# DEPLOYMENT TARGETS (PROPER ORDER)
# =============================================================================

deploy: check-prerequisites ## Deploy the entire system in correct order
	@echo "$(GREEN)üöÄ Starting full deployment...$(NC)"
	@$(MAKE) deploy-infrastructure
	@$(MAKE) deploy-storage
	@$(MAKE) deploy-messaging
	@$(MAKE) wait-for-infrastructure
	@$(MAKE) initialize-system
	@$(MAKE) deploy-core-services
	@$(MAKE) deploy-agents
	@$(MAKE) deploy-ingress-monitoring
	@echo "$(GREEN)‚úÖ Deployment completed successfully!$(NC)"

deploy-infrastructure: ## Deploy namespace, secrets, and configmaps
	@echo "$(GREEN)üì¶ Deploying infrastructure...$(NC)"
	kubectl apply -f k8s/namespace.yaml
	@echo "$(YELLOW)‚è≥ Waiting for namespace to be ready...$(NC)"
	@kubectl wait --for=jsonpath='{.status.phase}'=Active namespace/$(NAMESPACE) --timeout=$(TIMEOUT)
	kubectl apply -f k8s/configmap-common.yaml -n $(NAMESPACE)
	@echo "$(GREEN)‚úÖ Infrastructure deployed$(NC)"

deploy-storage: ## Deploy persistent storage (databases, object storage)
	@echo "$(GREEN)üíæ Deploying storage systems...$(NC)"
	kubectl apply -f k8s/postgres-clients.yaml
	kubectl apply -f k8s/postgres-templates.yaml
	kubectl apply -f k8s/mysql-auth.yaml
	kubectl apply -f k8s/minio.yaml
	@echo "$(GREEN)‚úÖ Storage systems deployed$(NC)"

deploy-messaging: ## Deploy Kafka message queue
	@echo "$(GREEN)üì® Deploying messaging system...$(NC)"
	kubectl apply -f k8s/kafka.yaml
	@echo "$(GREEN)‚úÖ Messaging system deployed$(NC)"

wait-for-infrastructure: ## Wait for infrastructure to be ready
	@echo "$(GREEN)‚è≥ Waiting for infrastructure to be ready...$(NC)"
	@echo "$(YELLOW)Waiting for PostgreSQL clients...$(NC)"
	kubectl wait --for=condition=ready pod -l app=postgres-clients -n $(NAMESPACE) --timeout=$(TIMEOUT)
	@echo "$(YELLOW)Waiting for PostgreSQL templates...$(NC)"
	kubectl wait --for=condition=ready pod -l app=postgres-templates -n $(NAMESPACE) --timeout=$(TIMEOUT)
	@echo "$(YELLOW)Waiting for MySQL auth...$(NC)"
	kubectl wait --for=condition=ready pod -l app=mysql-auth -n $(NAMESPACE) --timeout=$(TIMEOUT)
	@echo "$(YELLOW)Waiting for MinIO...$(NC)"
	kubectl wait --for=condition=ready pod -l app=minio -n $(NAMESPACE) --timeout=$(TIMEOUT)
	@echo "$(YELLOW)Waiting for Kafka cluster...$(NC)"
	kubectl wait --for=condition=ready pod -l app=kafka -n $(NAMESPACE) --timeout=$(TIMEOUT)
	@echo "$(GREEN)‚úÖ Infrastructure is ready$(NC)"

deploy-automated: check-prerequisites build-all ## Automated deployment using the deployment script
	@echo "$(GREEN)üöÄ Starting automated deployment...$(NC)"
	@chmod +x scripts/deploy-system.sh
	@./scripts/deploy-system.sh
	@echo "$(GREEN)‚úÖ Automated deployment completed!$(NC)"

initialize-system: ## Initialize databases and create Kafka topics
	@echo "$(GREEN)üîß Initializing system...$(NC)"
	kubectl apply -f k8s/jobs/database-init-job.yaml
	kubectl apply -f k8s/jobs/kafka-topics-job.yaml
	@echo "$(YELLOW)‚è≥ Waiting for initialization jobs to complete...$(NC)"
	@kubectl wait --for=condition=complete job/database-init -n $(NAMESPACE) --timeout=600s
	@kubectl wait --for=condition=complete job/kafka-topics-init -n $(NAMESPACE) --timeout=300s
	@kubectl wait --for=condition=complete job/data-seeder -n $(NAMESPACE) --timeout=300s
	@echo "$(GREEN)‚úÖ System initialization complete$(NC)"

deploy-core-services: ## Deploy core services (auth, core-manager)
	@echo "$(GREEN)üèóÔ∏è  Deploying core services...$(NC)"
	kubectl apply -f k8s/auth-service.yaml
	kubectl apply -f k8s/core-manager.yaml
	@echo "$(YELLOW)‚è≥ Waiting for core services to be ready...$(NC)"
	kubectl wait --for=condition=ready pod -l app=auth-service -n $(NAMESPACE) --timeout=$(TIMEOUT)
	kubectl wait --for=condition=ready pod -l app=core-manager -n $(NAMESPACE) --timeout=$(TIMEOUT)
	@echo "$(GREEN)‚úÖ Core services deployed$(NC)"

deploy-agents: ## Deploy all agent services
	@echo "$(GREEN)ü§ñ Deploying agent services...$(NC)"
	kubectl apply -f k8s/agent-chassis.yaml
	kubectl apply -f k8s/reasoning-agent.yaml
	kubectl apply -f k8s/image-generator-adapter.yaml
	kubectl apply -f k8s/web-search-adapter.yaml
	@echo "$(YELLOW)‚è≥ Waiting for agents to be ready...$(NC)"
	kubectl wait --for=condition=ready pod -l app=agent-chassis -n $(NAMESPACE) --timeout=$(TIMEOUT)
	kubectl wait --for=condition=ready pod -l app=reasoning-agent -n $(NAMESPACE) --timeout=$(TIMEOUT)
	@echo "$(GREEN)‚úÖ Agent services deployed$(NC)"

deploy-ingress-monitoring: ## Deploy ingress and monitoring
	@echo "$(GREEN)üìä Deploying ingress and monitoring...$(NC)"
	kubectl apply -f k8s/ingress.yaml
	kubectl apply -f k8s/monitoring/
	@echo "$(GREEN)‚úÖ Ingress and monitoring deployed$(NC)"

# =============================================================================
# DATABASE MANAGEMENT
# =============================================================================

migrate-all-databases: ## Run all database migrations
	@echo "$(GREEN)üìù Running database migrations...$(NC)"
	@$(MAKE) migrate-pgvector
	@$(MAKE) migrate-templates-db
	@$(MAKE) migrate-clients-db
	@$(MAKE) migrate-auth-db
	@echo "$(GREEN)‚úÖ All migrations completed$(NC)"

migrate-pgvector: ## Enable pgvector extension
	@echo "$(YELLOW)üîß Enabling pgvector extension...$(NC)"
	kubectl exec -n $(NAMESPACE) postgres-clients-0 -- psql -U clients_user -d clients_db -c "CREATE EXTENSION IF NOT EXISTS vector;"
	@echo "$(GREEN)‚úÖ pgvector enabled$(NC)"

migrate-templates-db: ## Migrate templates database
	@echo "$(YELLOW)üìù Migrating templates database...$(NC)"
	kubectl cp platform/database/migrations/002_create_templates_schema.sql $(NAMESPACE)/postgres-templates-0:/tmp/
	kubectl exec -n $(NAMESPACE) postgres-templates-0 -- psql -U templates_user -d templates_db -f /tmp/002_create_templates_schema.sql
	@echo "$(GREEN)‚úÖ Templates database migrated$(NC)"

migrate-clients-db: ## Migrate clients database (requires CLIENT_ID)
	@echo "$(YELLOW)üìù Migrating clients database...$(NC)"
	kubectl cp platform/database/migrations/003_create_client_schema.sql $(NAMESPACE)/postgres-clients-0:/tmp/
	@# Note: This creates the base structure, client-specific schemas are created on-demand
	@echo "$(GREEN)‚úÖ Clients database migrated$(NC)"

migrate-auth-db: ## Migrate auth database
	@echo "$(YELLOW)üìù Migrating auth database...$(NC)"
	kubectl cp platform/database/migrations/004_auth_schema.sql $(NAMESPACE)/mysql-auth-0:/tmp/
	kubectl exec -n $(NAMESPACE) mysql-auth-0 -- mysql -u auth_user -p$(shell kubectl get secret db-secrets -n $(NAMESPACE) -o jsonpath='{.data.auth-db-password}' | base64 -d) auth_db < /tmp/004_auth_schema.sql
	kubectl cp platform/database/migrations/005_projects_schema.sql $(NAMESPACE)/mysql-auth-0:/tmp/
	kubectl exec -n $(NAMESPACE) mysql-auth-0 -- mysql -u auth_user -p$(shell kubectl get secret db-secrets -n $(NAMESPACE) -o jsonpath='{.data.auth-db-password}' | base64 -d) auth_db < /tmp/005_projects_schema.sql
	@echo "$(GREEN)‚úÖ Auth database migrated$(NC)"

create-client-schema: ## Create schema for a specific client (requires CLIENT_ID env var)
	@if [ -z "$(CLIENT_ID)" ]; then \
		echo "$(RED)‚ùå CLIENT_ID environment variable is required$(NC)"; \
		echo "Usage: make create-client-schema CLIENT_ID=client_123"; \
		exit 1; \
	fi
	@echo "$(YELLOW)üîß Creating schema for client: $(CLIENT_ID)$(NC)"
	@sed 's/{client_id}/$(CLIENT_ID)/g' platform/database/migrations/003_create_client_schema.sql > /tmp/client_schema_$(CLIENT_ID).sql
	kubectl cp /tmp/client_schema_$(CLIENT_ID).sql $(NAMESPACE)/postgres-clients-0:/tmp/
	kubectl exec -n $(NAMESPACE) postgres-clients-0 -- psql -U clients_user -d clients_db -f /tmp/client_schema_$(CLIENT_ID).sql
	@rm /tmp/client_schema_$(CLIENT_ID).sql
	@echo "$(GREEN)‚úÖ Schema created for client: $(CLIENT_ID)$(NC)"

# =============================================================================
# KAFKA MANAGEMENT
# =============================================================================

create-all-kafka-topics: ## Create all required Kafka topics
	@echo "$(GREEN)üì® Creating all Kafka topics...$(NC)"
	@$(MAKE) kafka-create-system-topics
	@$(MAKE) kafka-create-core-topics
	@echo "$(GREEN)‚úÖ All Kafka topics created$(NC)"

kafka-create-core-topics: ## Create core system topics used by agents
	@echo "$(YELLOW)üîß Creating core Kafka topics...$(NC)"
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.agent.reasoning.process --partitions 3 --replication-factor 1 --if-not-exists
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.responses.reasoning --partitions 6 --replication-factor 1 --if-not-exists
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.adapter.image.generate --partitions 3 --replication-factor 1 --if-not-exists
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.responses.image --partitions 6 --replication-factor 1 --if-not-exists
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.adapter.web.search --partitions 3 --replication-factor 1 --if-not-exists
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.responses.websearch --partitions 6 --replication-factor 1 --if-not-exists
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.notifications.ui --partitions 3 --replication-factor 1 --if-not-exists
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.commands.workflow.resume --partitions 3 --replication-factor 1 --if-not-exists
	@echo "$(GREEN)‚úÖ Core topics created$(NC)"

kafka-list-topics: ## List all Kafka topics
	@echo "$(GREEN)üìã Listing Kafka topics...$(NC)"
	@kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --list

kafka-create-agent-topics: ## Create topics for a specific agent type
	@read -p "Enter agent type (e.g., copywriter, researcher): " agent_type; \
	echo "$(YELLOW)üîß Creating topics for agent: $$agent_type$(NC)"; \
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic tasks.high.$$agent_type --partitions 3 --replication-factor 1 --if-not-exists; \
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic tasks.normal.$$agent_type --partitions 6 --replication-factor 1 --if-not-exists; \
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic tasks.low.$$agent_type --partitions 3 --replication-factor 1 --if-not-exists; \
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic responses.$$agent_type --partitions 6 --replication-factor 1 --if-not-exists; \
	kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic dlq.$$agent_type --partitions 1 --replication-factor 1 --if-not-exists; \
	echo "$(GREEN)‚úÖ Topics created for agent: $$agent_type$(NC)"

kafka-create-system-topics: ## Create system-level topics
	@echo "$(YELLOW)üîß Creating system topics...$(NC)"
	@kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic orchestrator.state-changes --partitions 12 --replication-factor 1 --if-not-exists
	@kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic human.approvals --partitions 6 --replication-factor 1 --if-not-exists
	@kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --create --topic system.events --partitions 3 --replication-factor 1 --if-not-exists
	@echo "$(GREEN)‚úÖ System topics created$(NC)"

kafka-delete-agent-topics: ## Delete topics for a specific agent type
	@read -p "Enter agent type to delete topics for: " agent_type; \
	read -p "Are you sure you want to delete all topics for $$agent_type? (y/N): " confirm; \
	if [ "$$confirm" = "y" ] || [ "$$confirm" = "Y" ]; then \
		echo "$(RED)üóëÔ∏è  Deleting topics for agent: $$agent_type$(NC)"; \
		kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --delete --topic tasks.high.$$agent_type --if-exists; \
		kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --delete --topic tasks.normal.$$agent_type --if-exists; \
		kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --delete --topic tasks.low.$$agent_type --if-exists; \
		kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --delete --topic responses.$$agent_type --if-exists; \
		kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --delete --topic dlq.$$agent_type --if-exists; \
		echo "$(GREEN)‚úÖ Topics deleted for agent: $$agent_type$(NC)"; \
	else \
		echo "$(YELLOW)‚ùå Deletion cancelled$(NC)"; \
	fi

# =============================================================================
# DATA SEEDING
# =============================================================================

seed-initial-data: ## Seed the system with initial templates and data
	@echo "$(GREEN)üå± Seeding initial data...$(NC)"
	@$(MAKE) seed-persona-templates
	@$(MAKE) seed-subscription-tiers
	@echo "$(GREEN)‚úÖ Initial data seeded$(NC)"

seed-persona-templates: ## Seed initial persona templates
	@echo "$(YELLOW)ü§ñ Seeding persona templates...$(NC)"
	@# Create a basic copywriter template
	kubectl exec -n $(NAMESPACE) postgres-templates-0 -- psql -U templates_user -d templates_db -c \
		"INSERT INTO persona_templates (id, name, description, category, config) VALUES \
		('00000000-0000-0000-0000-000000000001', 'Basic Copywriter', 'A versatile copywriting assistant', 'copywriter', \
		'{\"model\": \"claude-3-sonnet\", \"temperature\": 0.7, \"max_tokens\": 2000}') \
		ON CONFLICT (id) DO NOTHING;"
	@# Create a research assistant template
	kubectl exec -n $(NAMESPACE) postgres-templates-0 -- psql -U templates_user -d templates_db -c \
		"INSERT INTO persona_templates (id, name, description, category, config) VALUES \
		('00000000-0000-0000-0000-000000000002', 'Research Assistant', 'In-depth research and analysis', 'researcher', \
		'{\"model\": \"claude-3-opus\", \"temperature\": 0.3, \"max_tokens\": 4000}') \
		ON CONFLICT (id) DO NOTHING;"
	@echo "$(GREEN)‚úÖ Persona templates seeded$(NC)"

seed-subscription-tiers: ## Ensure subscription tiers exist
	@echo "$(YELLOW)üí≥ Ensuring subscription tiers exist...$(NC)"
	@# The tiers should already be created by the migration, but this ensures they exist
	kubectl exec -n $(NAMESPACE) mysql-auth-0 -- mysql -u auth_user -p$(shell kubectl get secret db-secrets -n $(NAMESPACE) -o jsonpath='{.data.auth-db-password}' | base64 -d) auth_db -e \
		"SELECT COUNT(*) as tier_count FROM subscription_tiers;" 2>/dev/null || echo "Subscription tiers table not ready yet"
	@echo "$(GREEN)‚úÖ Subscription tiers verified$(NC)"

# =============================================================================
# AGENT MANAGEMENT
# =============================================================================

register-agent: ## Register a new agent type
	@read -p "Enter agent type (e.g., copywriter): " agent_type; \
	read -p "Enter display name: " display_name; \
	read -p "Enter category (data-driven/code-driven/adapter): " category; \
	echo "$(YELLOW)üìù Registering agent: $$agent_type$(NC)"; \
	$(MAKE) kafka-create-agent-topics; \
	kubectl exec -n $(NAMESPACE) core-manager-0 -- /app/core-manager register-agent \
		--type="$$agent_type" \
		--name="$$display_name" \
		--category="$$category"

# =============================================================================
# SYSTEM MONITORING AND DEBUGGING
# =============================================================================

status: ## Check overall system status
	@echo "$(GREEN)üìä System Status Overview$(NC)"
	@echo "$(YELLOW)Namespace:$(NC)"
	@kubectl get namespace $(NAMESPACE) 2>/dev/null || echo "$(RED)Namespace not found$(NC)"
	@echo ""
	@echo "$(YELLOW)Pods Status:$(NC)"
	@kubectl get pods -n $(NAMESPACE) -o wide 2>/dev/null || echo "$(RED)No pods found$(NC)"
	@echo ""
	@echo "$(YELLOW)Services:$(NC)"
	@kubectl get services -n $(NAMESPACE) 2>/dev/null || echo "$(RED)No services found$(NC)"
	@echo ""
	@echo "$(YELLOW)Persistent Volumes:$(NC)"
	@kubectl get pvc -n $(NAMESPACE) 2>/dev/null || echo "$(RED)No PVCs found$(NC)"

system-check: ## Comprehensive system health check
	@echo "$(GREEN)üîç Comprehensive System Check$(NC)"
	@echo ""
	@echo "$(YELLOW)üìä Kafka Topics:$(NC)"
	@$(MAKE) kafka-list-topics 2>/dev/null || echo "$(RED)Kafka not accessible$(NC)"
	@echo ""
	@echo "$(YELLOW)üìä Database Tables (Templates):$(NC)"
	@kubectl exec -n $(NAMESPACE) postgres-templates-0 -- psql -U templates_user -d templates_db -c "\dt" 2>/dev/null || echo "$(RED)Templates DB not accessible$(NC)"
	@echo ""
	@echo "$(YELLOW)üìä Database Tables (Clients):$(NC)"
	@kubectl exec -n $(NAMESPACE) postgres-clients-0 -- psql -U clients_user -d clients_db -c "\dt" 2>/dev/null || echo "$(RED)Clients DB not accessible$(NC)"
	@echo ""
	@echo "$(YELLOW)üìä Persona Templates:$(NC)"
	@kubectl exec -n $(NAMESPACE) postgres-templates-0 -- psql -U templates_user -d templates_db -c "SELECT id, name, category FROM persona_templates WHERE is_active = true;" 2>/dev/null || echo "$(RED)Templates not accessible$(NC)"

logs: ## View logs for a specific service
	@echo "$(GREEN)Available services:$(NC)"
	@echo "  auth-service"
	@echo "  core-manager"
	@echo "  agent-chassis"
	@echo "  reasoning-agent"
	@echo "  image-generator-adapter"
	@echo "  web-search-adapter"
	@echo "  kafka"
	@echo "  postgres-clients"
	@echo "  postgres-templates"
	@echo "  mysql-auth"
	@echo ""
	@read -p "Enter service name: " service; \
	echo "$(YELLOW)üìã Showing logs for $$service...$(NC)"; \
	kubectl logs -n $(NAMESPACE) -l app=$$service --tail=100 -f

describe-pod: ## Describe a specific pod for debugging
	@kubectl get pods -n $(NAMESPACE)
	@echo ""
	@read -p "Enter pod name: " pod; \
	kubectl describe pod $$pod -n $(NAMESPACE)

port-forward: ## Set up port forwarding for local access
	@echo "$(GREEN)üîó Setting up port forwarding...$(NC)"
	@echo "$(YELLOW)Auth Service: http://localhost:8081$(NC)"
	@echo "$(YELLOW)Core Manager: http://localhost:8088$(NC)"
	@echo "$(YELLOW)Grafana: http://localhost:3000$(NC)"
	@echo "$(YELLOW)Kafka UI: http://localhost:8080$(NC)"
	@echo ""
	@echo "$(YELLOW)Starting port forwards (Ctrl+C to stop)...$(NC)"
	@trap 'kill %1 %2 %3 %4 2>/dev/null' EXIT; \
	kubectl port-forward -n $(NAMESPACE) svc/auth-service 8081:8081 & \
	kubectl port-forward -n $(NAMESPACE) svc/core-manager 8088:8088 & \
	kubectl port-forward -n $(NAMESPACE) svc/grafana 3000:3000 & \
	kubectl port-forward -n $(NAMESPACE) svc/kafka-ui 8080:8080 & \
	wait

# =============================================================================
# COMPLETE WORKFLOWS
# =============================================================================

quickstart: ## Complete setup and deployment from scratch
	@echo "$(GREEN)üöÄ Starting AI Persona System Quickstart$(NC)"
	@$(MAKE) check-prerequisites
	@$(MAKE) setup
	@$(MAKE) deploy-automated
	@echo ""
	@echo "$(GREEN)‚úÖ Quickstart completed successfully!$(NC)"
	@echo ""
	@echo "$(YELLOW)Next steps:$(NC)"
	@echo "1. Run 'make port-forward' to access services locally"
	@echo "2. Run 'make system-check' to verify everything is working"
	@echo "3. Create your first client: 'make create-client-schema CLIENT_ID=demo_client'"
	@echo "4. Register your first agent: 'make register-agent'"
	@echo ""
	@echo "$(YELLOW)Access URLs:$(NC)"
	@echo "- Auth API: http://localhost:8081"
	@echo "- Core API: http://localhost:8088"
	@echo "- Grafana: http://localhost:3000 (admin/admin)"

quickstart-manual: ## Manual step-by-step deployment
	@echo "$(GREEN)üöÄ Starting AI Persona System Manual Deployment$(NC)"
	@$(MAKE) check-prerequisites
	@$(MAKE) setup
	@$(MAKE) build-all
	@$(MAKE) deploy
	@echo ""
	@echo "$(GREEN)‚úÖ Manual deployment completed successfully!$(NC)"

restart-service: ## Restart a specific service
	@echo "$(GREEN)Available services to restart:$(NC)"
	@kubectl get deployments -n $(NAMESPACE) -o name | sed 's|deployment.apps/||'
	@echo ""
	@read -p "Enter service name: " service; \
	echo "$(YELLOW)üîÑ Restarting $$service...$(NC)"; \
	kubectl rollout restart deployment/$$service -n $(NAMESPACE); \
	kubectl rollout status deployment/$$service -n $(NAMESPACE)

# =============================================================================
# CLEANUP
# =============================================================================

clean: ## Clean up everything (DESTRUCTIVE!)
	@echo "$(RED)‚ö†Ô∏è  This will DELETE the entire $(NAMESPACE) namespace and all data!$(NC)"
	@read -p "Are you sure? Type 'DELETE' to confirm: " confirm; \
	if [ "$$confirm" = "DELETE" ]; then \
		echo "$(RED)üóëÔ∏è  Deleting namespace $(NAMESPACE)...$(NC)"; \
		kubectl delete namespace $(NAMESPACE) --ignore-not-found=true; \
		echo "$(GREEN)‚úÖ Cleanup completed$(NC)"; \
	else \
		echo "$(YELLOW)‚ùå Cleanup cancelled$(NC)"; \
	fi

clean-pods: ## Delete all pods (they will be recreated)
	@echo "$(YELLOW)üîÑ Deleting all pods in $(NAMESPACE)...$(NC)"
	@kubectl delete pods --all -n $(NAMESPACE)
	@echo "$(GREEN)‚úÖ Pods deleted (they will be recreated automatically)$(NC)"

clean-failed-jobs: ## Clean up failed jobs
	@echo "$(YELLOW)üßπ Cleaning up failed jobs...$(NC)"
	@kubectl delete jobs -n $(NAMESPACE) --field-selector status.successful=0
	@echo "$(GREEN)‚úÖ Failed jobs cleaned up$(NC)"

# =============================================================================
# TESTING
# =============================================================================

test-api: ## Test the API endpoints
	@echo "$(GREEN)üß™ Testing API endpoints...$(NC)"
	@chmod +x scripts/test-system.sh
	@./scripts/test-system.sh

smoke-test: ## Run smoke tests to verify basic functionality
	@echo "$(GREEN)üí® Running smoke tests...$(NC)"
	@$(MAKE) system-check
	@echo ""
	@echo "$(YELLOW)Testing basic connectivity...$(NC)"
	@kubectl exec -n $(NAMESPACE) postgres-clients-0 -- pg_isready -U clients_user && echo "$(GREEN)‚úÖ Clients DB ready$(NC)" || echo "$(RED)‚ùå Clients DB not ready$(NC)"
	@kubectl exec -n $(NAMESPACE) postgres-templates-0 -- pg_isready -U templates_user && echo "$(GREEN)‚úÖ Templates DB ready$(NC)" || echo "$(RED)‚ùå Templates DB not ready$(NC)"
	@kubectl exec -n $(NAMESPACE) kafka-0 -- kafka-topics --bootstrap-server localhost:9092 --list >/dev/null 2>&1 && echo "$(GREEN)‚úÖ Kafka ready$(NC)" || echo "$(RED)‚ùå Kafka not ready$(NC)"
	@echo ""
	@echo "$(GREEN)‚úÖ Smoke tests completed$(NC)"-------------------------------------------------
filepath = ./README-notes.md

# set up .env file with real keys

# Enable pgvector on clients database
psql -h localhost -U clients_user -d clients_db -f platform/database/migrations/001_enable_pgvector.sql

# Create schemas
psql -h localhost -U templates_user -d templates_db -f platform/database/migrations/002_create_templates_schema.sql

docker-compose up -d

# Clone and enter the project
cd ai-persona-system

# Run the setup script
chmod +x scripts/setup.sh
./scripts/setup.sh

# Build and deploy
make build
make deploy

# Access locally
make port-forward

Create a client
make create-client
# Enter: client_123

# Access the API
curl -X POST http://localhost:8081/api/v1/auth/register \
-H "Content-Type: application/json" \
-d '{"email":"user@example.

# Continue from registration...
curl -X POST http://localhost:8081/api/v1/auth/register \
-H "Content-Type: application/json" \
-d '{"email":"user@example.com","password":"securepass123","client_id":"client_123"}'

# Login to get token
curl -X POST http://localhost:8081/api/v1/auth/login \
-H "Content-Type: application/json" \
-d '{"email":"user@example.com","password":"securepass123"}' \
| jq -r '.access_token' > token.txt

# Create an agent instance
TOKEN=$(cat token.txt)
curl -X POST http://localhost:8088/api/v1/personas/instances \
-H "Authorization: Bearer $TOKEN" \
-H "Content-Type: application/json" \
-d '{"template_id":"00000000-0000-0000-0000-000000000001","instance_name":"My Copywriter"}'

// Example: Send a copywriting task
producer, _ := kafka.NewProducer([]string{"localhost:9092"}, logger)

taskPayload := map[string]interface{}{
"action": "generate_blog_post",
"data": map[string]interface{}{
"topic": "AI and the Future of Work",
"tone": "professional",
"length": "1000 words",
},
}

headers := map[string]string{
"correlation_id": uuid.NewString(),
"request_id": uuid.NewString(),
"client_id": "client_123",
"agent_instance_id": "your-agent-id",
"fuel_budget": "100",
}

payloadBytes, _ := json.Marshal(taskPayload)
producer.Produce(ctx, "system.tasks.copywriter", headers, nil, payloadBytes)

# View logs
make logs
# Enter: agent-chassis

# Check Grafana dashboards
# Open http://localhost:3000
# Default login: admin/admin

==
# Using the Makefile
make deploy

# Or manually in order:
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/network-policies.yaml
kubectl apply -f k8s/configmap-common.yaml
# ... (rest of the files as shown in Makefile)
k8s/
‚îú‚îÄ‚îÄ namespace.yaml
‚îú‚îÄ‚îÄ configmap-common.yaml
‚îú‚îÄ‚îÄ network-policies.yaml
‚îú‚îÄ‚îÄ postgres-clients.yaml
‚îú‚îÄ‚îÄ postgres-templates.yaml
‚îú‚îÄ‚îÄ mysql-auth.yaml
‚îú‚îÄ‚îÄ kafka.yaml
‚îú‚îÄ‚îÄ minio.yaml
‚îú‚îÄ‚îÄ auth-service.yaml
‚îú‚îÄ‚îÄ core-manager.yaml
‚îú‚îÄ‚îÄ agent-chassis.yaml
‚îú‚îÄ‚îÄ reasoning-agent.yaml
‚îú‚îÄ‚îÄ image-generator-adapter.yaml
‚îú‚îÄ‚îÄ web-search-adapter.yaml
‚îú‚îÄ‚îÄ ingress.yaml
‚îú‚îÄ‚îÄ secrets-template.yaml
‚îî‚îÄ‚îÄ monitoring/
‚îú‚îÄ‚îÄ prometheus-config.yaml
‚îú‚îÄ‚îÄ prometheus.yaml
‚îú‚îÄ‚îÄ grafana.yaml
‚îî‚îÄ‚îÄ service-monitor.yaml


# Option 1: Do everything automatically
make quickstart

# Option 2: Step by step
make setup          # Run the setup.sh script
make build          # Build Docker images  
make deploy         # Deploy to Kubernetes
make migrate-up     # Run database migrations
make seed-data      # Add initial templates
make port-forward   # Access services locally

--

If Using Standard Prometheus (your current setup):

You don't need ServiceMonitor
Your prometheus-config.yaml already uses annotation-based discovery
Services just need the prometheus.io/scrape: "true" annotation

If Using Prometheus Operator:

Use the ServiceMonitor definitions provided
Install with: kubectl apply -f k8s/monitoring/service-monitor.yaml

To Enable Metrics:

Ensure services expose metrics port (9090)
Implement /metrics endpoint in your Go services
Add Prometheus metrics to your code (you already have this in platform/observability/metrics.go)

Quick Check:
# Verify metrics are exposed
kubectl port-forward -n ai-persona-system svc/auth-service 9090:9090
curl http://localhost:9090/metrics

The ServiceMonitor approach provides more fine-grained control over metric collection and is the preferred method when using Prometheus Operator in production environments.


# Complete deployment from scratch
make quickstart

# Step-by-step deployment
make setup
make build  
make deploy

# Create a new client
make create-client-schema CLIENT_ID=client_123

# Check system health
make system-check
make status

# Access services locally
make port-forward

# View logs
make logs
# (then type: auth-service)

# Register a new agent type
make register-agent

# Clean up everything
make clean-------------------------------------------------
filepath = ./outputtotext.sh
#!/bin/bash

output_file="resulttextoutput.txt"

# Clear or create the output file
> "$output_file"

find . -type f \
    -not \( -name "go.mod" -o -name "go.sum" -o -name "*.hcl" -o -name "*.tar" -o -name "*.log" -o -name "*.tfstate" -o -name "$output_file" \) \
    -not \( -name "*.tfstate.backup" -o -name "terraform.tfstate.*prod-cluster" \) \
		-not \( -name "persona-cli" -o -name "README.md" -o -name "create_persona.sql" -o -name "*.secret" -o -name "*-lock.json" -o -name "*.txt" \) \
    -not -path "*/\\.*/*" \
		-not -path "*/backup/*" \
		-not -path "*/images/*" \
		-not -path "*/project3/*" \
		-not -path "*/docs/*" \
		-not -path "./projects/*" \
		-not -path "*/gateway/templates/*" \
		-not -path "*/strimzi-yaml-0.45.0/*" \
		-not -path "*/production/sydney/*" \
    -print0 | \
while IFS= read -r -d $'\0' file; do
    echo "filepath = $file" >> "$output_file" || { echo "Failed to write to $output_file"; exit 1; }
    cat "$file" >> "$output_file" || { echo "Failed to write to $output_file"; exit 1; }
    echo "-------------------------------------------------" >> "$output_file" || { echo "Failed to write to $output_file"; exit 1; }
done
-------------------------------------------------
